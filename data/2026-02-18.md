<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.LG](#cs.LG) [Total: 69]
- [cs.MM](#cs.MM) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: GRAFNet是一种受生物视觉系统启发的息肉分割网络，通过模拟人类视觉层次结构，结合导向注意力、多尺度视网膜模块和皮层反馈机制，显著提升了结肠镜息肉分割的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 结肠镜息肉分割面临三大挑战：息肉形态高度可变（从平坦到突出病变）、与正常结构（褶皱、血管）视觉相似性强、需要鲁棒的多尺度检测。现有深度学习方法存在单向处理、多尺度融合弱、缺乏解剖约束等问题，导致假阳性（正常结构过分割）和假阴性（遗漏细微平坦病变）。

Method: 提出GRAFNet生物启发架构，模拟人类视觉系统层次组织：1) 导向非对称注意力模块(GAAM)，模拟方向调谐皮层神经元以增强息肉边界；2) 多尺度视网膜模块(MSRM)，复制视网膜神经节细胞通路进行并行多特征分析；3) 导向皮层注意力反馈模块(GCAFM)，应用预测编码进行迭代优化。这些模块在息肉编码器-解码器模块(PEDM)中统一，通过分辨率自适应反馈强制空间语义一致性。

Result: 在五个公开基准数据集（Kvasir-SEG、CVC-300、CVC-ColonDB、CVC-Clinic、PolypGen）上实现了一致的SOTA性能：Dice系数提升3-8%，泛化能力提高10-20%，同时提供可解释的决策路径。

Conclusion: 这项工作建立了神经计算原理连接AI准确性与临床可信推理的新范式，为医学图像分析提供了生物启发的可靠解决方案。

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

</details>


### [2] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出解耦框架，将目标检测与交互识别分离，利用多模态大语言模型进行零样本交互识别，实现无需训练的零样本HOI检测


<details>
  <summary>Details</summary>
Motivation: 现有方法将交互识别与特定检测器紧密耦合，依赖粗粒度视觉语言模型特征，限制了未见交互的泛化能力

Method: 1) 解耦框架分离目标检测与交互识别；2) 确定性生成方法将交互识别转化为视觉问答任务；3) 空间感知池化模块整合外观和成对空间线索；4) 一次性确定性匹配方法在单次前向传播中预测所有候选交互

Result: 在HICO-DET和V-COCO数据集上实现优越的零样本性能，具备强大的跨数据集泛化能力，并能与任何目标检测器集成而无需重新训练

Conclusion: 提出的解耦框架通过分离检测与识别、利用MLLMs进行零样本交互识别，显著提升了零样本HOI检测的性能和灵活性

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [3] [MB-DSMIL-CL-PL: Scalable Weakly Supervised Ovarian Cancer Subtype Classification and Localisation Using Contrastive and Prototype Learning with Frozen Patch Features](https://arxiv.org/abs/2602.15138)
*Marcus Jenkins,Jasenka Mazibrada,Bogdan Leahu,Michal Mackiewicz*

Main category: cs.CV

TL;DR: 提出一种基于对比学习和原型学习的卵巢癌组织病理学图像亚型分类与定位方法，使用预计算的冻结特征，通过特征空间增强实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 卵巢癌组织病理学亚型研究对个性化治疗很重要，但诊断工作量增加给病理科带来挑战。传统方法依赖预计算的冻结特征，而端到端方法虽然准确率更高但训练可扩展性差且实验耗时。

Method: 使用对比学习和原型学习，结合预计算的冻结特征，通过特征空间增强进行卵巢癌组织病理学图像的亚型分类和定位。

Result: 相比DSMIL方法，在实例级和切片级分类的F1分数分别提升70.4%和15.3%，实例定位的AUC提升16.9%，切片分类AUC提升2.3%，同时保持使用冻结特征。

Conclusion: 该方法在保持使用冻结特征的高效性的同时，显著提升了卵巢癌组织病理学图像亚型分类和定位的性能，平衡了准确性和可扩展性。

Abstract: The study of histopathological subtypes is valuable for the personalisation of effective treatment strategies for ovarian cancer. However, increasing diagnostic workloads present a challenge for UK pathology departments, leading to the rise in AI approaches. While traditional approaches in this field have relied on pre-computed, frozen image features, recent advances have shifted towards end-to-end feature extraction, providing an improvement in accuracy but at the expense of significantly reduced scalability during training and time-consuming experimentation. In this paper, we propose a new approach for subtype classification and localisation in ovarian cancer histopathology images using contrastive and prototype learning with pre-computed, frozen features via feature-space augmentations. Compared to DSMIL, our method achieves an improvement of 70.4\% and 15.3\% in F1 score for instance- and slide-level classification, respectively, along with AUC gains of 16.9\% for instance localisation and 2.3\% for slide classification, while maintaining the use of frozen patch features.

</details>


### [4] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出基于累积样本损失（CSL）的视频标注错误检测方法，通过分析帧级损失轨迹识别错误标注和时序错乱，无需标注错误真值，在EgoPER和Cholec80数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频数据集常存在标注错误（错误标签和时序错乱），这些错误在相位标注任务中特别有害，因为时序一致性至关重要。现有方法需要标注错误真值，限制了应用。

Method: 提出模型无关的标注错误检测方法：训练视频分割模型并保存每个epoch的权重，使用这些检查点评估测试视频中每帧的损失，计算累积样本损失（CSL）。错误标注或时序错乱的帧会呈现持续高损失或不规则模式，而正确标注的帧会早期收敛到低损失。

Result: 在EgoPER和Cholec80数据集上的实验表明，该方法能有效检测错误标注和帧错乱等细微不一致性，提供强大的数据集审计工具。

Conclusion: 提出的CSL方法为视频数据集审计提供了有效工具，能识别标注错误，提高视频机器学习训练的可靠性，且无需标注错误真值，具有跨数据集泛化能力。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [5] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 提出一种分布深度学习框架，用于提升4D Flow MRI超分辨率性能，通过结合CFD模拟数据和小规模真实配对数据，解决临床场景中的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 医学影像超分辨率通常依赖人工降采样的配对数据训练，但真实临床采集的低分辨率数据与训练数据存在域偏移，导致模型泛化能力差。特别是4D Flow MRI这种新型血流成像模态，对评估动脉瘤破裂风险至关重要。

Method: 提出分布深度学习框架：1）首先在高分辨率计算流体动力学(CFD)模拟数据及其降采样版本上训练；2）然后在少量配对的4D Flow MRI和CFD数据上进行微调；3）推导分布估计器的理论性质。

Result: 框架在真实数据应用中显著优于传统深度学习方法，证明了分布学习在解决域偏移和提升临床现实场景中超分辨率性能的有效性。

Conclusion: 分布深度学习框架能够有效解决医学影像超分辨率中的域偏移问题，提高模型在真实临床场景中的鲁棒性和泛化能力，特别适用于4D Flow MRI等新型成像模态。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [6] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 提出基于神经体渲染的相机虚拟化方法，支持动态场景的高质量新视角合成和时间归档功能，适用于体育直播等应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的动态场景方法依赖准确点云且难以处理快速非刚性运动，无法支持时间归档功能，限制了在体育直播等应用中的实用性。

Method: 采用神经体渲染框架，将动态场景建模为多相机视图间的刚性变换，通过神经表示学习实现高质量渲染，支持时间归档功能。

Result: 方法能处理快速非刚性运动，提供增强的视觉渲染质量，支持用户回溯任意时间点进行新视角合成，实现回放、分析和归档功能。

Conclusion: 神经体渲染框架为相机虚拟化提供了更实用的解决方案，特别适用于需要时间归档功能的体育直播和舞台表演等应用场景。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [7] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 首个大规模长上下文视觉语言模型研究，训练至344K上下文，在长文档视觉问答任务上取得SOTA性能，并发现视觉长上下文训练可迁移到文本长上下文任务。


<details>
  <summary>Details</summary>
Motivation: 虽然已有Qwen3 VL和GLM 4.5/6V等开源长上下文视觉语言模型，但它们的训练方法和数据流程不可复现。本研究旨在填补这一空白，系统研究长上下文视觉语言模型的训练方法。

Method: 对24B和32B参数模型进行系统研究，包括持续预训练、监督微调和偏好优化，使用大量长上下文评估和消融实验。还发布了MMLBD-C基准测试的修正版本。

Result: 在MMLongBenchDoc基准测试的两个参数规模上都取得了最先进的性能。关键发现包括：训练上下文长度与评估长度匹配时效果最佳；使用页面索引能显著提升性能；合成数据管道支持自我改进；视觉长上下文训练可迁移到文本长上下文任务。

Conclusion: 本研究提供了首个可复现的长上下文视觉语言模型训练框架，发现了多个重要训练规律，并证明了视觉与文本长上下文能力之间的双向迁移性，为未来研究奠定了基础。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [8] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: E^2D是一种探索-利用蒸馏方法，通过两阶段优化策略减少冗余计算，在大规模数据集蒸馏中同时实现高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 现有解耦式数据集蒸馏方法存在效率与精度之间的权衡：基于优化的方法精度高但计算密集，无优化方法高效但精度低。需要克服这一矛盾。

Method: 提出探索-利用蒸馏(E^2D)：1) 全图像初始化保持语义完整性和特征多样性；2) 两阶段优化策略：探索阶段进行均匀更新并识别高损失区域，利用阶段集中更新这些区域加速收敛。

Result: 在ImageNet-1K上超越SOTA且快18倍，在ImageNet-21K上显著提升精度且快4.3倍。证明针对性更新比暴力优化更能平衡精度与效率。

Conclusion: 通过减少冗余计算的有针对性更新，而非暴力优化，能够在大规模数据集蒸馏中弥合精度与效率之间的差距。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [9] [Visual Persuasion: What Influences Decisions of Vision-Language Models?](https://arxiv.org/abs/2602.15278)
*Manuel Cherep,Pranav M R,Pattie Maes,Nikhil Singh*

Main category: cs.CV

TL;DR: 提出一个框架来研究视觉语言模型（VLMs）的视觉偏好，通过系统编辑图像并观察模型选择行为，推断其潜在视觉效用函数，识别可能的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 网络图像越来越多地被VLMs解读和决策，但我们对这些AI代理的视觉偏好结构知之甚少，需要系统方法来理解其决策机制和潜在安全风险。

Method: 通过受控图像选择任务，系统扰动VLM输入；使用视觉提示优化方法，迭代生成视觉上合理的修改（如构图、照明、背景）；通过选择概率变化推断视觉效用函数；开发自动解释性管道识别偏好模式。

Result: 在主流VLM上进行大规模实验，优化编辑显著改变了头对头比较中的选择概率；自动解释性管道识别出驱动选择的一致视觉主题。

Conclusion: 该方法为揭示视觉漏洞提供实用高效途径，支持对基于图像的AI代理进行更主动的审计和治理，避免在真实环境中被动发现问题。

Abstract: The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.

</details>


### [10] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出一种用于流匹配视频生成器的联合采样框架，在保持时间一致性的同时提高批次多样性，避免视频解码和反向传播


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成成本高，通常每个提示只能生成少量样本。在低样本情况下，最大化每批次价值需要高跨视频多样性。现有方法虽然能提高图像生成的多样性，但在视频生成中往往会降低时间一致性，且需要昂贵的视频解码器反向传播

Method: 提出联合采样框架，应用多样性驱动的更新，然后移除会降低时间一致性目标的分量。为避免图像空间梯度，使用轻量级潜在空间模型计算两个目标，避免视频解码和解码器反向传播

Result: 在最先进的文本到视频流匹配模型上的实验显示，与强联合采样基线相比，多样性相当，同时显著提高了时间一致性和色彩自然度

Conclusion: 该方法能够在保持视频时间一致性的同时有效提高批次多样性，且计算成本较低，代码将开源发布

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [11] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: 提出一种无需训练、基于批处理的零样本异常检测框架，将2D基础模型扩展到3D脑MRI，通过聚合多轴切片构建局部体积token，实现体积异常检测。


<details>
  <summary>Details</summary>
Motivation: 当前零样本异常检测主要局限于2D数据集，扩展到3D医学图像面临挑战，现有方法依赖切片级特征和视觉语言模型，无法捕捉体积结构。

Method: 通过聚合多轴切片处理后的2D基础模型特征，构建局部体积token，恢复立方空间上下文，直接与基于距离的批级异常检测流程集成。

Result: 训练免费、批处理的零样本异常检测可以从2D编码器有效扩展到完整3D MRI体积，提供简单鲁棒的体积异常检测方法。

Conclusion: 该框架为3D脑MRI提供了一种无需微调、提示或监督的零样本异常检测解决方案，能够有效捕捉体积结构信息。

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [12] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: Sparrow框架通过视觉感知文本锚定窗口注意力、中间层视觉状态桥接和多token预测策略，解决了视频大语言模型中推测解码的性能崩溃问题，实现了2.82倍的平均加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码在视觉语言模型中广泛用于加速推理，但在视频大语言模型中面临严重性能崩溃问题。草案模型因键值缓存爆炸和上下文窗口不匹配而陷入注意力稀释和负视觉增益的陷阱。

Method: 1. 利用隐藏状态重用的视觉感知文本锚定窗口注意力，将视觉计算完全卸载到目标模型；2. 通过中间层视觉状态桥接训练草案模型，过滤低级视觉噪声；3. 引入多token预测策略来桥接训练-推理分布偏移。

Result: Sparrow在即使有25k视觉token的情况下也能实现平均2.82倍的加速，有效解决了长序列中的性能下降问题，为实时长视频任务提供了实用解决方案。

Conclusion: Sparrow框架通过视觉语义内部化现象的理解，解决了视频大语言模型中推测解码的性能崩溃问题，为实时长视频处理提供了高效推理方案。

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [13] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: EventMemAgent：基于分层记忆模块的主动在线视频理解框架，通过事件边界检测、事件粒度采样和结构化长期记忆解决无限视频流与有限模型上下文窗口的矛盾


<details>
  <summary>Details</summary>
Motivation: 在线视频理解面临无限视频流输入与多模态大语言模型有限上下文窗口的矛盾。现有被动处理方法需要在保持长程上下文和捕获细粒度细节之间权衡，难以满足复杂任务需求。

Method: 提出EventMemAgent主动在线视频代理框架：1) 短期记忆动态检测事件边界，使用事件粒度水库采样处理流视频帧；2) 长期记忆按事件结构化归档历史观察；3) 集成多粒度感知工具包进行主动迭代证据捕获；4) 使用Agentic RL端到端内化推理和工具使用策略。

Result: 在在线视频基准测试中取得了有竞争力的结果

Conclusion: EventMemAgent通过分层记忆和主动感知机制有效解决了在线视频理解中的无限流与有限上下文矛盾，为复杂视频理解任务提供了新框架

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [14] [Effective and Robust Multimodal Medical Image Analysis](https://arxiv.org/abs/2602.15346)
*Joy Dhar,Nayyar Zaidi,Maryam Haghighat*

Main category: cs.CV

TL;DR: 提出MAIL和Robust-MAIL网络，通过多注意力集成学习解决多模态融合在医学影像分析中的泛化性、计算效率和对抗鲁棒性问题，在20个公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合学习方法存在三个主要问题：1) 专注于特定模态，忽视跨模态的互补信息，限制了多疾病分析的泛化能力；2) 依赖计算昂贵的模型，在资源受限环境中应用受限；3) 缺乏对抗攻击的鲁棒性，影响医疗AI应用的可靠性。

Method: 提出MAIL网络，包含两个关键组件：1) 高效残差学习注意力块，用于捕捉细化的模态特定多尺度模式；2) 高效多模态交叉注意力模块，用于学习跨不同模态的丰富互补共享表示。为增强对抗鲁棒性，进一步提出Robust-MAIL，引入随机投影滤波器和调制注意力噪声。

Result: 在20个公开数据集上的广泛评估表明，MAIL和Robust-MAIL均优于现有方法，性能提升高达9.34%，同时计算成本降低高达78.3%。

Conclusion: 提出的MAIL和Robust-MAIL方法在多模态医学影像分析中表现出优越性，解决了现有方法的局限性，实现了更可靠、高效且鲁棒的预测性能。

Abstract: Multimodal Fusion Learning (MFL), leveraging disparate data from various imaging modalities (e.g., MRI, CT, SPECT), has shown great potential for addressing medical problems such as skin cancer and brain tumor prediction. However, existing MFL methods face three key limitations: a) they often specialize in specific modalities, and overlook effective shared complementary information across diverse modalities, hence limiting their generalizability for multi-disease analysis; b) they rely on computationally expensive models, restricting their applicability in resource-limited settings; and c) they lack robustness against adversarial attacks, compromising reliability in medical AI applications. To address these limitations, we propose a novel Multi-Attention Integration Learning (MAIL) network, incorporating two key components: a) an efficient residual learning attention block for capturing refined modality-specific multi-scale patterns and b) an efficient multimodal cross-attention module for learning enriched complementary shared representations across diverse modalities. Furthermore, to ensure adversarial robustness, we extend MAIL network to design Robust-MAIL by incorporating random projection filters and modulated attention noise. Extensive evaluations on 20 public datasets show that both MAIL and Robust-MAIL outperform existing methods, achieving performance gains of up to 9.34% while reducing computational costs by up to 78.3%. These results highlight the superiority of our approaches, ensuring more reliable predictions than top competitors. Code: https://github.com/misti1203/MAIL-Robust-MAIL.

</details>


### [15] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: CREMD数据集研究不同呈现模式（上下文、音频、视频）和标注者特征（养狗经验、性别、专业背景）如何影响狗情绪识别，发现视觉上下文显著提高标注一致性，音频增加标注者信心，但非养狗者和男性标注者表现出更高一致性。


<details>
  <summary>Details</summary>
Motivation: 狗情绪识别对于改善人-动物互动、兽医护理和自动化监测系统至关重要，但由于情绪评估的主观性和缺乏标准化方法，准确识别狗情绪具有挑战性。需要研究不同呈现模式和标注者特征如何影响情绪感知和标注。

Method: 创建CREMD数据集，包含923个视频片段，以三种模式呈现：无上下文无音频、有上下文无音频、有上下文有音频。收集来自不同背景参与者（养狗者、专业人士、不同人口统计特征）的标注，分析影响可靠狗情绪识别的因素。

Result: 1) 添加视觉上下文显著提高标注一致性，但音频线索结果不确定（因设计限制）；2) 非养狗者和男性标注者比养狗者和女性标注者表现出更高一致性，专业人士一致性更高；3) 音频存在显著增加标注者对特定情绪（特别是愤怒和恐惧）识别的信心。

Conclusion: 研究揭示了影响狗情绪识别可靠性的关键因素，强调了视觉上下文的重要性，挑战了关于标注者特征的先入之见，为未来数据集设计和标注策略提供了重要见解。

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [16] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: DAV-GSWT：利用扩散先验和主动视角采样，从少量输入观测合成高质量高斯泼溅Wang Tiles的数据高效框架


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法虽然能实现逼真神经渲染，但生成大规模场景时依赖密集采样的示例重建，数据需求量大。需要开发数据高效的方法来合成大规模虚拟环境。

Method: 结合扩散先验和主动视角采样，通过分层不确定性量化机制识别最有信息量的视角，同时利用生成扩散模型补全缺失的结构细节，确保瓦片间的无缝过渡。

Result: 实验结果表明，该系统显著减少了所需数据量，同时保持了大规模虚拟环境所需的视觉完整性和交互性能。

Conclusion: DAV-GSWT为大规模虚拟环境的创建提供了一种数据高效的高质量合成方案，通过扩散先验和主动采样实现了从少量观测到完整场景的生成。

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [17] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: 提出GMAIL框架，将生成图像视为独立模态而非简单替代真实图像，通过多模态学习在潜在空间对齐真实与生成图像，提升视觉语言任务性能


<details>
  <summary>Details</summary>
Motivation: 生成模型能合成高度真实图像，但直接将生成图像当作真实图像使用会导致模态差异和模式崩溃问题，需要更智能地利用生成图像

Method: 提出GMAIL框架：1) 使用跨模态对齐损失在生成图像上微调模型；2) 用对齐后的模型结合生成图像训练各种视觉语言模型；3) 在潜在空间而非像素空间对齐两种模态

Result: 显著提升图像描述、零样本图像检索、零样本图像分类和长描述检索任务性能；展示正生成数据缩放趋势；显著增强LLaVA等大型多模态模型的描述能力

Conclusion: 通过将生成图像视为独立模态并在潜在空间对齐，GMAIL框架能有效利用生成模型的优势，提升视觉语言任务性能，且易于与各种视觉语言模型集成

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [18] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: 提出一种新的无配对图像翻译框架，通过双头判别器和类别特定原型检测并抑制目标类别特征的幻觉，显著提升日到夜翻译质量


<details>
  <summary>Details</summary>
Motivation: 日到夜无配对图像翻译对下游任务很重要，但现有方法常产生语义幻觉（如错误合成交通标志、车辆和人造光效），严重影响下游性能

Method: 1) 双头判别器：同时进行语义分割以识别背景区域的幻觉内容；2) 类别特定原型：通过聚合目标域标注对象的特征构建，作为每类的语义锚点；3) 基于薛定谔桥的翻译模型进行迭代优化，将检测到的幻觉特征从类别原型推离

Result: 在BDD100K数据集上，日到夜域适应的mAP提升15.5%，对易产生幻觉的类别（如交通灯）增益达31.7%，在质量和数量上均优于现有方法

Conclusion: 通过检测和抑制目标类别特征的幻觉，提出的框架能有效保持对象语义，显著提升无配对日到夜图像翻译的质量和下游任务性能

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [19] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: ASBM是一种新的生成建模框架，通过两阶段方法学习最优轨迹：首先将Schrödinger Bridge前向动态视为耦合构建问题，然后通过简单匹配损失学习后向生成动态，从而产生更直、更高效的采样路径。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型由于采用无信息、无记忆的前向过程，导致高度弯曲的轨迹和噪声评分目标，采样效率低下。需要一种能够恢复高维最优轨迹的方法。

Method: 提出Adjoint Schrödinger Bridge Matching (ASBM)框架：1) 将SB前向动态视为耦合构建问题，通过数据到能量的采样视角学习，将数据传输到能量定义的先验；2) 使用由诱导最优耦合监督的简单匹配损失学习后向生成动态。

Result: ASBM在非无记忆机制下运行，产生显著更直、更高效的采样路径。相比先前工作，ASBM能扩展到高维数据，稳定性和效率显著提升。图像生成实验显示ASBM以更少采样步骤提高保真度，并通过蒸馏到一步生成器展示了最优轨迹的有效性。

Conclusion: ASBM通过两阶段Schrödinger Bridge方法有效解决了扩散模型轨迹弯曲和采样效率低的问题，在高维数据上实现了更稳定、高效的生成，为快速生成模型提供了新思路。

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [20] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: 首次系统评估开源多模态大语言模型在零样本单图像人脸伪造检测中的表现，LLaVA1.6-Mistral-7B在未微调情况下超越任务专用基线方法23%以上。


<details>
  <summary>Details</summary>
Motivation: 人脸伪造攻击威胁生物识别验证，现有伪造攻击检测系统需要任务特定训练且对未见攻击类型泛化能力差。开源多模态大语言模型展现出强大的视觉-语言推理能力，但在生物识别取证领域的潜力尚未充分探索。

Method: 首次系统评估开源多模态大语言模型在零样本单图像人脸伪造检测中的表现，使用公开可用的模型权重和标准化、可复现的评估协议，不进行任何微调或领域适应。

Result: 在多种伪造技术下，许多多模态大语言模型展现出非平凡的判别能力，LLaVA1.6-Mistral-7B达到最先进性能，在等错误率方面超越高度竞争的任务特定伪造攻击检测基线至少23%。

Conclusion: 多模态预训练能够隐式编码指示伪造伪影的细粒度面部不一致性，实现零样本取证敏感性。开源多模态大语言模型成为生物识别安全和取证图像分析的可复现、可解释且有竞争力的基础，为通过针对性微调或轻量级适应开发最先进伪造攻击检测系统提供了新机会。

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [21] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: 提出RPT-SR模型，通过区域先验注意力Transformer解决红外图像超分辨率问题，特别针对固定视角场景，利用场景布局先验提升性能


<details>
  <summary>Details</summary>
Motivation: 现有通用超分辨率模型（特别是Vision Transformers）在固定视角的红外成像场景（如监控、自动驾驶）中效率低下，未能利用这些场景中固有的强空间先验信息，导致冗余学习和次优性能

Method: 提出RPT-SR架构，采用双token框架：1）可学习的区域先验token作为场景全局结构的持久记忆；2）局部token捕获当前输入的帧特定内容。通过注意力机制将这些token融合，使先验能够动态调制局部重建过程

Result: 在覆盖长波（LWIR）和短波（SWIR）光谱的多样化数据集上建立了新的最先进性能，展示了RPT-SR的广泛适用性和多功能性

Conclusion: RPT-SR通过显式编码场景布局信息到注意力机制中，有效解决了固定视角红外图像超分辨率问题，相比通用模型具有更好的效率和性能

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [22] [LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction](https://arxiv.org/abs/2602.15493)
*Raffaele Cappelli,Matteo Ferrara*

Main category: cs.CV

TL;DR: LEADER是一个轻量级端到端的指纹细节点提取神经网络，无需预处理和后处理，仅用0.9M参数实现从原始指纹图像到细节点描述符的直接映射。


<details>
  <summary>Details</summary>
Motivation: 指纹识别中的细节点提取正转向深度学习，但真正消除单独预处理和后处理的端到端方法仍然稀缺。现有方法通常需要复杂的处理流程，缺乏完全端到端的解决方案。

Method: 提出LEADER架构，集成非极大值抑制和角度解码，采用新颖的"城堡-护城河-城墙"真值编码和双自编码器结构，通过注意力门控机制连接，实现完全端到端推理。

Result: 在NIST SD27数据集上比专门的潜在指纹细节点提取器F1分数高34%，在47%的样本中排名第一，推理速度GPU 15ms、CPU 322ms，计算效率优于领先商业软件。

Conclusion: LEADER实现了轻量级、完全端到端的指纹细节点提取，在精度、泛化能力和计算效率方面均达到最先进水平，学习到的内部表示与指纹领域特征一致。

Abstract: Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel "Castle-Moat-Rampart" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.

</details>


### [23] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: 提出基于语义过滤的框架，利用视觉语言模型实现类别感知的瞬态物体去除，解决3D高斯溅射中的鬼影伪影问题


<details>
  <summary>Details</summary>
Motivation: 多视角捕获中的瞬态物体（如行人、车辆）会在3D高斯溅射重建中产生鬼影伪影。现有方法要么依赖场景分解导致内存成本高，要么基于运动启发式方法容易受到视差模糊的影响

Method: 使用视觉语言模型（CLIP）进行类别感知的瞬态物体去除。通过计算渲染视图与干扰物文本提示之间的CLIP相似度得分，在训练迭代中为每个高斯累积得分。超过校准阈值的高斯进行透明度正则化和周期性剪枝

Result: 在RobustNeRF基准测试中，相比原始3DGS在四个序列上重建质量持续提升，同时保持最小内存开销和实时渲染性能。阈值校准和基线比较验证了语义指导在可预测干扰物类别场景中的实用性

Conclusion: 语义分类通过独立于运动模式识别物体类别，解决了视差模糊问题。该方法为具有可预测干扰物类别的场景中的瞬态物体去除提供了实用策略，优于基于运动的方法

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [24] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: 本文提出了一套全面的评估指标来衡量手势生物特征质量得分的好坏，超越了传统基于错误率的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有生物特征容量估计文献依赖错误率，但这些错误率不能反映得分质量的好坏，因此需要开发更全面的评估指标。

Method: 提出综合考虑排名顺序、得分相关性、趋势对应性和身份特征解缠等因素的评估框架，通过加权整合这些元素形成高级接受分数作为整体评估指标。

Result: 在三个数据集上对五个SOTA模型进行深入实验，结果显示使用本文提出的指标选择的最优得分比现有其他指标更合适，且与现有指标具有相关性，验证了其可靠性。

Conclusion: 本文提出的评估指标能够更全面地评估手势生物特征得分质量，为相关研究提供了有效的评估工具，代码已开源。

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [25] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出动态免训练的LoRA融合框架，通过特征层KL散度选择和指标引导的潜在调整，实现主题与风格的连贯合成


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法多采用静态统计启发式，偏离了LoRA学习自适应特征调整的初衷，且忽略了采样输入的随机性

Method: 1) 前向传播时在LoRA应用层动态计算基模型与主题/风格LoRA特征的KL散度，自适应选择融合权重；2) 反向去噪阶段通过CLIP/DINO等指标梯度修正生成轨迹；3) 特征层选择与指标引导调整在整个扩散时间线上集成

Result: 在多样化主题-风格组合上的实验表明，该方法在定性和定量上均优于现有最先进的LoRA融合方法

Conclusion: 通过动态特征选择和指标引导调整的互补机制，无需重新训练即可实现连贯的主题-风格合成

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [26] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出PADE方法，通过增强LVLMs内部正注意力动态来减少幻觉，无需训练即可提升视觉基础能力


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态推理方面表现出色，但容易产生幻觉，输出与视觉输入或用户指令不一致。现有免训练方法存在计算开销大、可能引入干扰、易受注意力下沉现象影响等问题

Method: 提出正注意力动态增强(PADE)：1)构建PAD图识别语义核心视觉区域；2)应用每头中位数绝对偏差缩放自适应控制干预强度；3)利用系统令牌补偿保持对复杂用户指令的注意力并支持长期输出一致性

Result: 在多个LVLMs和基准测试上的实验表明，PADE改善了视觉基础能力并减少了幻觉，验证了利用内部注意力动态进行可靠多模态推理的有效性

Conclusion: 通过利用LVLMs内部的正注意力动态，PADE提供了一种有效的免训练注意力干预方法，能够增强视觉基础并减少幻觉，为可靠的多模态推理提供了新思路

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [27] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: 提出全自动OCT图像血管分割与分类管道，结合预处理、聚类和机器学习分类器，达到99.68%的准确率


<details>
  <summary>Details</summary>
Motivation: 冠状动脉OCT成像虽能高分辨率显示血管解剖结构，但面临噪声、成像伪影和组织结构复杂等挑战，需要自动化解决方案

Method: 集成图像预处理、导丝伪影去除、极坐标-笛卡尔坐标转换、无监督K-means聚类和局部特征提取，使用逻辑回归和SVM进行像素级血管分类

Result: 实验结果显示优异性能，精确度、召回率和F1分数最高达1.00，总体分类准确率达99.68%，计算复杂度低且需要最少人工标注

Conclusion: 该方法为自动化OCT图像分析提供了可靠高效的解决方案，在临床决策支持和实时医学图像处理中具有应用潜力

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [28] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: IRIS-v2数据集：首个包含图像、点云、CAD模型、P&ID等完整工业场景数据的综合数据集，用于支持2D/3D功能示意图对齐研究，通过分割和图匹配方法减少对齐时间。


<details>
  <summary>Details</summary>
Motivation: 老旧工业设施缺乏原生数字模型，需要将功能示意图与2D/3D场景采集数据对齐以构建数字孪生。当前基于图像和LiDAR的手动对齐方法因工业现场的复杂性和繁琐性而难以扩展，且示意图与现实不一致、公开工业数据集稀缺，使该问题既具挑战性又研究不足。

Method: 提出IRIS-v2综合数据集，包含图像、点云、2D标注框和分割掩码、CAD模型、3D管道布线信息以及P&ID图。在实际案例研究中，结合分割和图匹配方法进行对齐实验。

Result: IRIS-v2是首个为工业场景功能示意图对齐研究提供的全面数据集，包含多种数据模态，为后续研究提供了基础资源。

Conclusion: IRIS-v2数据集填补了工业场景公开数据集的空白，通过分割和图匹配相结合的方法，有望显著减少功能示意图与场景数据对齐所需的时间，推动数字孪生技术在老旧工业设施中的应用。

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [29] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: CEMRAG框架通过将视觉表示分解为可解释的临床概念并与多模态RAG结合，统一提升放射学报告生成的解释性和事实准确性，挑战了传统认为解释性与性能之间存在权衡的观点。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的放射学报告生成存在解释性不足和幻觉问题，临床采用受限。现有研究通常将解释性和准确性作为独立目标处理，缺乏统一解决方案。

Method: 提出概念增强多模态RAG（CEMRAG）框架：1）将视觉表示分解为可解释的临床概念；2）将这些概念与多模态检索增强生成结合；3）利用丰富的上下文提示改进放射学报告生成。

Result: 在MIMIC-CXR和IU X-Ray数据集上，跨多种VLM架构、训练机制和检索配置的实验显示，CEMRAG在临床准确性指标和标准NLP指标上均优于传统RAG和仅概念基线方法。

Conclusion: 透明视觉概念可以增强而非损害医学VLM的诊断准确性，挑战了解释性与性能之间的传统权衡假设。模块化设计为临床可信赖的AI辅助放射学提供了原则性路径。

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [30] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: 本研究提出一个新的公开草莓成熟度数据集，包含566张图像和1201个标注对象，并在YOLO系列模型上进行测试，YOLOv8s在mAP@50指标上表现最佳（86.09%）。


<details>
  <summary>Details</summary>
Motivation: 草莓成熟度判断对生产者和消费者都至关重要，但传统视觉评估方法主观性强、误差大。现有研究中缺乏公开全面的数据集，难以进行有效比较。

Method: 创建了一个新的公开草莓成熟度数据集，包含在土耳其两个不同温室中采集的566张图像和1201个标注对象。使用YOLOv8、YOLOv9和YOLO11系列模型进行对比测试。

Result: YOLOv9c模型的精确度最高（90.94%），YOLO11s模型的召回率最高（83.74%），而YOLOv8s模型在mAP@50指标上表现最佳（86.09%）。

Conclusion: 中小型模型在这类数据集上表现更平衡高效，为智慧农业应用建立了基础参考点。公开数据集有助于推动该领域研究。

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [31] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: 提出3D数据分析优化流程，通过两阶段贝叶斯优化自动选择分割模型、优化参数和分类器设计，减少人工调参负担


<details>
  <summary>Details</summary>
Motivation: 在3D生物医学图像分析中，虽然深度学习分割和分类方法众多，但选择合适的模型和调参仍然是实践中的主要瓶颈，需要自动化解决方案

Method: 开发两阶段贝叶斯优化流程：第一阶段选择分割模型并优化后处理参数，使用领域适应的合成基准数据集和分割质量指标；第二阶段优化分类器设计选择，包括编码器、分类头架构、先验知识整合和预训练策略，并包含辅助类别标注工作流程

Result: 在四个案例研究中，该流程能够高效地为各个数据集识别出有效的模型和参数配置

Conclusion: 3D数据分析优化流程通过自动化模型选择和参数优化，解决了生物医学图像分析中的实践瓶颈，减少了人工标注和调参工作量

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [32] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: 提出"标准优先、语义后置"的图像分析新范式，将结构发现与语义标注分离，以应对科学发现中的标签漂移和跨域可比性问题。


<details>
  <summary>Details</summary>
Motivation: 传统语义优先的图像分析范式在开放科学发现、跨传感器/跨站点可比性、长期监测等场景下存在系统性失败，因为领域本体和标签集会随时间发生文化、制度和生态漂移。

Method: 引入统一的标准优先结构发现框架：首先基于明确优化标准进行语义无关的结构提取，产生稳定分区、结构场或层次；然后将语义映射作为下游步骤，将发现的结构映射到领域本体或词汇表。

Result: 该框架提供了跨图像科学的可复现分析支架，支持多种解释和明确的跨域映射，无需重写上游提取过程。基于控制论、观察即区分和信息论的信息与意义分离原则。

Conclusion: 标准优先方法为超越类别准确度的验证提供了基础，并将结构产品视为FAIR、AI就绪的数字对象，适用于长期监测和数字孪生应用。

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [33] [ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT](https://arxiv.org/abs/2602.15720)
*Hyunchan Moon,Cheonjun Park,Steven L. Waslander*

Main category: cs.CV

TL;DR: ToaSt是一个解耦的ViT压缩框架，对多头自注意力模块采用耦合头结构化剪枝，对前馈网络采用令牌通道选择，在保持精度的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers虽然在各种视觉任务中表现出色，但其高昂的计算成本阻碍了实际部署。现有的结构化权重剪枝和令牌压缩方法分别存在重训练时间长和全局传播导致的优化问题。

Method: 提出ToaSt解耦框架：1）对多头自注意力模块采用耦合头结构化剪枝，利用注意力操作特性增强鲁棒性；2）对占FLOPs 60%以上的前馈网络，引入令牌通道选择方法，避免全局传播问题，有效过滤冗余噪声。

Result: 在9个不同模型（包括DeiT、ViT-MAE、Swin Transformer）上的评估显示，ToaSt在精度和效率之间取得了优越的平衡。在ViT-MAE-Huge上实现了88.52%的准确率（提升1.64%），同时减少39.4%的FLOPs。在下游任务中，在COCO目标检测上达到52.2 mAP（vs 51.9 mAP）。

Conclusion: ToaSt通过解耦策略为ViT的不同组件设计专门的压缩方法，有效解决了现有压缩技术的局限性，在保持甚至提升精度的同时显著降低了计算成本，并能有效迁移到下游任务。

Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\% accuracy (+1.64 \%) with 39.4\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.

</details>


### [34] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: 提出检索增强框架提升基于大语言模型的视觉语言导航效率，通过指令级轨迹检索和候选方向剪枝减少LLM推理负担，无需微调模型。


<details>
  <summary>Details</summary>
Motivation: 基于提示的LLM导航存在决策效率低下的问题，需要反复从头解释指令并在每个步骤处理嘈杂冗长的导航候选，导致推理成本高且不稳定。

Method: 提出两级检索增强框架：1) 指令级嵌入检索器选择语义相似的成功导航轨迹作为上下文示例，提供任务特定先验；2) 模仿学习的候选检索器在LLM推理前剪枝无关导航方向，减少动作歧义和提示复杂度。

Result: 在Room-to-Room基准测试中，在已见和未见环境上均显著提升了成功率、Oracle成功率和SPL指标。消融研究表明指令级示例检索和候选剪枝对全局指导和逐步决策效率具有互补效益。

Conclusion: 检索增强决策支持是提升基于LLM的视觉语言导航的有效且可扩展策略，轻量级模块化设计无需修改或微调底层语言模型。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [35] [Spanning the Visual Analogy Space with a Weight Basis of LoRAs](https://arxiv.org/abs/2602.15727)
*Hila Manor,Rinon Gal,Haggai Maron,Tomer Michaeli,Gal Chechik*

Main category: cs.CV

TL;DR: LoRWeB：通过动态组合学习到的LoRA基模块实现视觉类比学习，提升未见视觉变换的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一LoRA模块来捕捉视觉变换，但固定模块限制了泛化能力。受限于LoRA在受限领域中能形成有意义、可插值语义空间的启发，需要更灵活的方法来处理多样化的视觉变换。

Method: 提出LoRWeB方法：1）学习一组LoRA基模块，覆盖不同的视觉变换空间；2）轻量级编码器根据输入类比对动态选择和加权这些基模块，在推理时通过动态组合学习到的变换基元来专门化模型。

Result: 综合评估表明该方法达到最先进性能，并显著提升对未见视觉变换的泛化能力。代码和数据已开源。

Conclusion: LoRA基分解是实现灵活视觉操纵的有前景方向，通过动态组合学习到的变换基元能更好地处理多样化的视觉类比任务。

Abstract: Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}'$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}'$ such that $\mathbf{a} : \mathbf{a}' :: \mathbf{b} : \mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a "space of LoRAs". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb

</details>


### [36] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: 提出了一种基于语言和几何的稀疏体素表示方法，在统一框架中协同建模3D场景的外观、语义和几何，实现整体场景理解和重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D开放词汇场景理解方法主要关注从2D基础模型蒸馏语言特征到3D特征场，但忽略了场景外观、语义和几何之间的协同作用，导致场景理解偏离底层几何结构并与重建过程脱节。

Method: 使用3D稀疏体素作为基元，构建外观场、密度场、特征场和置信度场来整体表示3D场景。通过特征调制模块促进各场之间的协同，从2D基础模型蒸馏语言特征。同时通过深度相关性正则化和模式一致性正则化，将几何基础模型的几何知识蒸馏到特征场中。

Result: 大量实验表明，该方法在整体场景理解和重建方面相比最先进方法实现了优越的综合性能。

Conclusion: 提出的统一框架能够协同建模3D场景的外观、语义和几何，解决了现有方法中场景理解与几何结构脱节的问题，实现了更好的整体场景理解和重建效果。

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [37] [RaCo: Ranking and Covariance for Practical Learned Keypoints](https://arxiv.org/abs/2602.15755)
*Abhiram Shenoi,Philipp Lindenberger,Paul-Edouard Sarlin,Marc Pollefeys*

Main category: cs.CV

TL;DR: RaCo是一个轻量级神经网络，通过学习鲁棒且通用的关键点来支持多种3D计算机视觉任务，无需共视图像对，仅使用透视图像裁剪进行训练。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统关键点检测方法在旋转鲁棒性、匹配效率和空间不确定性量化方面的不足，同时避免使用计算昂贵的等变网络架构。

Method: 整合三个核心组件：可重复关键点检测器、可微分排序器（在有限关键点数量下最大化匹配）、协方差估计器（量化度量尺度空间不确定性），通过大量数据增强实现旋转鲁棒性。

Result: 在多个挑战性数据集上达到最先进的性能，特别是在关键点可重复性和两视图匹配方面，对大平面内旋转表现出色。

Conclusion: RaCo提供了一种有效且简单的策略，无需额外标签即可独立估计关键点排序和度量协方差，检测可解释且可重复的兴趣点。

Abstract: This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.

</details>


### [38] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: 提出R3框架解决多模态模型中生成与理解能力的权衡问题，通过"生成-理解-再生成"的多步过程，利用理解能力提升生成效果，同时增强与生成相关的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型面临一个关键挑战：提升生成能力往往以牺牲理解为代价，反之亦然。研究发现这种权衡的主要原因是生成与理解之间可能存在冲突，在模型内部形成竞争动态。

Method: 提出Reason-Reflect-Refine (R3)框架，将单步生成任务重构为"生成-理解-再生成"的多步过程。该算法在生成过程中显式利用模型的理解能力，缓解优化困境。

Result: 成功缓解了优化困境，实现了更强的生成结果，并提升了与生成过程相关的理解能力。为设计下一代统一多模态模型提供了有价值的见解。

Conclusion: R3框架通过将生成任务重构为多步过程，有效解决了多模态模型中生成与理解能力的权衡问题，为下一代统一多模态模型的设计提供了新思路。

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [39] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: NeRFscopy：基于神经渲染的自监督管道，用于从单目内窥镜视频进行可变形组织的新视角合成和3D重建


<details>
  <summary>Details</summary>
Motivation: 内窥镜在医学成像中至关重要，但现有方法面临组织可变形、单目相机、光照变化、遮挡和未知相机轨迹等挑战。需要开发鲁棒的动态3D重建管道来增强可视化、提高诊断准确性、辅助治疗规划和指导手术。

Method: 提出NeRFscopy自监督管道，包含一个具有规范辐射场和时间相关变形场的可变形模型，变形场通过SE(3)变换参数化。高效利用彩色图像，引入复杂项来学习3D隐式模型，无需任何模板或预训练模型，仅从数据学习。

Result: NeRFscopy在新视角合成方面取得准确结果，在各种具有挑战性的内窥镜场景中优于竞争方法。

Conclusion: NeRFscopy为内窥镜视频的可变形组织重建提供了有效的自监督解决方案，能够应对医学内窥镜成像中的多种挑战，具有临床应用潜力。

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [40] [Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting](https://arxiv.org/abs/2602.15782)
*Ines Montoya-Espinagosa,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一种结合天空图像、光伏历史数据和气象数据的混合多模态方法，用于短期和长期光伏预测，特别关注云层条件下的预测鲁棒性和爬坡事件预测准确性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源特别是太阳能的广泛应用，光伏发电的波动性给电网运营带来挑战。需要改进预测方法以应对光伏发电的变异性，提高云层条件下的预测鲁棒性，并扩展超越即时预测的能力。

Method: 采用混合多模态方法，结合天空图像、光伏历史数据和气象数据。使用深度神经网络模型，包含单独和多个气象变量，以及太阳位置分析。针对即时预测和长期预测任务分别开发解决方案。

Result: 气象数据（特别是地表长波辐射、向下辐射以及风与太阳位置的组合）显著提高了即时预测和长期预测的准确性，尤其在多云天气条件下效果明显。研究证明了整合多样化数据源对提高预测可靠性和可解释性的重要性。

Conclusion: 整合天空图像、光伏历史数据和气象数据的多模态方法能有效提高光伏预测的准确性和鲁棒性，特别是在应对云层变化和爬坡事件方面。这种数据融合方法为电网高效运营和太阳能变异性管理提供了更好的支持。

Abstract: Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.

</details>


### [41] [Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers](https://arxiv.org/abs/2602.15783)
*Lucas Sancéré,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 提出使用可扩展的图变换器在完整WSI细胞图上进行分类，在皮肤鳞状细胞癌中区分健康与肿瘤上皮细胞，相比基于图像的方法获得更好性能。


<details>
  <summary>Details</summary>
Motivation: 全切片图像包含丰富的医学诊断信息，但现有深度学习方法依赖基于patch的表示，丢失了重要的组织级上下文信息。特别是在皮肤鳞状细胞癌中，健康与肿瘤上皮细胞形态相似，基于图像的方法难以区分。

Method: 使用可扩展的图变换器（SGFormer和DIFFormer）在完整WSI细胞图上进行分类。构建细胞图，节点表示细胞，边表示细胞间关系。评估了多种节点特征配置，发现最有效的表示结合了形态学、纹理特征以及非上皮细胞的细胞类别。

Result: 在单个WSI上，图变换器模型SGFormer和DIFFormer分别达到85.2±1.5和85.1±2.5的平衡准确率，而最佳基于图像的方法为81.2±3.0。在多患者多WSI设置中，DIFFormer达到83.6±1.9的平衡准确率，而最先进的基于图像模型CellViT256为78.1±0.5。

Conclusion: 图变换器在WSI细胞分类任务中优于基于图像的方法，特别是在细胞形态相似的情况下。周围细胞上下文信息对准确分类至关重要，图表示能有效捕捉组织级结构信息。

Abstract: Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.

</details>


### [42] [Task-Agnostic Continual Learning for Chest Radiograph Classification](https://arxiv.org/abs/2602.15811)
*Muthu Subash Kavitha,Anas Zafar,Amgad Muneer,Jia Wu*

Main category: cs.CV

TL;DR: CARL-XRay：用于胸部X光分类的持续学习框架，通过轻量级适配器和原型回放实现任务增量学习，无需存储原始图像或重新训练历史数据。


<details>
  <summary>Details</summary>
Motivation: 临床部署胸部X光分类器需要能够随着新数据集可用而更新的模型，无需重新训练先前观察到的数据或降低已验证性能。现有方法在任务标识符不可用的增量学习场景中存在挑战。

Method: 提出CARL-XRay框架：保持固定高容量主干网络，增量分配轻量级任务特定适配器和分类器头。使用潜在任务选择器在任务适应特征上操作，通过紧凑原型和特征级经验回放保留历史上下文。

Result: 在大规模公共胸部X光数据集上，CARL-XRay在任务未知部署下优于联合训练，路由准确率达75.0%（vs. 62.5%），AUROC在oracle设置下为0.74，任务未知推理下为0.75，使用显著更少的可训练参数。

Conclusion: 该框架为临床持续部署提供了实用的替代方案，避免了联合训练和重复完全重新训练，支持稳定的任务识别和适应，同时避免原始图像存储。

Abstract: Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.

</details>


### [43] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: 提出一种数据高效的时序草图生成方法，通过适配预训练文本到视频扩散模型来生成绘图过程，结合LLM的语义规划和视频扩散模型的渲染能力


<details>
  <summary>Details</summary>
Motivation: 现有生成模型将草图视为静态图像，忽略了绘图过程中的时序结构，而草图绘制本质上是顺序过程，笔划顺序对创意探索至关重要

Method: 将草图表示为短视频，笔划在空白画布上逐步绘制；采用两阶段微调策略：第一阶段使用合成形状组合学习笔划顺序，第二阶段用少量人工绘制草图数据学习视觉外观

Result: 尽管仅使用极少量人工草图数据（最少7个），方法能生成高质量的时序草图，准确遵循文本指定的顺序，同时展现丰富的视觉细节

Conclusion: 结合LLM的语义规划和视频扩散模型的渲染能力，实现了数据高效的时序草图生成，支持笔刷风格控制和自回归生成，为交互式协作绘图提供了灵活性

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [Attention-gated U-Net model for semantic segmentation of brain tumors and feature extraction for survival prognosis](https://arxiv.org/abs/2602.15067)
*Rut Pate,Snehal Rajput,Mehul S. Raval,Rupal A. Kapdi,Mohendra Roy*

Main category: cs.AI

TL;DR: 本文提出了一种基于注意力门控循环残差U-Net的Triplanar（2.5D）模型，用于改进脑肿瘤分割和生存预测，在BraTS2021验证集上获得0.900的Dice相似性分数。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤作为最常见的原发性脑肿瘤，具有侵袭性、预后和组织学特征的广泛差异，复杂且耗时的外科手术干预使得治疗具有挑战性，需要更精确的肿瘤分割方法来辅助治疗规划。

Method: 提出注意力门控循环残差U-Net（R2U-Net）为基础的Triplanar（2.5D）模型，整合残差、循环和triplanar架构以增强特征表示和分割精度，同时保持计算效率。Triplanar网络从每个平面模型提取64个特征用于生存天数预测，通过人工神经网络（ANN）将特征减少到28个。

Result: 在BraTS2021验证集上，全肿瘤（WT）分割的Dice相似性分数达到0.900，性能与领先模型相当。生存预测方面，测试数据集上获得45.71%的准确率、108,318.128的均方误差和0.338的斯皮尔曼等级相关系数。

Conclusion: 提出的注意力门控循环残差U-Net Triplanar模型在脑肿瘤分割方面表现出色，分割精度与最先进模型相当，同时通过triplanar特征提取为生存预测提供了有前景的方法，有助于改善治疗规划。

Abstract: Gliomas, among the most common primary brain tumors, vary widely in aggressiveness, prognosis, and histology, making treatment challenging due to complex and time-intensive surgical interventions. This study presents an Attention-Gated Recurrent Residual U-Net (R2U-Net) based Triplanar (2.5D) model for improved brain tumor segmentation. The proposed model enhances feature representation and segmentation accuracy by integrating residual, recurrent, and triplanar architectures while maintaining computational efficiency, potentially aiding in better treatment planning. The proposed method achieves a Dice Similarity Score (DSC) of 0.900 for Whole Tumor (WT) segmentation on the BraTS2021 validation set, demonstrating performance comparable to leading models. Additionally, the triplanar network extracts 64 features per planar model for survival days prediction, which are reduced to 28 using an Artificial Neural Network (ANN). This approach achieves an accuracy of 45.71%, a Mean Squared Error (MSE) of 108,318.128, and a Spearman Rank Correlation Coefficient (SRC) of 0.338 on the test dataset.

</details>


### [45] [ResearchGym: Evaluating Language Model Agents on Real-World AI Research](https://arxiv.org/abs/2602.15112)
*Aniketh Garikaparthi,Manasi Patwardhan,Arman Cohan*

Main category: cs.AI

TL;DR: ResearchGym是一个用于评估AI代理端到端研究能力的基准测试和执行环境，包含5个论文任务环境共39个子任务。GPT-5代理仅完成26.5%的子任务，在15次评估中只有1次超越基线（提升11.5%），显示出能力-可靠性差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估AI代理进行完整研究流程（提出假设、运行实验、超越人类基线）的基准环境。需要量化AI代理在端到端研究任务中的实际表现，识别其长期失败模式。

Method: 从ICML、ICLR和ACL会议中选取5篇口头/焦点论文，保留其数据集、评估框架和基线实现，但隐藏论文提出的方法。构建5个容器化任务环境（共39个子任务），要求代理提出新假设、运行实验并超越论文指标上的人类基线。

Result: GPT-5代理平均仅完成26.5%的子任务，在15次评估中只有1次（6.7%）超越基线（提升11.5%）。识别出缺乏耐心、资源管理差、对弱假设过度自信、并行实验协调困难、上下文长度限制等长期失败模式。但在单次运行中，代理成功超越了ICML 2025焦点任务的解决方案。

Conclusion: 前沿AI代理偶尔能达到最先进性能，但可靠性很低。ResearchGym为系统评估和分析自主代理在闭环研究中的表现提供了基础设施，揭示了当前AI代理在端到端研究任务中存在的显著能力-可靠性差距。

Abstract: We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.

</details>


### [46] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 论文提出通过修改教师模型生成的推理轨迹来防止未经授权的知识蒸馏，实现反蒸馏和API水印两种目标，同时保持答案正确性。


<details>
  <summary>Details</summary>
Motivation: 未经授权的知识蒸馏会不公平地利用前沿模型的开发成本和努力，需要保护模型知识产权，防止他人通过蒸馏技术非法获取模型能力。

Method: 引入动态重写教师模型推理输出的方法：1）基于LLM重写能力的指令重写方法；2）基于梯度的技术。这些方法在保持答案正确性和语义连贯性的同时修改推理轨迹。

Result: 简单的指令重写方法能有效实现反蒸馏效果，同时保持甚至提升教师模型性能；重写方法还能实现高可靠的水印检测，几乎没有误报。

Conclusion: 通过重写推理轨迹可以有效防止未经授权的知识蒸馏，实现模型知识产权保护，同时保持模型性能，为模型安全部署提供实用解决方案。

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [47] [Panini: Continual Learning in Token Space via Structured Memory](https://arxiv.org/abs/2602.15156)
*Shreyas Rajesh,Pavan Holur,Mehmet Yigit Turali,Chenda Duan,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: Panini提出了一种非参数持续学习框架，将文档表示为生成语义工作空间（GSW），通过实体和事件感知的问答对网络来积累和整合知识，相比传统RAG方法在推理效率和准确性上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）方法存在两个主要问题：1）测试时计算效率低，LLM需要重复处理相同文档；2）分块检索可能引入无关上下文，导致生成不支持的答案。需要一种更高效、更可靠的持续学习方法来处理新文档和知识。

Method: 提出Panini框架，将文档表示为生成语义工作空间（GSW）——一个实体和事件感知的问答对网络。基础模型保持不变，学习通过将每个新经验整合到外部语义记忆状态中实现，该状态持续积累和整合。给定查询时，Panini仅遍历持续更新的GSW（而非原始文档或分块），检索最可能的推理链。

Result: 在六个QA基准测试中，Panini实现了最高的平均性能，比其他竞争基线高出5%-7%，同时使用2-30倍更少的答案上下文标记，支持完全开源管道，并在精心策划的不可回答查询上减少了不支持的答案。

Conclusion: 在写入时高效准确地构建经验（如GSW框架所实现的）在读取时既能提高效率又能增强可靠性。该方法展示了非参数持续学习的有效性，为语言模型处理新知识和用户特定数据提供了更优方案。

Abstract: Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.

</details>


### [48] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: 本文提出了一种基于da Costian-Tarskianism的新型本体异质性方法，使用扩展后果系统和扩展开发图来关联本体


<details>
  <summary>Details</summary>
Motivation: 解决本体异质性问题，为不同本体系统之间的关联和整合提供理论基础

Method: 基于Carnapian-Goguenism，采用da Costian-Tarskianism方法，使用后果系统机制，引入扩展后果系统（包含本体公理）和扩展开发图（通过扩展后果系统的态射、纤维化和分裂等操作关联本体）

Result: 建立了扩展后果系统和扩展开发图的理论框架，为处理本体异质性提供了新的数学工具

Conclusion: 该方法对应用本体论领域具有重要意义，为未来研究提供了新的方向

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [49] [Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173)
*Luise Ge,Yongyan Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该研究比较了20个前沿和开源大语言模型在风险决策中的表现，发现LLM可分为推理模型和对话模型两类，前者更理性，后者更接近人类但理性程度较低。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在决策支持和智能体工作流中应用广泛，但对其在不确定性下的决策机制理解有限。研究旨在系统比较LLM的风险选择行为，探索其决策模式与人类和理性模型的差异。

Method: 研究从两个维度分析LLM风险决策：(1)前景呈现方式（显式vs经验基础）和(2)决策理由（解释）。涉及20个前沿和开源LLM，并辅以匹配的人类被试实验作为参考，同时以期望收益最大化的理性智能体模型作为另一参照。

Result: LLM可分为两类：推理模型（RMs）和对话模型（CMs）。RMs更接近理性行为，对前景顺序、得失框架和解释不敏感，在显式和经验式前景呈现下表现一致。CMs理性程度显著较低，更接近人类，对前景顺序、框架和解释敏感，且存在较大的描述-经验差距。开源LLM的配对比较表明，数学推理训练是区分RMs和CMs的关键因素。

Conclusion: LLM在风险决策中表现出明显的分化模式，训练目标（特别是数学推理能力）显著影响其决策行为。理解这种分化对于设计可靠的LLM决策系统至关重要，特别是在需要处理不确定性的应用场景中。

Abstract: The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

</details>


### [50] [Secure and Energy-Efficient Wireless Agentic AI Networks](https://arxiv.org/abs/2602.15212)
*Yuanyan Song,Kezhi Wang,Xinmian Xu*

Main category: cs.AI

TL;DR: 提出一个安全的无线代理AI网络，包含一个监督AI代理和多个其他AI代理，通过动态分配代理参与协作推理，未被选中的代理作为友好干扰器来保护隐私，同时优化能耗以延长服务时间。


<details>
  <summary>Details</summary>
Motivation: 在无线环境中为用户的推理任务提供服务质量保障，同时确保私有知识和推理结果的机密性，并解决AI代理的能耗问题以延长服务时间。

Method: 提出ASC和LAW两种资源分配方案：ASC使用ADMM、SDR和SCA算法迭代优化三个子问题；LAW使用基于大语言模型的优化器在代理工作流中处理子问题。

Result: 实验结果显示，相比其他基准方案，所提方案能降低网络能耗高达59.1%，并在基于Qwen的实际代理AI系统中验证了在各种公共基准测试中达到满意的推理准确率。

Conclusion: 提出的安全无线代理AI网络框架能有效保护隐私、优化能耗，并通过ASC和LAW两种方案实现了显著的性能提升，在实际系统中验证了可行性。

Abstract: In this paper, we introduce a secure wireless agentic AI network comprising one supervisor AI agent and multiple other AI agents to provision quality of service (QoS) for users' reasoning tasks while ensuring confidentiality of private knowledge and reasoning outcomes. Specifically, the supervisor AI agent can dynamically assign other AI agents to participate in cooperative reasoning, while the unselected AI agents act as friendly jammers to degrade the eavesdropper's interception performance. To extend the service duration of AI agents, an energy minimization problem is formulated that jointly optimizes AI agent selection, base station (BS) beamforming, and AI agent transmission power, subject to latency and reasoning accuracy constraints. To address the formulated problem, we propose two resource allocation schemes, ASC and LAW, which first decompose it into three sub-problems. Specifically, ASC optimizes each sub-problem iteratively using the proposed alternating direction method of multipliers (ADMM)-based algorithm, semi-definite relaxation (SDR), and successive convex approximation (SCA), while LAW tackles each sub-problem using the proposed large language model (LLM) optimizer within an agentic workflow. The experimental results show that the proposed solutions can reduce network energy consumption by up to 59.1% compared to other benchmark schemes. Furthermore, the proposed schemes are validated using a practical agentic AI system based on Qwen, demonstrating satisfactory reasoning accuracy across various public benchmarks.

</details>


### [51] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: 论文提出AI/机器学习框架补充确定性算法，预测供应链金融中的发票稀释风险，使用实时动态信用限额替代传统不可撤销付款承诺。


<details>
  <summary>Details</summary>
Motivation: 发票稀释（批准金额与实际收款之间的差额）是供应链金融中非信用风险和利润损失的重要来源。传统依赖买方不可撤销付款承诺（IPU）的方法阻碍了供应链金融的采用，特别是对于次级投资级买方。需要更灵活的数据驱动方法来管理这一风险。

Method: 提出AI/机器学习框架，补充确定性算法来预测发票稀释。使用实时动态信用限额方法，为每个买方-供应商对实时预测稀释风险。基于包含九个关键交易字段的广泛生产数据集进行评估。

Result: 论文评估了AI/机器学习框架在预测发票稀释方面的效果，但摘要中未提供具体数值结果。该方法旨在通过数据驱动方式管理稀释风险，提高供应链金融的采用率。

Conclusion: AI/机器学习框架可以补充传统确定性算法，为供应链金融中的发票稀释风险提供更灵活、数据驱动的管理方法，有助于克服传统IPU方法的局限性，促进供应链金融的广泛采用。

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [52] [Enhancing Diversity and Feasibility: Joint Population Synthesis from Multi-source Data Using Generative Models](https://arxiv.org/abs/2602.15270)
*Farbod Abbasi,Zachary Patterson,Bilal Farooq*

Main category: cs.AI

TL;DR: 提出一种基于WGAN-GP的多源数据联合学习方法来生成合成人口，通过逆梯度惩罚正则化提升多样性和可行性，显著优于传统顺序方法。


<details>
  <summary>Details</summary>
Motivation: 当前合成人口生成方法存在两大局限：1）依赖单一数据集或顺序数据融合，无法捕捉特征间复杂交互；2）难以处理采样零值（有效但未观测的组合）和结构零值（逻辑上不可行的组合），导致生成数据多样性和可行性不足。

Method: 提出基于Wasserstein GAN with gradient penalty (WGAN-GP)的多源数据联合学习方法，在生成器损失函数中引入逆梯度惩罚正则化项，同时整合多源数据集进行联合学习，而非传统的顺序处理。

Result: 联合方法相比顺序基线，召回率提升7%，精确率提升15%。正则化项进一步将召回率提升10%，精确率提升1%。相似性分布评估中，联合方法得分为88.1，优于顺序方法的84.6。

Conclusion: 提出的多源联合生成方法能显著提升合成人口的多样性和可行性，从而增强基于代理模型（ABM）的准确性和可靠性，为交通和城市规划提供更高质量的输入数据。

Abstract: Generating realistic synthetic populations is essential for agent-based models (ABM) in transportation and urban planning. Current methods face two major limitations. First, many rely on a single dataset or follow a sequential data fusion and generation process, which means they fail to capture the complex interplay between features. Second, these approaches struggle with sampling zeros (valid but unobserved attribute combinations) and structural zeros (infeasible combinations due to logical constraints), which reduce the diversity and feasibility of the generated data. This study proposes a novel method to simultaneously integrate and synthesize multi-source datasets using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty. This joint learning method improves both the diversity and feasibility of synthetic data by defining a regularization term (inverse gradient penalty) for the generator loss function. For the evaluation, we implement a unified evaluation metric for similarity, and place special emphasis on measuring diversity and feasibility through recall, precision, and the F1 score. Results show that the proposed joint approach outperforms the sequential baseline, with recall increasing by 7\% and precision by 15\%. Additionally, the regularization term further improves diversity and feasibility, reflected in a 10\% increase in recall and 1\% in precision. We assess similarity distributions using a five-metric score. The joint approach performs better overall, and reaches a score of 88.1 compared to 84.6 for the sequential method. Since synthetic populations serve as a key input for ABM, this multi-source generative approach has the potential to significantly enhance the accuracy and reliability of ABM.

</details>


### [53] [When Remembering and Planning are Worth it: Navigating under Change](https://arxiv.org/abs/2602.15274)
*Omid Madani,J. Brian Burns,Reza Eghbali,Thomas L. Dean*

Main category: cs.AI

TL;DR: 研究不同记忆类型在动态不确定环境中如何辅助空间导航，发现结合多种策略的架构能显著提升导航效率


<details>
  <summary>Details</summary>
Motivation: 探索在动态变化、感知受限的不确定环境中，记忆如何帮助智能体进行有效的空间导航，解决地图构建和路径规划面临的挑战

Method: 研究从简单到复杂的多种策略，包括使用非平稳概率学习技术更新情景记忆，基于记忆构建即时地图并进行路径规划

Result: 能够结合多种策略的架构在处理不同性质子任务时表现更好，当任务难度增加时，使用记忆的智能体比简单智能体效率显著提升

Conclusion: 在不确定性不过大的情况下，利用记忆构建即时地图并进行规划的方法能有效提升动态不确定环境中的导航效率

Abstract: We explore how different types and uses of memory can aid spatial navigation in changing uncertain environments. In the simple foraging task we study, every day, our agent has to find its way from its home, through barriers, to food. Moreover, the world is non-stationary: from day to day, the location of the barriers and food may change, and the agent's sensing such as its location information is uncertain and very limited. Any model construction, such as a map, and use, such as planning, needs to be robust against these challenges, and if any learning is to be useful, it needs to be adequately fast. We look at a range of strategies, from simple to sophisticated, with various uses of memory and learning. We find that an architecture that can incorporate multiple strategies is required to handle (sub)tasks of a different nature, in particular for exploration and search, when food location is not known, and for planning a good path to a remembered (likely) food location. An agent that utilizes non-stationary probability learning techniques to keep updating its (episodic) memories and that uses those memories to build maps and plan on the fly (imperfect maps, i.e. noisy and limited to the agent's experience) can be increasingly and substantially more efficient than the simpler (minimal-memory) agents, as the task difficulties such as distance to goal are raised, as long as the uncertainty, from localization and change, is not too large.

</details>


### [54] [EAA: Automating materials characterization with vision language model agents](https://arxiv.org/abs/2602.15294)
*Ming Du,Yanqi Luo,Srutarshi Banerjee,Michael Wojcik,Jelena Popovic,Mathew J. Cherukara*

Main category: cs.AI

TL;DR: EAA是一个基于视觉语言模型的代理系统，用于自动化复杂的实验显微镜工作流程，整合多模态推理、工具增强操作和可选长期记忆，支持自主程序和交互式用户引导测量。


<details>
  <summary>Details</summary>
Motivation: 传统实验显微镜工作流程复杂且需要专业知识，EAA旨在通过自动化降低操作负担、提高束线效率，并降低用户专业知识门槛。

Method: 基于灵活的任务管理器架构，整合多模态推理和工具增强操作，支持Model Context Protocol双向兼容的工具生态系统，实现从完全代理驱动自动化到逻辑定义工作流程的多种操作模式。

Result: 在Advanced Photon Source成像束线上成功演示了自动区域板聚焦、自然语言描述特征搜索和交互式数据采集等功能，展示了系统在实际应用中的有效性。

Conclusion: 视觉能力代理系统能够显著增强束线效率、减少操作负担，并为用户降低专业知识门槛，为实验自动化提供了新的解决方案。

Abstract: We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.

</details>


### [55] [X-MAP: eXplainable Misclassification Analysis and Profiling for Spam and Phishing Detection](https://arxiv.org/abs/2602.15298)
*Qi Zhang,Dian Chen,Lance M. Kaplan,Audun Jøsang,Dong Hyun Jeong,Feng Chen,Jin-Hee Cho*

Main category: cs.AI

TL;DR: X-MAP是一个可解释的误分类分析和分析框架，通过主题级语义模式揭示模型失败原因，结合SHAP特征归因和非负矩阵分解构建可解释主题配置文件，用于改进垃圾邮件和钓鱼检测。


<details>
  <summary>Details</summary>
Motivation: 垃圾邮件和钓鱼检测中的误分类非常有害：假阴性使用户暴露于攻击，假阳性降低信任。现有基于不确定性的检测器可以标记潜在错误，但可能被欺骗且可解释性有限。

Method: X-MAP结合SHAP特征归因和非负矩阵分解，为可靠分类的垃圾邮件/钓鱼邮件和合法邮件构建可解释主题配置文件，使用Jensen-Shannon散度测量每条消息与这些配置文件的偏差。

Result: 实验显示误分类消息的偏差至少是正确分类消息的两倍。作为检测器，X-MAP达到0.98 AUROC，在95% TRR时将假拒绝率降至0.089。作为修复层，它能恢复高达97%的误拒绝正确预测。

Conclusion: X-MAP通过揭示主题级语义模式，有效提高了垃圾邮件和钓鱼检测的可解释性和性能，展示了其在改善检测系统方面的实用价值。

Abstract: Misclassifications in spam and phishing detection are very harmful, as false negatives expose users to attacks while false positives degrade trust. Existing uncertainty-based detectors can flag potential errors, but possibly be deceived and offer limited interpretability. This paper presents X-MAP, an eXplainable Misclassification Analysis and Profilling framework that reveals topic-level semantic patterns behind model failures. X-MAP combines SHAP-based feature attributions with non-negative matrix factorization to build interpretable topic profiles for reliably classified spam/phishing and legitimate messages, and measures each message's deviation from these profiles using Jensen-Shannon divergence. Experiments on SMS and phishing datasets show that misclassified messages exhibit at least two times larger divergence than correctly classified ones. As a detector, X-MAP achieves up to 0.98 AUROC and lowers the false-rejection rate at 95% TRR to 0.089 on positive predictions. When used as a repair layer on base detectors, it recovers up to 97% of falsely rejected correct predictions with moderate leakage. These results demonstrate X-MAP's effectiveness and interpretability for improving spam and phishing detection.

</details>


### [56] [AgriWorld:A World Tools Protocol Framework for Verifiable Agricultural Reasoning with Code-Executing LLM Agents](https://arxiv.org/abs/2602.15325)
*Zhixing Zhang,Jesen Zhang,Hao Liu,Qinhan Lv,Jing Yang,Kaitong Cai,Keze Wang*

Main category: cs.AI

TL;DR: 提出Agro-Reflective框架，通过LLM代理在农业数据环境中执行代码、观察结果、迭代优化，解决现有农业基础模型缺乏语言推理能力的问题


<details>
  <summary>Details</summary>
Motivation: 当前农业基础模型虽然能处理大量时空数据，但缺乏语言推理和交互能力；而大语言模型擅长文本处理却无法直接处理高维异构农业数据。需要桥接这一鸿沟

Method: 开发AgriWorld Python执行环境，提供地理空间查询、遥感时间序列分析、作物生长模拟等工具；设计Agro-Reflective多轮LLM代理，通过执行-观察-优化循环迭代分析

Result: 在AgroBench基准测试中，涵盖查找、预测、异常检测和反事实分析等多种农业QA任务，性能优于纯文本和直接工具使用基线

Conclusion: 执行驱动的反思机制能实现可靠的农业推理，为农业科学提供有效的智能体框架

Abstract: Foundation models for agriculture are increasingly trained on massive spatiotemporal data (e.g., multi-spectral remote sensing, soil grids, and field-level management logs) and achieve strong performance on forecasting and monitoring. However, these models lack language-based reasoning and interactive capabilities, limiting their usefulness in real-world agronomic workflows. Meanwhile, large language models (LLMs) excel at interpreting and generating text, but cannot directly reason over high-dimensional, heterogeneous agricultural datasets. We bridge this gap with an agentic framework for agricultural science. It provides a Python execution environment, AgriWorld, exposing unified tools for geospatial queries over field parcels, remote-sensing time-series analytics, crop growth simulation, and task-specific predictors (e.g., yield, stress, and disease risk). On top of this environment, we design a multi-turn LLM agent, Agro-Reflective, that iteratively writes code, observes execution results, and refines its analysis via an execute-observe-refine loop. We introduce AgroBench, with scalable data generation for diverse agricultural QA spanning lookups, forecasting, anomaly detection, and counterfactual "what-if" analysis. Experiments outperform text-only and direct tool-use baselines, validating execution-driven reflection for reliable agricultural reasoning.

</details>


### [57] [World-Model-Augmented Web Agents with Action Correction](https://arxiv.org/abs/2602.15384)
*Zhouzhou Shen,Xueyu Hu,Xiyun Li,Tianqing Fang,Juncheng Li,Shengyu Zhang*

Main category: cs.AI

TL;DR: WAC是一个集成模型协作、结果模拟和反馈驱动动作优化的Web智能体，通过多智能体协作和两阶段推理链提升任务执行的安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的Web智能体在预测环境变化方面存在局限，缺乏对执行风险的全面认知，容易过早执行危险动作导致任务失败和损失。

Method: 提出WAC智能体，采用多智能体协作：动作模型咨询作为Web环境专家的世界模型获取策略指导；引入两阶段推理链：世界模型模拟动作结果，法官模型审查并触发必要的动作修正反馈。

Result: 实验显示WAC在VisualWebArena上获得1.8%的绝对提升，在Online-Mind2Web上获得1.3%的绝对提升。

Conclusion: WAC通过模型协作和风险感知的弹性任务执行机制，有效解决了当前Web智能体在环境变化预测和风险意识方面的局限性，显著提升了任务成功率。

Abstract: Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.

</details>


### [58] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 提出自适应弃权系统，通过动态调整安全阈值和级联检测架构，在保持高性能的同时平衡LLM的安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM部署面临安全性与实用性的根本权衡：严格过滤会误阻良性查询，宽松控制则产生不安全内容。传统护栏基于静态规则或固定置信度阈值，缺乏上下文敏感性且计算成本高，导致高延迟和用户体验下降。

Method: 引入自适应弃权系统，基于实时上下文信号（如领域和用户历史）动态调整安全阈值。框架集成五路并行检测器的多维检测架构，通过分层级联机制优化速度和精度。级联设计通过逐步过滤查询减少不必要计算。

Result: 在混合和特定领域工作负载上的广泛评估显示，假阳性显著减少，特别是在医疗建议和创意写作等敏感领域。系统在严格操作模式下保持高安全精度和接近完美的召回率，相比非级联模型和外部护栏系统实现了显著的延迟改进。

Conclusion: 上下文感知的弃权框架有效平衡安全性与实用性，同时保持性能，为可靠的LLM部署提供了可扩展解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [59] [Common Belief Revisited](https://arxiv.org/abs/2602.15403)
*Thomas Ågotnes*

Main category: cs.AI

TL;DR: 本文研究了在KD45个体信念下共同信念的逻辑性质，发现共同信念具有shift-reflexivity属性，并最终完整刻画了共同信念的逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为共同信念是KD4，但作者发现当个体信念是KD45时，共同信念实际上失去了5属性而保留了D和4属性，并具有shift-reflexivity属性。这引发了一个开放性问题：KD4加上shift-reflexivity公理是否能完全刻画共同信念？

Method: 通过逻辑分析，作者研究了共同信念在KD45个体信念下的性质，发现除了shift-reflexivity外还需要一个额外的公理，且这个公理依赖于智能体的数量。

Result: 研究结果表明：1) KD4加上shift-reflexivity公理不足以完全刻画共同信念；2) 需要添加一个额外的公理；3) 这个公理依赖于智能体数量；4) 最终得到了共同信念的完整逻辑刻画。

Conclusion: 本文解决了共同信念逻辑的开放性问题，完整刻画了在KD45个体信念下共同信念的逻辑系统，发现它需要KD4加上shift-reflexivity和一个依赖于智能体数量的额外公理。

Abstract: Contrary to common belief, common belief is not KD4.
  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(Cφ\rightarrow φ)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:
  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.

</details>


### [60] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: EduEVAL-DB是一个基于教师角色的数据集，用于评估和训练自动教学评估器和AI导师，包含854个解释，涵盖科学、语言和社会科学K-12年级内容，采用半自动标注和专家审核。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估和训练自动教学评估器和AI导师的数据集，特别是在教学解释质量评估方面。需要基于真实教育实践中的教学风格和缺陷来构建数据集，以支持教育AI系统的开发和评估。

Method: 1. 基于ScienceQA基准构建包含139个问题的数据集，涵盖K-12年级的科学、语言和社会科学；2. 为每个问题提供1个人类教师解释和6个LLM模拟的教师角色解释；3. 通过提示工程实例化基于真实教学风格和缺陷的教师角色；4. 提出包含五个维度的教学风险评估标准；5. 采用半自动流程和专家教师审核进行二元风险标注。

Result: 创建了包含854个解释的EduEVAL-DB数据集，每个解释都标注了教学风险标签。初步验证实验表明，该数据集适用于评估目的，能够支持教学风险检测，并且可以在消费级硬件上部署的模型上进行监督微调。

Conclusion: EduEVAL-DB是一个有价值的资源，可用于评估和训练自动教学评估器和AI导师，特别是在教学解释质量评估方面。该数据集基于真实教育实践构建，具有教育标准对齐的风险评估框架，为教育AI系统的开发和评估提供了重要支持。

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [61] [Quantifying construct validity in large language model evaluations](https://arxiv.org/abs/2602.15532)
*Ryan Othniel Kearns*

Main category: cs.AI

TL;DR: 本文提出结构化能力模型，首次从大量LLM基准测试结果中提取可解释且可泛化的能力，解决了现有方法在构造效度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社区将基准测试结果等同于模型通用能力，但基准测试存在测试集污染、标注错误等问题。现有方法（潜在因子模型和缩放定律）都无法有效分离基准结果与真实能力，无法可靠评估LLM的构造效度。

Method: 提出结构化能力模型，结合缩放定律和潜在因子模型的优势：模型规模应影响能力（如缩放定律），而这些能力应通过测量误差影响观察结果（如潜在因子模型）。在OpenLLM排行榜的大规模结果样本上拟合该模型及其两种替代方法。

Result: 结构化能力模型在简约拟合指标上优于潜在因子模型，在分布外基准预测上优于缩放定律。该模型能更好地分离模型规模与能力，提供更好的解释和预测能力。

Conclusion: 结构化能力模型通过适当结合缩放定律和潜在因子模型的见解，为LLM评估中的构造效度量化提供了更好的解释和预测能力，是首个能从大量基准结果中提取可解释且可泛化能力的模型。

Abstract: The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.
  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.
  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.

</details>


### [62] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: Ruva提出首个"透明盒"架构，用于人类参与的记忆管理，将个人AI从向量匹配转向知识图谱推理，确保用户对AI知识的可检查性和精确删除权。


<details>
  <summary>Details</summary>
Motivation: 当前个人AI领域被"黑盒"检索增强生成主导，标准向量数据库缺乏可追溯性：当AI产生幻觉或检索敏感数据时，用户无法检查原因或纠正错误。此外，从向量空间中"删除"概念在数学上不精确，会留下违反隐私的"幽灵"数据。

Method: Ruva采用"透明盒"架构，将个人AI建立在个人知识图谱基础上，支持人类参与的记忆管理。通过从向量匹配转向图谱推理，允许用户检查AI知道的内容，并对特定事实进行精确删除。

Result: Ruva实现了用户对AI知识的完全透明性和控制权，确保"被遗忘权"。用户成为自己生活的编辑者，能够精确管理AI记忆，消除传统向量数据库中的隐私风险。

Conclusion: Ruva通过知识图谱架构解决了当前个人AI系统的透明性和隐私问题，为用户提供了对AI记忆的完全控制权，实现了真正的人类参与记忆管理范式。

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [63] [How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning](https://arxiv.org/abs/2602.15580)
*Hongxuan Wu,Yukun Zhang,Xueqing Zhou*

Main category: cs.AI

TL;DR: 该研究使用部分信息分解(PID)分析多模态Transformer在视觉问答任务中的信息处理模式，发现视觉信息在早期层达到峰值后衰减，语言信息在后期层主导预测(约82%)，跨模态协同作用始终低于2%。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解多模态Transformer在回答视觉问题时，预测是基于视觉证据、语言推理还是真正的跨模态计算，以及这种信息处理结构如何在不同层间演化。

Method: 提出PID Flow框架，结合降维、归一化流高斯化和闭式高斯PID估计，对LLaVA-1.5-7B和LLaVA-1.6-7B在六个GQA推理任务中进行层级分析，并通过注意力敲除实验建立因果关系。

Result: 发现一致的模态转换模式：视觉独特信息早期达到峰值后衰减，语言独特信息在后期层激增(占最终预测约82%)，跨模态协同作用始终低于2%。模型变体间层间相关性>0.96，但任务依赖性很强。

Conclusion: 该研究为多模态Transformer中视觉如何转换为语言提供了信息论和因果解释，并为识别模态特定信息丢失的架构瓶颈提供了定量指导。

Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\% of the final prediction, and cross-modal synergy remains below 2\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.

</details>


### [64] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: 提出一种预处理方法，通过推断额外的累积约束来捕获多资源交互，提升调度问题的搜索性能


<details>
  <summary>Details</summary>
Motivation: 传统约束编程中累积约束的传播通常是逐个约束进行的，忽略了多资源之间的交互作用，导致在某些基准测试上性能严重下降

Method: 将累积约束解释为占用向量的线性不等式，通过三个步骤生成有效不等式：(1) 发现覆盖集（不能并行运行的任务集合），(2) 通过提升技术加强覆盖不等式，(3) 将生成的约束注入调度问题实例

Result: 在标准RCPSP和RCPSP/max测试集上的实验表明，推断的约束提高了搜索性能，在有利实例上收紧目标界限，在不利实例上仅有轻微性能下降。此外，发现了25个新的下界和5个新的最优解，其中8个下界直接来自推断的约束

Conclusion: 该方法通过预处理推断额外的累积约束，有效捕获多资源交互，显著提升了调度问题的求解性能，为约束编程中的累积约束处理提供了新的有效方法

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [65] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: CARE Drive是一个评估自动驾驶中视觉语言模型"理由响应性"的框架，通过对比基线模型和理由增强模型在受控上下文变化下的决策，评估人类理由是否真正影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注结果性能（如安全性、轨迹精度），但无法确定模型决策是否真正反映了人类相关考量。这导致不清楚模型生成的解释是真正的理由响应决策还是事后合理化，在安全关键领域可能产生虚假信心。

Method: 提出CARE Drive框架：1) 提示校准确保稳定输出；2) 系统性的上下文扰动，测量决策对人类理由（如安全边际、社会压力、效率约束）的敏感性。通过对比基线模型和理由增强模型在受控上下文变化下的决策来评估理由响应性。

Result: 在自行车超车场景中，明确的人类理由显著影响模型决策，改善了与专家推荐行为的一致性。但响应性在不同上下文因素间存在差异，表明对不同类型理由的敏感性不均匀。

Conclusion: CARE Drive提供了实证证据，表明基础模型的理由响应性可以在不修改模型参数的情况下进行系统评估。该框架有助于确保自动驾驶系统中的模型决策真正反映人类相关考量，而不仅仅是事后合理化。

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [66] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: PERSONA是一个无需训练的人格控制框架，通过在激活空间中直接操作人格向量实现微调级别的性能，无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 当前LLM人格控制方法依赖静态提示或昂贵的微调，无法捕捉人类特质的动态性和组合性，需要更高效、可解释的控制方法。

Method: 通过三个阶段：Persona-Base（通过对比激活分析提取正交特质向量）、Persona-Algebra（通过向量算术实现精确控制）、Persona-Flow（在推理时动态组合向量实现上下文感知适配）。

Result: 在PersonalityBench上平均得分9.60（接近监督微调上限9.61），在Persona-Evolve动态人格适配基准上达到91%胜率，跨多种模型家族表现优异。

Conclusion: LLM的人格方面具有数学可处理性，为可解释且高效的行为控制开辟了新方向，证明无需训练即可实现精细的人格操控。

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [67] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: RCE框架让预训练语言模型在推理时动态修改内部表示几何，通过生成低秩概念子空间来构建新抽象，显著提升组合推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩展token级搜索（如思维链、自洽性、强化学习）来改进推理，但保持模型的潜在表示空间固定。当所需抽象未编码在该空间中时，性能会急剧下降。

Method: 提出递归概念演化（RCE）框架，在推理时动态生成低秩概念子空间：检测表示不足时生成子空间，通过最小描述长度准则选择，协同时合并，通过约束优化进行整合以保持稳定性。

Result: 在Mistral-7B上集成RCE，在组合推理基准测试中取得显著提升：ARC-AGI-2提高12-18点，GPQA和BBH提高8-14点，MATH和HLE上深度诱导误差持续减少。

Conclusion: RCE使预训练语言模型能够在推理时修改内部表示几何，构建新抽象而非仅重组现有概念，显著提升组合推理能力，为解决复杂推理任务提供了新方向。

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [68] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: 提出GlobeDiff算法，通过多模态扩散过程从局部观测推断全局状态，解决多智能体系统中的部分可观测问题


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的部分可观测性是有效协调和决策的关键障碍。现有方法如信念状态估计和智能体间通信存在局限：信念方法主要依赖过去经验而未充分利用全局信息，通信方法缺乏有效利用辅助信息的鲁棒模型。

Method: 提出GlobeDiff算法，将状态推断过程建模为多模态扩散过程。通过扩散模型从局部观测推断全局状态，克服状态估计中的模糊性，同时实现高保真度的全局状态推断。

Result: 理论证明了GlobeDiff在单模态和多模态分布下的估计误差有界。大量实验结果表明，GlobeDiff实现了优越性能，能够准确推断全局状态。

Conclusion: GlobeDiff通过多模态扩散过程有效解决了多智能体系统中的部分可观测性问题，为全局状态推断提供了新的解决方案。

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [69] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: 论文对比了两种使用LLM作为合成参与者的策略：启发式方法和统计校准，分析了它们在探索性与验证性研究中的适用性及假设条件。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的研究使用大型语言模型（LLM）作为合成参与者进行社会科学实验，但缺乏关于何时这种模拟能够有效推断人类行为的指导。需要明确不同策略的适用场景和假设条件。

Method: 对比两种策略：1）启发式方法：通过提示工程、模型微调等修复策略使模拟与观察的人类行为可互换；2）统计校准：结合辅助人类数据与统计调整来考虑观察与模拟响应之间的差异。

Result: 启发式方法适用于探索性任务但缺乏验证性研究所需的正式统计保证；统计校准在明确假设下能保持有效性，并以比纯人类参与者实验更低的成本提供更精确的因果效应估计。

Conclusion: 两种方法的潜力都取决于LLM对相关人群的近似程度。研究人员不应仅仅狭隘地关注用LLM替代人类参与者，而应考虑更广泛的机会。

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [70] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: 该研究提出使用大语言模型（LLM）嵌入作为编码方式，替代传统的one-hot编码，以更好地捕捉建筑对象子类型之间的语义关系，在BIM分类任务中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统编码方法（如one-hot）无法有效传达建筑对象子类型之间的细微语义关系，限制了AI对建筑语义的理解能力。在AECO行业中，准确表示建筑语义（包括通用对象类型和特定子类型）对于AI模型训练至关重要。

Method: 提出使用LLM嵌入（OpenAI GPT和Meta LLaMA）作为编码方式，保留建筑语义的精细区分。使用GraphSAGE模型对5个高层住宅BIM中的42个建筑对象子类型进行分类。测试了不同嵌入维度，包括原始高维LLM嵌入（1,536、3,072或4,096维）和通过Matryoshka表示模型生成的1,024维压缩嵌入。

Result: LLM编码优于传统的one-hot基线，其中llama-3（压缩）嵌入实现了0.8766的加权平均F1分数，而one-hot编码为0.8475。结果表明LLM编码能够更好地捕捉建筑对象子类型之间的语义关系。

Conclusion: LLM编码方法在增强AI理解复杂领域特定建筑语义方面具有潜力。随着LLM和降维技术的不断发展，这种方法在AECO行业的语义细化任务中具有广泛的应用前景。

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [71] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本章介绍了基于仿真的合成数据生成方法，用于解决AI训练中数据不足和质量问题，并提出了数字孪生AI仿真解决方案的参考框架。


<details>
  <summary>Details</summary>
Motivation: 现代符号AI的采用面临数据量不足和数据质量差的关键障碍，因此对合成数据生成技术的需求很高。仿真提供了一种系统化的方法来生成多样化的合成数据。

Method: 本章介绍了基于仿真的合成数据生成的关键概念、优势和挑战，并提出了一个参考框架来描述、设计和分析基于数字孪生的AI仿真解决方案。

Result: 提出了一个系统化的参考框架，用于指导基于数字孪生的AI仿真解决方案的设计和实施，以生成高质量的合成训练数据。

Conclusion: 仿真为AI训练提供了有效的合成数据生成方法，数字孪生框架为系统化设计和分析AI仿真解决方案提供了实用工具，有助于克服数据不足的瓶颈。

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [72] [Near-Optimal Sample Complexity for Online Constrained MDPs](https://arxiv.org/abs/2602.15076)
*Chang Liu,Yunfan Li,Lin F. Yang*

Main category: cs.LG

TL;DR: 提出基于模型的原对偶算法，用于约束马尔可夫决策过程的安全强化学习，在允许小违规和零违规两种设置下实现高效学习，样本复杂度分别匹配无约束MDP和生成模型的下界。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习应用（如自动驾驶、机器人、医疗）中安全至关重要，现有方法存在严重安全违规或样本复杂度高的问题，需要开发能平衡性能与安全约束的高效算法。

Method: 提出基于模型的原对偶算法，结合在线RL和约束优化技术，平衡遗憾和约束违规。处理两种设置：允许小违规的松弛可行性和零违规的严格可行性。

Result: 对于松弛可行性：算法以任意高概率返回ε最优策略且违规有界，需要Õ(SAH³/ε²)学习回合，匹配无约束MDP下界。对于严格可行性：算法以任意高概率返回ε最优策略且零违规，需要Õ(SAH⁵/ε²ζ²)学习回合，匹配生成模型下界。

Conclusion: 在线学习CMDP与使用生成模型学习同样容易，当允许小违规时，学习CMDP不比学习无约束MDP更困难，为安全强化学习提供了理论保证。

Abstract: Safety is a fundamental challenge in reinforcement learning (RL), particularly in real-world applications such as autonomous driving, robotics, and healthcare. To address this, Constrained Markov Decision Processes (CMDPs) are commonly used to enforce safety constraints while optimizing performance. However, existing methods often suffer from significant safety violations or require a high sample complexity to generate near-optimal policies. We address two settings: relaxed feasibility, where small violations are allowed, and strict feasibility, where no violation is allowed. We propose a model-based primal-dual algorithm that balances regret and bounded constraint violations, drawing on techniques from online RL and constrained optimization. For relaxed feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with $\varepsilon$-bounded violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ learning episodes, matching the lower bound for unconstrained MDPs. For strict feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with zero violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2ζ^2}\right)$ learning episodes, where $ζ$ is the problem-dependent Slater constant characterizing the size of the feasible region. This result matches the lower bound for learning CMDPs with access to a generative model.
  Our results demonstrate that learning CMDPs in an online setting is as easy as learning with a generative model and is no more challenging than learning unconstrained MDPs when small violations are allowed.

</details>


### [73] [Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction](https://arxiv.org/abs/2602.15089)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习时间序列嵌入与统计特征的混合方法，用于HVAC设备异常预测，在64台设备上实现了高精度检测。


<details>
  <summary>Details</summary>
Motivation: 在设备预测性维护中，纯深度学习方法在真实数据上往往精度不足，需要结合领域知识来提高异常检测的准确性。

Method: 采用混合方法：1) 使用LoRA微调的Granite TinyTimeMixer编码器提取64维时间序列嵌入；2) 基于领域知识提取28维统计特征（趋势、波动性、回撤等）；3) 使用LightGBM梯度提升分类器进行学习。

Result: 在64台设备、51,564个样本的实验中，30/60/90天预测的精度达到91-95%，ROC-AUC为0.995，误报率≤1.1%，检测率88-94%。

Conclusion: 通过结合深度学习的表示学习能力和统计特征工程的互补优势，可以实现实用的异常检测系统，适用于预测性维护应用。

Abstract: In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\% or less and a detection rate of 88--94\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.

</details>


### [74] [PolyNODE: Variable-dimension Neural ODEs on M-polyfolds](https://arxiv.org/abs/2602.15128)
*Per Åhag,Alexander Friedrich,Fredrik Ohlsson,Viktor Vigren Näslund*

Main category: cs.LG

TL;DR: PolyNODEs将神经常微分方程扩展到可变维度空间，是几何深度学习中的首个可变维度流模型，能够处理维度瓶颈并用于重建和分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有神经常微分方程模型受限于固定维度，无法处理可变维度的几何结构。本文旨在扩展NODEs到可变维度空间，解决这一根本限制。

Method: 将NODEs扩展到M-polyfolds（同时容纳可变维度和可微概念的空间），引入PolyNODEs模型。构建具有维度瓶颈的M-polyfolds，并基于参数化向量场构建PolyNODE自编码器。

Result: 实验证明PolyNODE模型能够训练解决重建任务，并能提取输入的潜在表示用于下游分类任务。

Conclusion: PolyNODEs是几何深度学习中首个可变维度流模型，成功扩展了NODEs的能力，为处理可变维度几何结构提供了新框架。

Abstract: Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at https://github.com/turbotage/PolyNODE .

</details>


### [75] [Refine Now, Query Fast: A Decoupled Refinement Paradigm for Implicit Neural Fields](https://arxiv.org/abs/2602.15155)
*Tianyu Xiong,Skylar Wurster,Han-Wei Shen*

Main category: cs.LG

TL;DR: DRR-Net通过解耦表示精炼架构解决INRs的保真度-速度困境，使用深度精炼网络和离线处理将丰富表示编码到紧凑嵌入结构中，实现高保真度同时推理速度提升27倍。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)作为3D科学模拟的替代模型面临保真度-速度困境：深度MLP推理成本高，而高效的基于嵌入的模型表达能力不足。需要解决这一根本矛盾。

Method: 提出解耦表示精炼(DRR)架构范式：使用深度精炼网络和非参数变换，在一次性离线过程中将丰富表示编码到紧凑高效的嵌入结构中，将慢速高容量神经网络与快速推理路径解耦。提出DRR-Net验证该范式，以及变分对(VP)数据增强策略。

Result: 在多个集合模拟数据集上的实验表明，该方法达到最先进的保真度，推理速度比高保真度基线快27倍，同时与最快模型保持竞争力。

Conclusion: DRR范式为构建强大实用的神经场替代模型提供了有效策略，在速度和质量之间实现最小妥协，可广泛应用于INRs相关应用。

Abstract: Implicit Neural Representations (INRs) have emerged as promising surrogates for large 3D scientific simulations due to their ability to continuously model spatial and conditional fields, yet they face a critical fidelity-speed dilemma: deep MLPs suffer from high inference cost, while efficient embedding-based models lack sufficient expressiveness. To resolve this, we propose the Decoupled Representation Refinement (DRR) architectural paradigm. DRR leverages a deep refiner network, alongside non-parametric transformations, in a one-time offline process to encode rich representations into a compact and efficient embedding structure. This approach decouples slow neural networks with high representational capacity from the fast inference path. We introduce DRR-Net, a simple network that validates this paradigm, and a novel data augmentation strategy, Variational Pairs (VP) for improving INRs under complex tasks like high-dimensional surrogate modeling. Experiments on several ensemble simulation datasets demonstrate that our approach achieves state-of-the-art fidelity, while being up to 27$\times$ faster at inference than high-fidelity baselines and remaining competitive with the fastest models. The DRR paradigm offers an effective strategy for building powerful and practical neural field surrogates and \rev{INRs in broader applications}, with a minimal compromise between speed and quality.

</details>


### [76] [Learning Representations from Incomplete EHR Data with Dual-Masked Autoencoding](https://arxiv.org/abs/2602.15159)
*Xiao Xiang,David Restrepo,Hyewon Jeong,Yugang Jia,Leo Anthony Celi*

Main category: cs.LG

TL;DR: AID-MAE：一种双掩码自编码器，直接从不完整时间序列学习，通过内在缺失掩码和增强掩码处理电子健康记录数据，在多个临床任务上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录时间序列学习面临不规则采样、异构缺失和观测稀疏性等挑战。现有自监督方法要么先插补再学习，要么通过专用输入信号表示缺失，要么仅优化插补任务，限制了学习支持临床下游任务表示的能力。

Method: 提出增强-内在双掩码自编码器（AID-MAE），直接从不完整时间序列学习：应用内在缺失掩码表示自然缺失值，应用增强掩码隐藏部分观测值用于训练重建。模型仅处理未掩码的token子集。

Result: AID-MAE在两个数据集上的多个临床任务中持续优于强基线方法（包括XGBoost和DuETT）。学习到的嵌入在表示空间中自然分层患者队列。

Conclusion: AID-MAE通过双掩码策略有效处理电子健康记录时间序列的不完整性和稀疏性，学习到支持临床下游任务的强大表示，并能自然分层患者群体。

Abstract: Learning from electronic health records (EHRs) time series is challenging due to irregular sam- pling, heterogeneous missingness, and the resulting sparsity of observations. Prior self-supervised meth- ods either impute before learning, represent missingness through a dedicated input signal, or optimize solely for imputation, reducing their capacity to efficiently learn representations that support clinical downstream tasks. We propose the Augmented-Intrinsic Dual-Masked Autoencoder (AID-MAE), which learns directly from incomplete time series by applying an intrinsic missing mask to represent naturally missing values and an augmented mask that hides a subset of observed values for reconstruction during training. AID-MAE processes only the unmasked subset of tokens and consistently outperforms strong baselines, including XGBoost and DuETT, across multiple clinical tasks on two datasets. In addition, the learned embeddings naturally stratify patient cohorts in the representation space.

</details>


### [77] [Seeing to Generalize: How Visual Data Corrects Binding Shortcuts](https://arxiv.org/abs/2602.15183)
*Nicolas Buzeta,Felipe del Rio,Cristian Hinostroza,Denis Parra,Hans Lobel,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: 视觉语言模型在纯文本任务上能超越其底层语言模型，特别是在长上下文信息检索中，这归因于视觉训练改变了模型的内部绑定策略，使其采用更稳健的符号绑定机制


<details>
  <summary>Details</summary>
Motivation: 研究一个令人惊讶的现象：视觉语言模型在纯文本任务上（特别是长上下文信息检索）能超越其底层语言模型，探索这种跨模态训练如何提升单模态任务的推理和泛化能力

Method: 构建受控的合成检索任务，比较纯文本训练和图像标记化训练的效果，使用机制可解释性分析内部绑定策略的变化，研究不同训练机制、视觉编码器和初始化条件下的绑定策略差异

Result: 纯文本训练的模型在分布内表现完美但无法泛化到分布外，而图像训练使文本任务的OOD性能几乎翻倍；视觉训练通过空间平移不变性破坏位置捷径，迫使模型采用更稳健的符号绑定机制，这种机制在重新引入纯文本示例后仍然保持

Conclusion: 跨模态训练可以增强推理和泛化能力，即使对于单模态任务也是如此；视觉训练改变了模型的内部表示策略，使其从位置捷径转向更稳健的符号绑定，这解释了VLMs在纯文本任务上超越LLMs的现象

Abstract: Vision Language Models (VLMs) are designed to extend Large Language Models (LLMs) with visual capabilities, yet in this work we observe a surprising phenomenon: VLMs can outperform their underlying LLMs on purely text-only tasks, particularly in long-context information retrieval. To investigate this effect, we build a controlled synthetic retrieval task and find that a transformer trained only on text achieves perfect in-distribution accuracy but fails to generalize out of distribution, while subsequent training on an image-tokenized version of the same task nearly doubles text-only OOD performance. Mechanistic interpretability reveals that visual training changes the model's internal binding strategy: text-only training encourages positional shortcuts, whereas image-based training disrupts them through spatial translation invariance, forcing the model to adopt a more robust symbolic binding mechanism that persists even after text-only examples are reintroduced. We further characterize how binding strategies vary across training regimes, visual encoders, and initializations, and show that analogous shifts occur during pretrained LLM-to-VLM transitions. Our findings suggest that cross-modal training can enhance reasoning and generalization even for tasks grounded in a single modality.

</details>


### [78] [Learning Data-Efficient and Generalizable Neural Operators via Fundamental Physics Knowledge](https://arxiv.org/abs/2602.15184)
*Siying Ma,Mehrdad M. Zadeh,Mauricio Soroco,Wuyang Chen,Jiguo Cao,Vijay Ganesh*

Main category: cs.LG

TL;DR: 提出多物理训练框架，通过联合学习原始PDE及其简化基本形式，提升神经算子的数据效率、预测精度和分布外泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法主要关注从目标PDE学习模拟，但忽略了更基本的物理原理。受数值求解器能与不同PDE设置兼容的启发，需要将基础物理知识显式融入训练

Method: 提出多物理训练框架，联合学习原始PDE及其简化基本形式，该框架与架构无关，通过同时训练不同复杂度的PDE来增强泛化能力

Result: 方法在1D/2D/3D多种PDE问题上显著降低归一化均方根误差，提升数据效率，减少预测误差，改善物理参数偏移和合成到真实迁移的分布外泛化

Conclusion: 显式融入基础物理知识能显著增强神经算子的泛化能力，多物理训练框架为科学机器学习提供了有效方法

Abstract: Recent advances in scientific machine learning (SciML) have enabled neural operators (NOs) to serve as powerful surrogates for modeling the dynamic evolution of physical systems governed by partial differential equations (PDEs). While existing approaches focus primarily on learning simulations from the target PDE, they often overlook more fundamental physical principles underlying these equations. Inspired by how numerical solvers are compatible with simulations of different settings of PDEs, we propose a multiphysics training framework that jointly learns from both the original PDEs and their simplified basic forms. Our framework enhances data efficiency, reduces predictive errors, and improves out-of-distribution (OOD) generalization, particularly in scenarios involving shifts of physical parameters and synthetic-to-real transfer. Our method is architecture-agnostic and demonstrates consistent improvements in normalized root mean square error (nRMSE) across a wide range of 1D/2D/3D PDE problems. Through extensive experiments, we show that explicit incorporation of fundamental physics knowledge significantly strengthens the generalization ability of neural operators. We will release models and codes at https://sites.google.com/view/sciml-fundemental-pde.

</details>


### [79] [COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression](https://arxiv.org/abs/2602.15200)
*Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Ammar Ali,Baher Mohammad,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: COMPOT是一种无需训练的后训练压缩框架，使用校准数据集估计稀疏权重分解，通过正交字典实现闭式Procrustes更新，并采用一次性动态分配策略自适应调整层间压缩率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于截断SVD的Transformer模型后训练压缩方法在适度压缩时就会导致精度下降，而稀疏字典学习方法虽然提供了更灵活的表示，但通常需要迭代的字典和系数更新，计算效率低。

Method: COMPOT使用小规模校准数据集估计稀疏权重分解，采用正交字典实现闭式Procrustes更新和解析单步稀疏编码，无需迭代优化。同时引入一次性动态分配策略，根据层敏感性自适应调整全局压缩预算下的层间压缩率。

Result: 在多种架构和任务上的实验表明，COMPOT在质量-压缩权衡方面始终优于强大的低秩和稀疏基线方法，并且完全兼容后训练量化以实现极端压缩。

Conclusion: COMPOT提供了一种高效、无需训练的后训练压缩框架，通过正交字典和动态分配策略实现了优于现有方法的压缩效果，为Transformer模型压缩提供了新的解决方案。

Abstract: Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available $\href{https://github.com/mts-ai/COMPOT}{here}$.

</details>


### [80] [MAVRL: Learning Reward Functions from Multiple Feedback Types with Amortized Variational Inference](https://arxiv.org/abs/2602.15206)
*Raphaël Baur,Yannick Metz,Maria Gkoulta,Mennatallah El-Assady,Giorgia Ramponi,Thomas Kleine Buening*

Main category: cs.LG

TL;DR: 提出一种贝叶斯推理方法，通过变分推断联合学习来自多种反馈类型（演示、比较、评分、停止）的奖励函数，避免手动损失平衡，提高策略鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前奖励学习通常依赖单一反馈类型或手动加权组合多种反馈，缺乏有效方法从异质反馈类型（演示、比较、评分、停止）中联合学习奖励函数，这些反馈提供不同性质的信号。

Method: 将多反馈类型奖励学习建模为共享潜在奖励函数的贝叶斯推理，每种反馈通过显式似然函数贡献信息。采用可扩展的摊销变分推断方法，学习共享奖励编码器和反馈特定似然解码器，通过优化单一证据下界进行训练。

Result: 在离散和连续控制基准测试中，联合推断的奖励后验优于单类型基线，能利用跨反馈类型的互补信息，产生的策略对环境扰动更鲁棒。推断的奖励不确定性为分析模型置信度和跨反馈类型一致性提供可解释信号。

Conclusion: 提出的贝叶斯推理框架能有效联合学习多种异质反馈类型，避免手动损失平衡，提高奖励学习和策略的鲁棒性，同时提供不确定性估计增强可解释性。

Abstract: Reward learning typically relies on a single feedback type or combines multiple feedback types using manually weighted loss terms. Currently, it remains unclear how to jointly learn reward functions from heterogeneous feedback types such as demonstrations, comparisons, ratings, and stops that provide qualitatively different signals. We address this challenge by formulating reward learning from multiple feedback types as Bayesian inference over a shared latent reward function, where each feedback type contributes information through an explicit likelihood. We introduce a scalable amortized variational inference approach that learns a shared reward encoder and feedback-specific likelihood decoders and is trained by optimizing a single evidence lower bound. Our approach avoids reducing feedback to a common intermediate representation and eliminates the need for manual loss balancing. Across discrete and continuous-control benchmarks, we show that jointly inferred reward posteriors outperform single-type baselines, exploit complementary information across feedback types, and yield policies that are more robust to environment perturbations. The inferred reward uncertainty further provides interpretable signals for analyzing model confidence and consistency across feedback types.

</details>


### [81] [ÜberWeb: Insights from Multilingual Curation for a 20-Trillion-Token Dataset](https://arxiv.org/abs/2602.15210)
*DatologyAI,:,Aldo Gael Carranza,Kaleigh Mentzer,Ricardo Pio Monti,Alex Fang,Alvin Deng,Amro Abbas,Anshuman Suri,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Diego Kiner,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 研究发现多语言性能下降并非固有缺陷，而是数据质量问题，通过针对性的单语言数据优化可以显著提升多语言模型性能，且优化任一语言都能惠及其他语言。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型需要多语言能力，但多语言训练面临数据分布不均和性能干扰问题。传统观点认为多语言扩展存在"多语言诅咒"，但本文质疑这是否是固有缺陷而非数据质量问题。

Method: 在13种语言上进行多语言数据策展研究，通过双语对照实验验证数据质量改进的影响。使用大规模通用训练混合，其中策展的多语言分配仅占总token的8%。构建了20T token的预训练语料库，训练3B和8B参数模型。

Result: 改善任一语言的数据质量都能提升其他语言性能：优化英语提升12/13种非英语语言，优化非英语也能提升英语性能。针对性单语言策展产生更大的语言内改进。使用仅8%策展token的模型在1T token训练后，以4-10倍更少的FLOPs达到竞争性多语言准确性。400B参数模型也表现出强大的多语言性能。

Conclusion: 针对性的单语言数据策展能够缓解多语言干扰，实现计算高效的多语言扩展。多语言性能下降主要是数据质量问题而非模型容量限制，通过数据优化可以建立多语言性能与计算的新帕累托前沿。

Abstract: Multilinguality is a core capability for modern foundation models, yet training high-quality multilingual models remains challenging due to uneven data availability across languages. A further challenge is the performance interference that can arise from joint multilingual training, commonly referred to as the "curse of multilinguality". We study multilingual data curation across thirteen languages and find that many reported regressions are not inherent to multilingual scaling but instead stem from correctable deficiencies in data quality and composition rather than fundamental capacity limits. In controlled bilingual experiments, improving data quality for any single language benefits others: curating English improves non-English performance in 12 of 13 languages, while curating non-English yields reciprocal improvements in English. Bespoke per-language curation produces substantially larger within-language improvements. Extending these findings to large-scale general-purpose training mixtures, we show that curated multilingual allocations comprising under 8% of total tokens remain remarkably effective. We operationalize this approach within an effort that produced a 20T-token pretraining corpus derived entirely from public sources. Models with 3B and 8B parameters trained on a 1T-token random subset achieve competitive multilingual accuracy with 4-10x fewer training FLOPs than strong public baselines, establishing a new Pareto frontier in multilingual performance versus compute. Moreover, these benefits extend to frontier model scale: the 20T-token corpus served as part of the pretraining dataset for Trinity Large (400B/A13B), which exhibits strong multilingual performance relative to its training FLOPs. These results show that targeted, per-language data curation mitigates multilingual interference and enables compute-efficient multilingual scaling.

</details>


### [82] [Automatically Finding Reward Model Biases](https://arxiv.org/abs/2602.15222)
*Atticus Wang,Iván Arcuschin,Arthur Conmy*

Main category: cs.LG

TL;DR: 本文提出了一种自动发现奖励模型偏见的LLM迭代方法，能够识别已知和新颖的偏见，如Skywork-V2-8B模型偏好冗余空格和幻觉内容


<details>
  <summary>Details</summary>
Motivation: 奖励模型在LLM后训练中至关重要，但现有研究表明它们可能奖励虚假或不良属性（如长度、格式、幻觉和奉承）。需要自动发现这些偏见的方法来改进奖励模型

Method: 使用LLM迭代提出和精炼候选偏见的简单方法，通过进化迭代优于平面最佳N搜索，并使用合成注入偏见验证管道召回率

Result: 方法能够恢复已知偏见并发现新颖偏见，例如发现Skywork-V2-8B模型经常错误地偏好带有冗余空格的响应和包含幻觉内容的响应

Conclusion: 该工作为通过自动可解释性方法改进奖励模型的研究做出了贡献，展示了进化迭代方法的有效性

Abstract: Reward models are central to large language model (LLM) post-training. However, past work has shown that they can reward spurious or undesirable attributes such as length, format, hallucinations, and sycophancy. In this work, we introduce and study the research problem of automatically finding reward model biases in natural language. We offer a simple approach of using an LLM to iteratively propose and refine candidate biases. Our method can recover known biases and surface novel ones: for example, we found that Skywork-V2-8B, a leading open-weight reward model, often mistakenly favors responses with redundant spacing and responses with hallucinated content. In addition, we show evidence that evolutionary iteration outperforms flat best-of-N search, and we validate the recall of our pipeline using synthetically injected biases. We hope our work contributes to further research on improving RMs through automated interpretability methods.

</details>


### [83] [tensorFM: Low-Rank Approximations of Cross-Order Feature Interactions](https://arxiv.org/abs/2602.15229)
*Alessio Mazzetto,Mohammad Mahdi Khalili,Laura Fee Nern,Michael Viderman,Alex Shtoff,Krzysztof Dembczyński*

Main category: cs.LG

TL;DR: tensorFM：一种用于表格分类数据预测的新模型，通过低秩张量近似高效捕获高阶交互，在保持低延迟的同时达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 表格分类数据（如点击率预测、社会科学）中需要高效建模多个分类属性间的高阶交互，现有方法在效率和表达能力上存在局限

Method: 提出tensorFM模型，使用低秩张量近似来表示属性间交互强度，扩展了场加权分解机，能高效捕获高阶交互

Result: tensorFM在实证中与最先进方法表现相当，且具有低延迟特性，适合在线广告等时间敏感应用

Conclusion: tensorFM为表格分类数据预测提供了一种高效的高阶交互建模方法，在性能和延迟间取得了良好平衡

Abstract: We address prediction problems on tabular categorical data, where each instance is defined by multiple categorical attributes, each taking values from a finite set. These attributes are often referred to as fields, and their categorical values as features. Such problems frequently arise in practical applications, including click-through rate prediction and social sciences. We introduce and analyze {tensorFM}, a new model that efficiently captures high-order interactions between attributes via a low-rank tensor approximation representing the strength of these interactions. Our model generalizes field-weighted factorization machines. Empirically, tensorFM demonstrates competitive performance with state-of-the-art methods. Additionally, its low latency makes it well-suited for time-sensitive applications, such as online advertising.

</details>


### [84] [BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening](https://arxiv.org/abs/2602.15236)
*Anjie Qiao,Zhen Wang,Yaliang Li,Jiahua Rao,Yuedong Yang*

Main category: cs.LG

TL;DR: BindCLIP提出了一种结合对比学习和生成式学习的统一框架，通过结合结合姿态生成监督来改进虚拟筛选中的分子表示学习，解决了现有CLIP风格模型对精细结合相互作用不敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP风格的虚拟筛选模型（如DrugCLIP）虽然能够实现可扩展的虚拟筛选，但其表示对精细的结合相互作用不敏感，并且可能依赖训练数据中的捷径相关性，限制了其根据真实结合兼容性对配体进行排序的能力。

Method: BindCLIP采用统一的对比-生成表示学习框架，同时训练口袋和配体编码器：1）使用CLIP风格对比学习；2）结合口袋条件扩散目标进行结合姿态生成，使姿态级监督直接塑造检索嵌入空间朝向相互作用相关特征；3）引入硬负样本增强和配体-配体锚定正则化器以防止表示崩溃。

Result: 在两个公共基准测试中表现优于强基线模型，在具有挑战性的分布外虚拟筛选中取得显著提升，并在FEP+基准测试中改进了配体类似物排序。

Conclusion: 将生成式姿态级监督与对比学习相结合，能够产生更具相互作用感知的嵌入表示，并在实际筛选场景中改善泛化能力，使虚拟筛选更接近实际应用。

Abstract: Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.

</details>


### [85] [Closing the Distribution Gap in Adversarial Training for LLMs](https://arxiv.org/abs/2602.15238)
*Chengzhi Hu,Jonas Dornbusch,David Lüdke,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出Distributional Adversarial Training (DAT)方法，通过扩散LLM近似真实数据分布，生成多样化的对抗样本，显著提升大语言模型对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前对抗训练方法虽然取得进展，但模型仍容易受到简单的分布内攻击（如改写时态、翻译等）。这种脆弱性源于现有方法仅最小化训练集上的对抗损失，未能充分覆盖数据分布

Method: 提出分布对抗训练(DAT)：1) 利用扩散LLM近似提示和响应的真实联合分布；2) 生成多样化、高似然的样本来解决泛化失败问题；3) 结合扩散模型提供的数据分布优化和持续对抗训练

Result: DAT相比先前方法实现了显著更高的对抗鲁棒性

Conclusion: 通过更好地覆盖数据分布，DAT能够有效解决当前对抗训练方法的局限性，提升大语言模型对看似简单攻击的防御能力

Abstract: Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.

</details>


### [86] [Size Transferability of Graph Transformers with Convolutional Positional Encodings](https://arxiv.org/abs/2602.15239)
*Javier Porras-Valenzuela,Zhiyang Wang,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 图Transformer通过GNN位置编码继承了可迁移性，能在小图上训练后泛化到大图上，具有与GNN相当的扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究图Transformer(GTs)的理论基础，特别是它们如何通过GNN位置编码获得结构信息，并探索GTs的可迁移性保证。

Method: 通过流形极限模型的理论框架，将带有GNN位置编码的GTs与流形神经网络(MNNs)建立理论联系，基于GNN在流形收敛下的可迁移性结果。

Result: 证明了GTs继承了其位置编码的可迁移性保证，在温和假设下，在小图上训练的GTs能够泛化到更大的图上，实验验证了GTs具有与GNN相当的扩展性。

Conclusion: GTs具有理论上的可迁移性保证，为大规模场景下高效训练GTs提供了实用方向，加深了对GTs的理解。

Abstract: Transformers have achieved remarkable success across domains, motivating the rise of Graph Transformers (GTs) as attention-based architectures for graph-structured data. A key design choice in GTs is the use of Graph Neural Network (GNN)-based positional encodings to incorporate structural information. In this work, we study GTs through the lens of manifold limit models for graph sequences and establish a theoretical connection between GTs with GNN positional encodings and Manifold Neural Networks (MNNs). Building on transferability results for GNNs under manifold convergence, we show that GTs inherit transferability guarantees from their positional encodings. In particular, GTs trained on small graphs provably generalize to larger graphs under mild assumptions. We complement our theory with extensive experiments on standard graph benchmarks, demonstrating that GTs exhibit scalable behavior on par with GNNs. To further show the efficiency in a real-world scenario, we implement GTs for shortest path distance estimation over terrains to better illustrate the efficiency of the transferable GTs. Our results provide new insights into the understanding of GTs and suggest practical directions for efficient training of GTs in large-scale settings.

</details>


### [87] [Scaling Laws for Masked-Reconstruction Transformers on Single-Cell Transcriptomics](https://arxiv.org/abs/2602.15253)
*Ihor Kendiukhov*

Main category: cs.LG

TL;DR: 首次系统研究单细胞RNA测序数据上掩码重建Transformer的缩放规律，发现数据丰富时存在清晰的幂律缩放，数据有限时缩放效应可忽略。


<details>
  <summary>Details</summary>
Motivation: 虽然神经缩放规律在语言和视觉Transformer中已被广泛记录，但在单细胞基因组学中仍未被充分探索。本研究旨在探索单细胞转录组学中是否存在类似的缩放规律。

Method: 使用CELLxGENE Census的表达谱构建两个实验方案：数据丰富方案（512个高变基因，200,000个细胞）和数据有限方案（1,024个基因，10,000个细胞）。在七个模型大小（参数数量跨越三个数量级）上拟合参数化缩放规律到验证均方误差。

Result: 数据丰富方案表现出清晰的幂律缩放，不可约损失下限c ~ 1.44；数据有限方案显示可忽略的缩放效应，表明模型容量不是主要限制因素。数据与参数比例是缩放行为的关键决定因素。

Conclusion: 当数据充足时，单细胞转录组学中确实会出现类似于自然语言处理的缩放规律。数据与参数比例是缩放行为的关键决定因素，这对单细胞基础模型的设计有重要启示。

Abstract: Neural scaling laws -- power-law relationships between loss, model size, and data -- have been extensively documented for language and vision transformers, yet their existence in single-cell genomics remains largely unexplored. We present the first systematic study of scaling behaviour for masked-reconstruction transformers trained on single-cell RNA sequencing (scRNA-seq) data. Using expression profiles from the CELLxGENE Census, we construct two experimental regimes: a data-rich regime (512 highly variable genes, 200,000 cells) and a data-limited regime (1,024 genes, 10,000 cells). Across seven model sizes spanning three orders of magnitude in parameter count (533 to 3.4 x 10^8 parameters), we fit the parametric scaling law to validation mean squared error (MSE). The data-rich regime exhibits clear power-law scaling with an irreducible loss floor of c ~ 1.44, while the data-limited regime shows negligible scaling, indicating that model capacity is not the binding constraint when data are scarce. These results establish that scaling laws analogous to those observed in natural language processing do emerge in single-cell transcriptomics when sufficient data are available, and they identify the data-to-parameter ratio as a critical determinant of scaling behaviour. A preliminary conversion of the data-rich asymptotic floor to information-theoretic units yields an estimate of approximately 2.30 bits of entropy per masked gene position. We discuss implications for the design of single-cell foundation models and outline the additional measurements needed to refine this entropy estimate.

</details>


### [88] [Fast and Effective On-policy Distillation from Reasoning Prefixes](https://arxiv.org/abs/2602.15260)
*Dongxu Zhang,Zhichao Yang,Sepehr Janghorbani,Jun Han,Andrew Ressler,Qian Qian,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.LG

TL;DR: 提出一种高效的在线蒸馏方法：仅对学生模型生成的前缀应用蒸馏目标，并在蒸馏过程中提前终止采样，大幅降低训练成本的同时保持性能


<details>
  <summary>Details</summary>
Motivation: 在线蒸馏（OPD）虽然能获得比离线蒸馏更好的泛化能力，但需要在训练过程中实时采样学生模型轨迹，训练成本高昂，特别是生成长响应时。研究发现训练信号通常集中在输出的前缀部分，短的前缀就能显著帮助学生生成正确答案。

Method: 提出在线前缀蒸馏：仅对学生模型生成输出的前缀应用蒸馏目标，并在蒸馏过程中提前终止采样。这样既保留了在线蒸馏的优势，又大幅减少了计算开销。

Result: 在AI-for-Math和跨领域基准测试中，在线前缀蒸馏达到了完整在线蒸馏的性能水平，同时将训练FLOP减少了2-47倍。

Conclusion: 在线前缀蒸馏是一种简单有效的改进方法，通过仅对输出前缀进行蒸馏和提前终止采样，在保持在线蒸馏性能优势的同时，显著降低了训练计算成本。

Abstract: On-policy distillation (OPD), which samples trajectories from the student model and supervises them with a teacher at the token level, avoids relying solely on verifiable terminal rewards and can yield better generalization than off-policy distillation. However, OPD requires expensive on-the-fly sampling of the student policy during training, which substantially increases training cost, especially for long responses. Our initial analysis shows that, during OPD, training signals are often concentrated in the prefix of each output, and that even a short teacher-generated prefix can significantly help the student produce the correct answer. Motivated by these observations, we propose a simple yet effective modification of OPD: we apply the distillation objective only to prefixes of student-generated outputs and terminate each sampling early during distillation. Experiments on a suite of AI-for-Math and out-of-domain benchmarks show that on-policy prefix distillation matches the performance of full OPD while reducing training FLOP by 2x-47x.

</details>


### [89] [Complex-Valued Unitary Representations as Classification Heads for Improved Uncertainty Quantification in Deep Neural Networks](https://arxiv.org/abs/2602.15283)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出一种量子启发的分类头架构，通过Cayley映射参数化的酉变换将特征投影到复值希尔伯特空间，显著改善了深度神经网络的校准性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络虽然预测准确率高，但校准性差：其置信度分数不能可靠反映正确的真实概率。需要改善神经网络的校准性能。

Method: 提出量子启发的分类头架构：将骨干网络特征投影到复值希尔伯特空间，通过Cayley映射参数化的酉变换演化特征。采用酉幅度头（复特征经酉演化后通过幅度和softmax读出），并与标准softmax头、温度缩放等方法对比。

Result: 在CIFAR-10上，酉幅度头获得0.0146的期望校准误差（ECE），比标准softmax头（0.0355）改善2.4倍，比温度缩放（0.0510）改善3.5倍。在CIFAR-10H人类不确定性基准上，波函数头获得最低KL散度（0.336），表明复值表示能更好捕捉人类感知模糊性。

Conclusion: 复值酉表示能显著改善神经网络校准性能，但Born规则测量层反而会降低校准性。该方法在安全关键应用中具有实用价值，但对分布外检测和情感分析效果有限。

Abstract: Modern deep neural networks achieve high predictive accuracy but remain poorly calibrated: their confidence scores do not reliably reflect the true probability of correctness. We propose a quantum-inspired classification head architecture that projects backbone features into a complex-valued Hilbert space and evolves them under a learned unitary transformation parameterised via the Cayley map. Through a controlled hybrid experimental design - training a single shared backbone and comparing lightweight interchangeable heads - we isolate the effect of complex-valued unitary representations on calibration. Our ablation study on CIFAR-10 reveals that the unitary magnitude head (complex features evolved under a Cayley unitary, read out via magnitude and softmax) achieves an Expected Calibration Error (ECE) of 0.0146, representing a 2.4x improvement over a standard softmax head (0.0355) and a 3.5x improvement over temperature scaling (0.0510). Surprisingly, replacing the softmax readout with a Born rule measurement layer - the quantum-mechanically motivated approach - degrades calibration to an ECE of 0.0819. On the CIFAR-10H human-uncertainty benchmark, the wave function head achieves the lowest KL-divergence (0.336) to human soft labels among all compared methods, indicating that complex-valued representations better capture the structure of human perceptual ambiguity. We provide theoretical analysis connecting norm-preserving unitary dynamics to calibration through feature-space geometry, report negative results on out-of-distribution detection and sentiment analysis to delineate the method's scope, and discuss practical implications for safety-critical applications. Code is publicly available.

</details>


### [90] [The Information Geometry of Softmax: Probing and Steering](https://arxiv.org/abs/2602.15293)
*Kiho Park,Todd Nief,Yo Joong Choe,Victor Veitch*

Main category: cs.LG

TL;DR: 论文探讨AI系统如何将语义结构编码到表示空间的几何结构中，提出信息几何是软最大分布表示的自然几何，并开发了"双重引导"方法用于概念操控。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是理解AI系统如何将语义结构编码到表示空间的几何结构中。核心观察是：表示空间的自然几何应该反映模型如何使用这些表示来产生行为。

Method: 聚焦于定义软最大分布的表示，论证信息几何是自然几何。开发了"双重引导"方法，使用线性探针稳健地引导表示以展示特定概念。该方法基于信息几何原理，通过线性探针实现概念操控。

Result: 理论证明：双重引导能够最优地修改目标概念，同时最小化对非目标概念的改变。实证发现：双重引导增强了概念操控的可控性和稳定性。

Conclusion: 信息几何为理解AI表示空间的语义编码提供了合适的几何框架，双重引导方法基于此框架实现了高效稳定的概念操控，验证了线性表示假说在信息几何背景下的有效性。

Abstract: This paper concerns the question of how AI systems encode semantic structure into the geometric structure of their representation spaces. The motivating observation of this paper is that the natural geometry of these representation spaces should reflect the way models use representations to produce behavior. We focus on the important special case of representations that define softmax distributions. In this case, we argue that the natural geometry is information geometry. Our focus is on the role of information geometry on semantic encoding and the linear representation hypothesis. As an illustrative application, we develop "dual steering", a method for robustly steering representations to exhibit a particular concept using linear probes. We prove that dual steering optimally modifies the target concept while minimizing changes to off-target concepts. Empirically, we find that dual steering enhances the controllability and stability of concept manipulation.

</details>


### [91] [Hybrid Federated and Split Learning for Privacy Preserving Clinical Prediction and Treatment Optimization](https://arxiv.org/abs/2602.15304)
*Farzana Akter,Rakib Hossain,Deb Kanna Roy Toushi,Mahmood Menon Khan,Sultana Amin,Lisan Al Amin*

Main category: cs.LG

TL;DR: 提出结合联邦学习与分割学习的混合隐私保护框架，用于医疗决策支持，无需共享原始数据，在预测性能、隐私泄露、通信开销等方面取得平衡。


<details>
  <summary>Details</summary>
Motivation: 医疗临床决策支持常受治理和隐私规则限制，无法跨机构共享患者级数据，需要一种隐私保护框架支持决策导向的医疗建模。

Method: 提出混合联邦学习-分割学习框架：客户端保留特征提取主干，协调服务器托管预测头；通过成员推理审计隐私泄露，采用激活裁剪和高斯噪声作为轻量级防御；在三个公开临床数据集上评估非独立同分布分区。

Result: 混合FL-SL变体在预测性能和决策优先排序方面与独立FL或SL相当，提供可调的隐私-效用权衡，能减少审计泄露风险，同时保持无需原始数据共享的优势。

Conclusion: 混合FL-SL为隐私保护医疗决策支持提供了实用的设计空间，能够明确平衡效用、泄露风险和部署成本。

Abstract: Collaborative clinical decision support is often constrained by governance and privacy rules that prevent pooling patient-level records across institutions. We present a hybrid privacy-preserving framework that combines Federated Learning (FL) and Split Learning (SL) to support decision-oriented healthcare modeling without raw-data sharing. The approach keeps feature-extraction trunks on clients while hosting prediction heads on a coordinating server, enabling shared representation learning and exposing an explicit collaboration boundary where privacy controls can be applied. Rather than assuming distributed training is inherently private, we audit leakage empirically using membership inference on cut-layer representations and study lightweight defenses based on activation clipping and additive Gaussian noise. We evaluate across three public clinical datasets under non-IID client partitions using a unified pipeline and assess performance jointly along four deployment-relevant axes: factual predictive utility, uplift-based ranking under capacity constraints, audited privacy leakage, and communication overhead. Results show that hybrid FL-SL variants achieve competitive predictive performance and decision-facing prioritization behavior relative to standalone FL or SL, while providing a tunable privacy-utility trade-off that can reduce audited leakage without requiring raw-data sharing. Overall, the work positions hybrid FL-SL as a practical design space for privacy-preserving healthcare decision support where utility, leakage risk, and deployment cost must be balanced explicitly.

</details>


### [92] [On Surprising Effectiveness of Masking Updates in Adaptive Optimizers](https://arxiv.org/abs/2602.15322)
*Taejong Joo,Wenhan Xia,Cheolmin Kim,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: 提出Magma优化器，通过随机掩码参数更新和动量梯度对齐，在LLM预训练中超越现有自适应优化器，显著降低困惑度


<details>
  <summary>Details</summary>
Motivation: 挑战当前LLM训练过度依赖复杂自适应优化器的现状，发现随机掩码参数更新能有效提升性能，从而开发更简单高效的优化方法

Method: 提出Momentum-aligned gradient masking (Magma)：1）随机掩码参数更新引入曲率相关的几何正则化；2）利用动量梯度对齐调节掩码更新

Result: 在LLM预训练实验中，Magma作为自适应优化器的简单替代方案，性能持续提升且计算开销可忽略。1B模型困惑度比Adam降低19%，比Muon降低9%

Conclusion: Magma证明了随机掩码参数更新的有效性，为LLM优化提供了简单高效的替代方案，挑战了当前依赖复杂自适应优化器的范式

Abstract: Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\% and 9\% compared to Adam and Muon, respectively.

</details>


### [93] [Prescriptive Scaling Reveals the Evolution of Language Model Capabilities](https://arxiv.org/abs/2602.15327)
*Hanlin Zhang,Jikai Jin,Vasilis Syrgkanis,Sham Kakade*

Main category: cs.LG

TL;DR: 本文提出了一种通过平滑分位数回归估计模型性能边界的方法，用于预测给定预训练计算预算下的下游任务性能，并验证了该方法的时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的部署需求增加，实践者需要能够根据预训练计算预算预测下游任务性能的缩放定律，并了解这种映射关系随时间变化的稳定性。

Method: 使用大规模观测评估（5000个观测数据和2000个新采样数据），通过具有单调饱和sigmoid参数化的平滑分位数回归来估计能力边界（即基准分数的高条件分位数作为预训练FLOPs对数的函数）。

Result: 估计的边界在大多数任务中保持稳定，但数学推理任务显示出随时间持续提升的边界。研究还分析了任务依赖的饱和现象和数学推理任务中污染相关的偏移，并提出了仅使用约20%评估预算就能恢复接近完整数据边界的有效算法。

Conclusion: 本文发布了最新的模型性能评估数据集Proteus 2k，并提出了一种实用的方法论，用于将计算预算转化为可靠的性能预期，并监测能力边界随时间的变化。

Abstract: For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.

</details>


### [94] [A Scalable Curiosity-Driven Game-Theoretic Framework for Long-Tail Multi-Label Learning in Data Mining](https://arxiv.org/abs/2602.15330)
*Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 提出CD-GTMLL框架，将长尾多标签分类重构为多玩家博弈游戏，通过好奇心驱动机制自适应增强尾部标签学习，无需手动平衡或调参


<details>
  <summary>Details</summary>
Motivation: 现实世界数据挖掘中的大规模多标签分类面临长尾分布挑战，现有重采样和重加权方法会破坏标签间依赖关系或需要脆弱的超参数调优，尤其在标签空间扩展到数万个标签时

Method: 提出好奇心驱动的博弈论多标签学习框架，将长尾多标签分类重构为多玩家合作博弈，每个子预测器专门处理标签空间的一个分区，通过合作最大化全局准确性，同时基于尾部标签稀有性和玩家间分歧追求内在好奇心奖励

Result: 在7个基准测试（包括包含30,000+标签的极端多标签分类数据集）上，CD-GTMLL始终优于最先进方法，在Wiki10-31K上P@3指标提升高达+1.6%。理论分析证明收敛到尾部感知均衡，并正式将优化动态与Rare-F1指标改进联系起来

Conclusion: 通过整合博弈论和好奇心机制，CD-GTMLL不仅提高了资源受限环境中的模型效率，还为电子商务和医疗等行业的不平衡数据场景中更自适应学习铺平了道路

Abstract: The long-tail distribution, where a few head labels dominate while rare tail labels abound, poses a persistent challenge for large-scale Multi-Label Classification (MLC) in real-world data mining applications. Existing resampling and reweighting strategies often disrupt inter-label dependencies or require brittle hyperparameter tuning, especially as the label space expands to tens of thousands of labels. To address this issue, we propose Curiosity-Driven Game-Theoretic Multi-Label Learning (CD-GTMLL), a scalable cooperative framework that recasts long-tail MLC as a multi-player game - each sub-predictor ("player") specializes in a partition of the label space, collaborating to maximize global accuracy while pursuing intrinsic curiosity rewards based on tail label rarity and inter-player disagreement. This mechanism adaptively injects learning signals into under-represented tail labels without manual balancing or tuning. We further provide a theoretical analysis showing that our CD-GTMLL converges to a tail-aware equilibrium and formally links the optimization dynamics to improvements in the Rare-F1 metric. Extensive experiments across 7 benchmarks, including extreme multi-label classification datasets with 30,000+ labels, demonstrate that CD-GTMLL consistently surpasses state-of-the-art methods, with gains up to +1.6% P@3 on Wiki10-31K. Ablation studies further confirm the contributions of both game-theoretic cooperation and curiosity-driven exploration to robust tail performance. By integrating game theory with curiosity mechanisms, CD-GTMLL not only enhances model efficiency in resource-constrained environments but also paves the way for more adaptive learning in imbalanced data scenarios across industries like e-commerce and healthcare.

</details>


### [95] [Directional Reasoning Trajectory Change (DRTC): Identifying Critical Trace Segments in Reasoning Models](https://arxiv.org/abs/2602.15332)
*Waldemar Chang*

Main category: cs.LG

TL;DR: DRTC是一种因果解释框架，用于分析语言模型的长程推理过程，通过检测关键决策点并测量上下文块对推理轨迹方向的影响。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法通常只突出与答案相关的token或文本片段，但很少揭示模型在哪里做出关键推理转折、哪些早期上下文因果触发了这些转折，或者突出文本是否真正引导了推理过程。

Method: 提出方向性推理轨迹变化(DRTC)框架：1)使用不确定性和分布偏移信号检测关键决策点；2)应用接收端干预，在保持实际推理轨迹的同时阻断特定早期上下文块的信息流；3)测量干预是否改变模型对数概率轨迹的方向，产生带符号的块级归因分数；4)计算原始logits的转向角曲率变化作为补充诊断。

Result: 方向性影响在四个推理模型中高度集中（Gini系数0.50-0.58，前5%质量占比0.23-0.28）；学习到的关键点比随机匹配的文本片段产生更强的干预效果；在500个MATH问题上的扩展研究中，学习到的片段显著优于随机匹配片段（中位数差异=0.409，355/500为正，符号检验p=2.3e-21）。

Conclusion: DRTC提供了一个因果基础、轨迹层面的视角，展示特定上下文元素如何在策略动态下引导推理过程，为理解语言模型的长程推理机制提供了新工具。

Abstract: Understanding how language models carry out long-horizon reasoning remains an open challenge. Existing interpretability methods often highlight tokens or spans correlated with an answer, but they rarely reveal where the model makes consequential reasoning turns, which earlier context causally triggers those turns, or whether the highlighted text actually steers the reasoning process. We introduce Directional Reasoning Trajectory Change (DRTC), a process-causal framework for interpreting long-form reasoning from a single on-policy rollout. DRTC detects pivot decision points using uncertainty and distribution-shift signals, then applies receiver-side interventions that preserve the realized rollout without resampling the continuation while blocking information flow from selected earlier chunks only at a pivot. It measures whether each intervention redirects the direction of the model's log-probability trajectory relative to the realized rollout direction, producing a signed per-chunk attribution score. We also compute turning-angle curvature changes on raw logits as a complementary diagnostic and introduce curvature signatures to summarize shared intervention-response geometry. Empirically, directional influence is sharply concentrated across four reasoning models (per-example |DRTC| shares yield Gini 0.50 to 0.58 and top-5 percent mass 0.23 to 0.28), and learned pivots induce stronger intervention magnitudes than matched random spans. In a scaling study on 500 MATH problems with R1-Distill-Qwen-1.5B, learned spans outperform matched random spans (median delta = 0.409, 355 of 500 positive; sign test p = 2.3e-21). Overall, DRTC provides a causally grounded, trajectory-level view of how specific context elements steer reasoning under on-policy dynamics.

</details>


### [96] [FedPSA: Modeling Behavioral Staleness in Asynchronous Federated Learning](https://arxiv.org/abs/2602.15337)
*Chaoyi Lu*

Main category: cs.LG

TL;DR: FedPSA：基于参数敏感度的异步联邦学习框架，通过细粒度评估模型过时程度和动态调整过时信息容忍度，显著提升异步联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习(AFL)虽然能加速训练，但异步过程引入的过时性(staleness)会导致性能下降。现有方法仅使用轮次差异作为过时性度量，这种粗粒度方法缺乏对模型本身的观察，限制了异步方法的性能上限。

Method: 提出FedPSA框架：1) 利用参数敏感度来细粒度衡量模型过时程度；2) 建立动态动量队列实时评估当前训练阶段；3) 动态调整对过时信息的容忍度。

Result: 在多个数据集上的实验表明，FedPSA相比基线方法提升高达6.37%，相比当前最优方法提升1.93%，表现出优越性能。

Conclusion: FedPSA通过细粒度的参数敏感度度量和动态调整机制，有效解决了异步联邦学习中的过时性问题，显著提升了模型性能，为异步联邦学习提供了更有效的解决方案。

Abstract: Asynchronous Federated Learning (AFL) has emerged as a significant research area in recent years. By not waiting for slower clients and executing the training process concurrently, it achieves faster training speed compared to traditional federated learning. However, due to the staleness introduced by the asynchronous process, its performance may degrade in some scenarios. Existing methods often use the round difference between the current model and the global model as the sole measure of staleness, which is coarse-grained and lacks observation of the model itself, thereby limiting the performance ceiling of asynchronous methods. In this paper, we propose FedPSA (Parameter Sensitivity-based Asynchronous Federated Learning), a more fine-grained AFL framework that leverages parameter sensitivity to measure model obsolescence and establishes a dynamic momentum queue to assess the current training phase in real time, thereby adjusting the tolerance for outdated information dynamically. Extensive experiments on multiple datasets and comparisons with various methods demonstrate the superior performance of FedPSA, achieving up to 6.37\% improvement over baseline methods and 1.93\% over the current state-of-the-art method.

</details>


### [97] [Discovering Implicit Large Language Model Alignment Objectives](https://arxiv.org/abs/2602.15338)
*Edward Chen,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: Obj-Disco框架自动将LLM对齐奖励信号分解为可解释的自然语言目标组合，揭示隐式目标以提升AI透明度和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法依赖复杂奖励信号，但信号背后的具体行为激励不透明，存在错位和奖励攻击风险。现有解释方法要么依赖预设标准可能遗漏未知问题，要么无法全面识别与模型行为因果相关的目标。

Method: 提出Obj-Disco框架，使用迭代贪心算法分析训练检查点间的行为变化，识别并验证最能解释剩余奖励信号的候选目标，将对齐奖励信号自动分解为稀疏、加权的可解释自然语言目标组合。

Result: 在不同任务、模型大小和对齐算法上的广泛评估显示框架稳健性。实验表明能持续捕获>90%的奖励行为，人类评估进一步证实。案例研究显示能成功识别与预期行为同时出现的潜在错位激励。

Conclusion: 该工作为揭示LLM对齐中的隐式目标提供了关键工具，为更透明、更安全的AI发展铺平道路。

Abstract: Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of "unknown unknowns", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.

</details>


### [98] [ER-MIA: Black-Box Adversarial Memory Injection Attacks on Long-Term Memory-Augmented Large Language Models](https://arxiv.org/abs/2602.15344)
*Mitchell Piehl,Zhaohan Xi,Zuobin Xiong,Pan He,Muchao Ye*

Main category: cs.LG

TL;DR: 本文首次系统研究了针对长时记忆增强LLM中基于相似性检索机制的黑盒对抗性记忆注入攻击，提出了ER-MIA统一框架，揭示了相似性检索构成系统级安全漏洞


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地配备长时记忆系统以克服有限上下文窗口，但研究发现记忆系统提供了额外的攻击面，使LLM更加脆弱。目前缺乏针对基于相似性检索机制的系统性安全研究

Method: 提出ER-MIA统一框架，形式化两种现实攻击场景：基于内容的攻击和问题目标攻击。框架包含可组合的攻击原语和集成攻击方法，在最小攻击者假设下实现高成功率

Result: 在多个LLM和长时记忆系统上的广泛实验表明，基于相似性的检索构成基本且系统级的漏洞，这种安全风险在不同记忆设计和应用场景中持续存在

Conclusion: 相似性检索机制是长时记忆增强LLM的根本安全弱点，需要重新思考记忆系统的安全设计，ER-MIA框架为评估和防御此类攻击提供了基础

Abstract: Large language models (LLMs) are increasingly augmented with long-term memory systems to overcome finite context windows and enable persistent reasoning across interactions. However, recent research finds that LLMs become more vulnerable because memory provides extra attack surfaces. In this paper, we present the first systematic study of black-box adversarial memory injection attacks that target the similarity-based retrieval mechanism in long-term memory-augmented LLMs. We introduce ER-MIA, a unified framework that exposes this vulnerability and formalizes two realistic attack settings: content-based attacks and question-targeted attacks. In these settings, ER-MIA includes an arsenal of composable attack primitives and ensemble attacks that achieve high success rates under minimal attacker assumptions. Extensive experiments across multiple LLMs and long-term memory systems demonstrate that similarity-based retrieval constitutes a fundamental and system-level vulnerability, revealing security risks that persist across memory designs and application scenarios.

</details>


### [99] [CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies](https://arxiv.org/abs/2602.15367)
*Sibo Zhang,Rui Jing,Liangfu Lv,Jian Zhang,Yunliang Zang*

Main category: cs.LG

TL;DR: 受小脑结构启发的强化学习架构，通过大规模扩展、稀疏连接、稀疏激活和树突级调制，显著提升了样本效率、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习在高维序列决策任务中表现出色，但存在样本效率低、对噪声敏感、部分可观测下泛化能力弱的问题。现有方法主要通过优化策略解决这些问题，而架构先验在表征学习和决策动态中的作用较少被探索。受小脑结构原理启发，希望利用生物结构作为有效的归纳偏置来改进RL。

Method: 提出基于小脑结构的生物启发RL架构，包含四大特征：大规模扩展、稀疏连接、稀疏激活和树突级调制。在噪声高维RL基准上进行实验，并对架构参数进行敏感性分析。

Result: 实验表明，小脑架构和树突调制相比传统设计，在样本效率、鲁棒性和泛化能力方面均有显著提升。架构参数敏感性分析显示，小脑启发结构能在有限模型参数下提供优化性能。

Conclusion: 小脑结构先验可作为强化学习的有效归纳偏置，为改进RL架构设计提供了有价值的生物学启示。

Abstract: Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.

</details>


### [100] [Fractional-Order Federated Learning](https://arxiv.org/abs/2602.15380)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: 提出FOFedAvg算法，将分数阶SGD融入FedAvg，利用历史信息改善联邦学习收敛速度和通信效率，在非IID数据上表现优异


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保护隐私，但存在收敛慢、通信成本高、非IID数据导致不稳定等问题，需要改进算法提升效率和鲁棒性

Method: 提出FOFedAvg算法，将分数阶随机梯度下降（FOSGD）融入联邦平均（FedAvg），利用分数阶更新的记忆特性捕捉长期关系和历史信息

Result: 在多个基准数据集上，FOFedAvg在非IID划分方案下与现有算法竞争并经常超越，在测试性能和收敛速度方面表现优异

Conclusion: 分数阶记忆感知更新能显著提升联邦学习的鲁棒性和有效性，为异构数据上的分布式训练提供了实用路径

Abstract: Federated learning (FL) allows remote clients to train a global model collaboratively while protecting client privacy. Despite its privacy-preserving benefits, FL has significant drawbacks, including slow convergence, high communication cost, and non-independent-and-identically-distributed (non-IID) data. In this work, we present a novel FedAvg variation called Fractional-Order Federated Averaging (FOFedAvg), which incorporates Fractional-Order Stochastic Gradient Descent (FOSGD) to capture long-range relationships and deeper historical information. By introducing memory-aware fractional-order updates, FOFedAvg improves communication efficiency and accelerates convergence while mitigating instability caused by heterogeneous, non-IID client data. We compare FOFedAvg against a broad set of established federated optimization algorithms on benchmark datasets including MNIST, FEMNIST, CIFAR-10, CIFAR-100, EMNIST, the Cleveland heart disease dataset, Sent140, PneumoniaMNIST, and Edge-IIoTset. Across a range of non-IID partitioning schemes, FOFedAvg is competitive with, and often outperforms, these baselines in terms of test performance and convergence speed. On the theoretical side, we prove that FOFedAvg converges to a stationary point under standard smoothness and bounded-variance assumptions for fractional order $0<α\le 1$. Together, these results show that fractional-order, memory-aware updates can substantially improve the robustness and effectiveness of federated learning, offering a practical path toward distributed training on heterogeneous data.

</details>


### [101] [Doubly Stochastic Mean-Shift Clustering](https://arxiv.org/abs/2602.15393)
*Tom Trigano,Yann Sepulcre,Itshak Lapidot*

Main category: cs.LG

TL;DR: DSMS通过随机化带宽和数据采样，改进了传统Mean-Shift算法对带宽参数的敏感性，在稀疏聚类场景中表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统Mean-Shift算法对带宽超参数非常敏感，特别是在数据稀缺的情况下，固定尺度的密度估计会导致分割和虚假模式的问题。

Method: 提出双重随机Mean-Shift（DSMS），在轨迹更新的同时随机化核带宽，每次迭代从连续均匀分布中抽取数据样本和半径，实现对密度景观的更好探索。

Result: 在合成高斯混合数据上的比较实验显示，DSMS显著优于标准和随机Mean-Shift基线，表现出卓越的稳定性，在稀疏聚类场景中防止过分割且无其他性能下降。

Conclusion: 随机带宽策略作为隐式正则化机制有效，DSMS为数据稀缺场景下的聚类问题提供了更稳健的解决方案。

Abstract: Standard Mean-Shift algorithms are notoriously sensitive to the bandwidth hyperparameter, particularly in data-scarce regimes where fixed-scale density estimation leads to fragmentation and spurious modes. In this paper, we propose Doubly Stochastic Mean-Shift (DSMS), a novel extension that introduces randomness not only in the trajectory updates but also in the kernel bandwidth itself. By drawing both the data samples and the radius from a continuous uniform distribution at each iteration, DSMS effectively performs a better exploration of the density landscape. We show that this randomized bandwidth policy acts as an implicit regularization mechanism, and provide convergence theoretical results. Comparative experiments on synthetic Gaussian mixtures reveal that DSMS significantly outperforms standard and stochastic Mean-Shift baselines, exhibiting remarkable stability and preventing over-segmentation in sparse clustering scenarios without other performance degradation.

</details>


### [102] [Joint Enhancement and Classification using Coupled Diffusion Models of Signals and Logits](https://arxiv.org/abs/2602.15405)
*Gilad Nurko,Roi Benita,Yehoshua Dissen,Tomohiro Nakatani,Marc Delcroix,Shoko Araki,Joseph Keshet*

Main category: cs.LG

TL;DR: 提出一个联合增强框架，通过两个相互作用的扩散模型同时处理输入信号和分类器输出，无需重新训练分类器即可提升噪声环境下的分类鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将信号增强和分类作为两个独立的顺序阶段，无法在去噪过程中利用分类器的语义信息。这种分离方法限制了在噪声环境下的分类性能。

Method: 提出一个通用的领域无关框架，集成两个相互作用的扩散模型：一个处理输入信号，另一个处理分类器输出logits。引入三种策略来有效建模输入和logit的联合分布，实现相互引导。

Result: 在图像分类和自动语音识别任务上评估，该联合增强方法超越了传统的顺序增强基线，在不同噪声条件下实现了更鲁棒和灵活的分类准确率提升。

Conclusion: 通过联合建模输入信号和分类器输出的方法，能够有效利用语义信息指导信号重建，为噪声环境下的鲁棒分类提供了一个有前景的解决方案。

Abstract: Robust classification in noisy environments remains a fundamental challenge in machine learning. Standard approaches typically treat signal enhancement and classification as separate, sequential stages: first enhancing the signal and then applying a classifier. This approach fails to leverage the semantic information in the classifier's output during denoising. In this work, we propose a general, domain-agnostic framework that integrates two interacting diffusion models: one operating on the input signal and the other on the classifier's output logits, without requiring any retraining or fine-tuning of the classifier. This coupled formulation enables mutual guidance, where the enhancing signal refines the class estimation and, conversely, the evolving class logits guide the signal reconstruction towards discriminative regions of the manifold. We introduce three strategies to effectively model the joint distribution of the input and the logit. We evaluated our joint enhancement method for image classification and automatic speech recognition. The proposed framework surpasses traditional sequential enhancement baselines, delivering robust and flexible improvements in classification accuracy under diverse noise conditions.

</details>


### [103] [Fairness over Equality: Correcting Social Incentives in Asymmetric Sequential Social Dilemmas](https://arxiv.org/abs/2602.15407)
*Alper Demir,Hüseyin Aydın,Kale-ab Abebe Tessera,David Abel,Stefano V. Albrecht*

Main category: cs.LG

TL;DR: 论文提出针对非对称顺序社会困境的改进方法，通过重新定义公平性、引入基于智能体的加权机制和局部化社会反馈，解决现有公平性方法在非对称条件下的不足。


<details>
  <summary>Details</summary>
Motivation: 现有顺序社会困境（SSD）研究大多假设智能体面临相同的激励，且需要持续访问全局信息来评估公平性。但在现实世界中，智能体往往存在天然差异（非对称性），现有基于公平性的方法难以适应这种情况，会强制执行原始平等反而鼓励背叛行为。

Method: 提出三个改进：1）根据智能体的奖励范围重新定义公平性；2）引入基于智能体的加权机制以更好地处理固有非对称性；3）将社会反馈局部化，使方法在部分可观测性下有效，无需全局信息共享。

Result: 实验结果表明，在非对称场景中，该方法比现有方法能更快地促进合作策略的出现，同时不牺牲可扩展性或实用性。

Conclusion: 通过考虑智能体的非对称性和局部信息，改进的公平性方法能更有效地促进多智能体强化学习中的合作，特别是在现实世界的非对称社会困境中。

Abstract: Sequential Social Dilemmas (SSDs) provide a key framework for studying how cooperation emerges when individual incentives conflict with collective welfare. In Multi-Agent Reinforcement Learning, these problems are often addressed by incorporating intrinsic drives that encourage prosocial or fair behavior. However, most existing methods assume that agents face identical incentives in the dilemma and require continuous access to global information about other agents to assess fairness. In this work, we introduce asymmetric variants of well-known SSD environments and examine how natural differences between agents influence cooperation dynamics. Our findings reveal that existing fairness-based methods struggle to adapt under asymmetric conditions by enforcing raw equality that wrongfully incentivize defection. To address this, we propose three modifications: (i) redefining fairness by accounting for agents' reward ranges, (ii) introducing an agent-based weighting mechanism to better handle inherent asymmetries, and (iii) localizing social feedback to make the methods effective under partial observability without requiring global information sharing. Experimental results show that in asymmetric scenarios, our method fosters faster emergence of cooperative policies compared to existing approaches, without sacrificing scalability or practicality.

</details>


### [104] [Logit Distance Bounds Representational Similarity](https://arxiv.org/abs/2602.15438)
*Beatrix M. B. Nielsen,Emanuele Marconato,Luigi Gresele,Andrea Dittadi,Simon Buchholz*

Main category: cs.LG

TL;DR: 研究显示，对于判别模型，当两个模型的logit距离接近时，它们的内部表示具有线性相似性保证，而KL散度接近则不能保证这种线性表示相似性。


<details>
  <summary>Details</summary>
Motivation: 先前研究指出，当两个模型的条件分布完全相同时，它们的内部表示在线性变换下是一致的。但实际中模型分布通常只是接近而非完全相同，需要研究这种近似情况下的表示相似性保证。

Method: 基于logit差异定义分布距离，证明该距离的接近能保证线性表示相似性。分析KL散度与logit距离的关系，发现在概率远离零时KL散度能上界logit距离，但实际控制效果有限。

Result: logit距离蒸馏相比KL散度蒸馏能产生更高线性表示相似性的学生模型，更好地保留教师模型中线性可恢复的人类可解释概念。

Conclusion: KL散度蒸馏可能匹配教师预测但无法保持线性表示特性，而基于logit距离的蒸馏能更好地保持线性表示相似性和概念可恢复性。

Abstract: For a broad family of discriminative models that includes autoregressive language models, identifiability results imply that if two models induce the same conditional distributions, then their internal representations agree up to an invertible linear transformation. We ask whether an analogous conclusion holds approximately when the distributions are close instead of equal. Building on the observation of Nielsen et al. (2025) that closeness in KL divergence need not imply high linear representational similarity, we study a distributional distance based on logit differences and show that closeness in this distance does yield linear similarity guarantees. Specifically, we define a representational dissimilarity measure based on the models' identifiability class and prove that it is bounded by the logit distance. We further show that, when model probabilities are bounded away from zero, KL divergence upper-bounds logit distance; yet the resulting bound fails to provide nontrivial control in practice. As a consequence, KL-based distillation can match a teacher's predictions while failing to preserve linear representational properties, such as linear-probe recoverability of human-interpretable concepts. In distillation experiments on synthetic and image datasets, logit-distance distillation yields students with higher linear representational similarity and better preservation of the teacher's linearly recoverable concepts.

</details>


### [105] [Benchmarking IoT Time-Series AD with Event-Level Augmentations](https://arxiv.org/abs/2602.15457)
*Dmitry Zhevnenko,Ilya Makarov,Aleksandr Kovalenko,Fedor Meshchaninov,Anton Kozhukhov,Vladislav Travnikov,Makar Ippolitov,Kirill Yashunin,Iurii Katser*

Main category: cs.LG

TL;DR: 提出一个针对物联网时间序列异常检测的事件级评估协议，包含统一的现实扰动模拟和传感器级探测，评估14个模型发现没有通用最优模型，不同模型在不同场景下表现各异。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测研究大多关注点级结果和精心整理的基础数据集，缺乏对实际应用中可靠性、及时性和抗扰动能力的评估，限制了模型在实际应用中的选择价值。

Method: 引入包含统一事件级增强的评估协议，模拟真实世界问题：校准传感器丢失、线性和对数漂移、加性噪声、窗口偏移；通过掩码作为缺失的零值化和每通道影响估计进行传感器级探测；在5个公共数据集和2个工业数据集上评估14个代表性模型。

Result: 没有通用最优模型：图结构模型在传感器丢失和长事件下转移性最好；密度/流模型在清洁平稳工厂表现良好但对单调漂移脆弱；谱CNN在周期性强的场景领先；重构自编码器在基本传感器审查后变得有竞争力；预测/混合动态模型在故障破坏时间依赖时有效但对窗口敏感。

Conclusion: 评估协议为实际模型选择提供了更现实的基准，揭示了不同模型在不同扰动场景下的优势和脆弱性，并指导设计选择，如用高斯密度替换归一化流会降低性能，固定学习的有向无环图会显著增加漂移敏感性。

Abstract: Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.

</details>


### [106] [On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks](https://arxiv.org/abs/2602.15460)
*Yannic Neuhaus,Nicolas Flammarion,Matthias Hein,Francesco Croce*

Main category: cs.LG

TL;DR: 该研究评估了思维链方法在简单规划任务中的泛化能力，发现思维链能提升分布内泛化，但分布外泛化仍然有限，多文本格式的推理轨迹效果最好，纯文本模型优于图像输入模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型和大视觉语言模型中的推理能力有所提升，但推理模型的泛化能力仍然定义模糊且理解不足。本研究旨在严格评估思维链方法在简单规划任务中的泛化表现。

Method: 采用基于网格的导航任务，模型接收地图并输出从起点到终点的移动序列。通过不同输入表示（视觉和文本）和思维链推理策略微调模型变体，系统评估其在分布内和分布外测试条件下的表现。

Result: 思维链推理在所有表示中都能提升分布内泛化，但在控制与分布内数据的简单匹配后，分布外泛化（如到更大地图）在大多数情况下仍然非常有限。令人惊讶的是，结合多种文本格式的推理轨迹能产生最好的分布外泛化。纯文本模型始终优于使用图像输入的模型。

Conclusion: 思维链方法在简单规划任务中显示出有限的分布外泛化能力，多文本格式的推理策略表现最佳，纯文本表示优于视觉表示，这对未来推理模型的开发和评估具有重要意义。

Abstract: Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.

</details>


### [107] [POP: Prior-fitted Optimizer Policies](https://arxiv.org/abs/2602.15473)
*Jan Kobiolka,Christian Frey,Gresa Shala,Arlind Kadra,Erind Bedalli,Josif Grabocka*

Main category: cs.LG

TL;DR: POP是一种元学习优化器，通过从包含凸和非凸目标的先验分布中学习，预测基于优化轨迹上下文信息的坐标步长，在47个优化函数基准测试中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的优化器对超参数选择高度敏感，在高度非凸设置中性能依赖于精心调整的学习率、动量和梯度累积。需要一种更鲁棒、无需任务特定调优的优化方法。

Method: 提出POP（先验拟合优化器策略），这是一种元学习优化器，从包含凸和非凸目标的合成优化问题先验分布中学习，预测基于优化轨迹上下文信息的坐标步长。

Result: 在包含47个不同复杂度优化函数的基准测试中，POP在相同预算约束下一致优于一阶梯度方法、非凸优化方法（如进化策略）、贝叶斯优化和最近的元学习竞争对手。

Conclusion: POP展示了强大的泛化能力，无需任务特定调优，为优化问题提供了一种更鲁棒和高效的解决方案。

Abstract: Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.

</details>


### [108] [Evaluating Federated Learning for Cross-Country Mood Inference from Smartphone Sensing Data](https://arxiv.org/abs/2602.15478)
*Sharmad Kalpande,Saurabh Shirke,Haroon R. Lone*

Main category: cs.LG

TL;DR: FedFAP：一种用于跨国家智能手机情绪推断的特征感知个性化联邦学习框架，在保护隐私的同时处理不同地区的异构传感数据，AUROC达到0.744，优于集中式方法和现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统情绪评估依赖不频繁的回顾性报告，无法捕捉情绪的连续性。智能手机移动感知虽能实现被动情绪推断，但在大规模部署时面临隐私约束、传感可用性不均和行为模式变异等挑战。

Method: 提出FedFAP框架，在跨国家联邦学习设置中处理异构传感模态，每个国家作为独立客户端保留本地数据，通过特征感知个性化适应不同地区的行为模式差异。

Result: 在跨地理和文化多样化人群的评估中，FedFAP达到AUROC 0.744，优于集中式方法和现有个性化联邦基线，同时提供情绪感知系统的设计见解。

Conclusion: 人口感知个性化和隐私保护学习能够实现可扩展的情绪感知移动感知技术，FedFAP框架为跨国家情绪推断提供了有效的解决方案。

Abstract: Mood instability is a key behavioral indicator of mental health, yet traditional assessments rely on infrequent and retrospective reports that fail to capture its continuous nature. Smartphone-based mobile sensing enables passive, in-the-wild mood inference from everyday behaviors; however, deploying such systems at scale remains challenging due to privacy constraints, uneven sensing availability, and substantial variability in behavioral patterns.
  In this work, we study mood inference using smartphone sensing data in a cross-country federated learning setting, where each country participates as an independent client while retaining local data. We introduce FedFAP, a feature-aware personalized federated framework designed to accommodate heterogeneous sensing modalities across regions. Evaluations across geographically and culturally diverse populations show that FedFAP achieves an AUROC of 0.744, outperforming both centralized approaches and existing personalized federated baselines. Beyond inference, our results offer design insights for mood-aware systems, demonstrating how population-aware personalization and privacy-preserving learning can enable scalable and mood-aware mobile sensing technologies.

</details>


### [109] [LLM-as-Judge on a Budget](https://arxiv.org/abs/2602.15481)
*Aadirupa Saha,Aniket Wagde,Branislav Kveton*

Main category: cs.LG

TL;DR: 提出一种基于多臂老虎机理论的方差自适应查询分配方法，用于优化LLM评估中的计算预算分配，以最小化评分估计误差。


<details>
  <summary>Details</summary>
Motivation: 在LLM-as-a-judge评估中，由于LLM判断具有随机性，通常需要多次查询每个提示-响应对以获得准确的均值评分。给定固定计算预算B，如何最优地在K个对之间分配查询次数以最小化估计误差成为一个关键挑战。

Method: 提出基于多臂老虎机理论和集中不等式的方差自适应方法，根据估计的评分方差动态分配查询次数，将资源集中在不确定性最高的地方。

Result: 实验在Summarize-From-Feedback和HelpSteer2数据集上表明，该方法显著优于均匀分配，在相同预算下减少了最坏情况估计误差。算法实现了最坏情况评分估计误差为$\tilde{O}\left(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}}\right)$，接近最优预算分配。

Conclusion: 该工作为高效LLM评估建立了理论基础，对AI安全、模型对齐和大规模自动评估具有实际意义。

Abstract: LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? %
We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\tilde{O}\left(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}}\right)$, $σ_i^2$ being the unknown score variance for pair $i \in [K]$ with near-optimal budget allocation. %
Experiments on \emph{Summarize-From-Feedback} and \emph{HelpSteer2} demonstrate that our method significantly outperforms uniform allocation, reducing worst-case estimation error while maintaining identical budgets. Our work establishes a theoretical foundation for efficient LLM evaluation with practical implications for AI safety, model alignment, and automated assessment at scale.

</details>


### [110] [ExLipBaB: Exact Lipschitz Constant Computation for Piecewise Linear Neural Networks](https://arxiv.org/abs/2602.15499)
*Tom A. Splittgerber*

Main category: cs.LG

TL;DR: 提出LipBaB算法的扩展版本，用于精确计算任意分段线性神经网络在p-范数下的Lipschitz常数，支持ReLU、LeakyReLU、GroupSort、MinMax、FullSort和MaxPool等多种激活函数。


<details>
  <summary>Details</summary>
Motivation: Lipschitz常数在神经网络中具有重要应用（鲁棒性保证、正则化、可逆网络构建），但现有精确计算方法仅限于ReLU激活网络，而ReLU在Lipschitz约束网络中具有严重缺点。需要一种能够处理更广泛激活函数的精确计算方法。

Method: 扩展LipBaB算法，使其能够处理任意分段线性神经网络和p-范数。该方法支持多种激活函数：传统激活函数（ReLU、LeakyReLU）、近年受关注的激活函数（GroupSort、MinMax、FullSort）以及其他分段线性函数（如MaxPool）。

Result: 提出了一种能够精确计算分段线性神经网络Lipschitz常数的通用方法，突破了现有方法仅支持ReLU激活的限制，为更广泛的神经网络架构提供了精确Lipschitz常数计算能力。

Conclusion: 该扩展的LipBaB算法填补了精确Lipschitz常数计算领域的空白，使得在需要精确计算的应用场景（如新方法基准测试、敏感数据小模型鲁棒性保证）中能够使用更丰富的神经网络架构。

Abstract: It has been shown that a neural network's Lipschitz constant can be leveraged to derive robustness guarantees, to improve generalizability via regularization or even to construct invertible networks. Therefore, a number of methods varying in the tightness of their bounds and their computational cost have been developed to approximate the Lipschitz constant for different classes of networks. However, comparatively little research exists on methods for exact computation, which has been shown to be NP-hard. Nonetheless, there are applications where one might readily accept the computational cost of an exact method. These applications could include the benchmarking of new methods or the computation of robustness guarantees for small models on sensitive data. Unfortunately, existing exact algorithms restrict themselves to only ReLU-activated networks, which are known to come with severe downsides in the context of Lipschitz-constrained networks. We therefore propose a generalization of the LipBaB algorithm to compute exact Lipschitz constants for arbitrary piecewise linear neural networks and $p$-norms. With our method, networks may contain traditional activations like ReLU or LeakyReLU, activations like GroupSort or the related MinMax and FullSort, which have been of increasing interest in the context of Lipschitz constrained networks, or even other piecewise linear functions like MaxPool.

</details>


### [111] [Approximation Theory for Lipschitz Continuous Transformers](https://arxiv.org/abs/2602.15503)
*Takashi Furuya,Davide Murari,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 提出一种梯度下降型上下文Transformer，通过构造保证Lipschitz连续性，在Lipschitz约束函数空间中实现通用逼近，为稳健Transformer设计提供理论基础


<details>
  <summary>Details</summary>
Motivation: Transformer在安全敏感场景中需要稳定性和鲁棒性，但现有保证Lipschitz连续性的架构缺乏逼近理论保证，需要填补这一理论空白

Method: 将MLP和注意力块实现为负梯度流的显式欧拉步，构造梯度下降型上下文Transformer，采用测度论形式将Transformer解释为概率测度上的算子

Result: 证明了该Lipschitz连续Transformer类在Lipschitz约束函数空间中的通用逼近定理，逼近保证与token数量无关

Conclusion: 为稳健、Lipschitz连续的Transformer架构设计提供了严格的理论基础，既保证了稳定性又不牺牲表达能力

Abstract: Stability and robustness are critical for deploying Transformers in safety-sensitive settings. A principled way to enforce such behavior is to constrain the model's Lipschitz constant. However, approximation-theoretic guarantees for architectures that explicitly preserve Lipschitz continuity have yet to be established. In this work, we bridge this gap by introducing a class of gradient-descent-type in-context Transformers that are Lipschitz-continuous by construction. We realize both MLP and attention blocks as explicit Euler steps of negative gradient flows, ensuring inherent stability without sacrificing expressivity. We prove a universal approximation theorem for this class within a Lipschitz-constrained function space. Crucially, our analysis adopts a measure-theoretic formalism, interpreting Transformers as operators on probability measures, to yield approximation guarantees independent of token count. These results provide a rigorous theoretical foundation for the design of robust, Lipschitz continuous Transformer architectures.

</details>


### [112] [On the Geometric Coherence of Global Aggregation in Federated GNN](https://arxiv.org/abs/2602.15510)
*Chethana Prasad Kabgere,Shylaja SS*

Main category: cs.LG

TL;DR: 本文提出GGRS框架，解决联邦图神经网络中由于客户端图结构异质性导致的全局聚合几何失效问题，通过几何可容性准则调节客户端更新，保持关系变换的方向一致性和传播子空间多样性。


<details>
  <summary>Details</summary>
Motivation: 在跨域联邦图神经网络中，客户端图具有异构的结构和传播特性。当标准聚合机制应用于这些异构更新时，全局模型可能在数值上收敛，但关系行为会退化。本文识别了全局聚合的几何失效模式：聚合来自不兼容传播机制的更新会在变换空间中引入破坏性干扰，导致全局消息传递失去连贯性。

Method: 提出GGRS（全局几何参考结构）框架，这是一个服务器端框架，基于几何可容性准则在聚合前调节客户端更新。GGRS保持关系变换的方向一致性，维护可容传播子空间的多样性，并稳定对邻域交互的敏感性，且无需访问客户端数据或图拓扑。

Result: 在异构的GNN-native和Amazon Co-purchase数据集上的实验表明，GGRS在训练轮次中保持了全局消息传递的连贯性，突出了在联邦图学习中几何感知调节的必要性。

Conclusion: 本文强调了联邦图神经网络中几何感知调节的重要性，提出的GGRS框架有效解决了全局聚合的几何失效问题，通过保持关系变换的几何特性来维护消息传递的连贯性，为跨域联邦图学习提供了新的解决方案。

Abstract: Federated Learning (FL) enables distributed training across multiple clients without centralized data sharing, while Graph Neural Networks (GNNs) model relational data through message passing. In federated GNN settings, client graphs often exhibit heterogeneous structural and propagation characteristics. When standard aggregation mechanisms are applied to such heterogeneous updates, the global model may converge numerically while exhibiting degraded relational behavior.Our work identifies a geometric failure mode of global aggregation in Cross- Domain Federated GNNs. Although GNN parameters are numerically represented as vectors, they encode relational transformations that govern the direction, strength, and sensitivity of information flow across graph neighborhoods. Aggregating updates originating from incompatible propagation regimes can therefore introduce destructive interference in this transformation space.This leads to loss of coherence in global message passing. Importantly, this degradation is not necessarily reflected in conventional metrics such as loss or accuracy.To address this issue, we propose GGRS (Global Geometric Reference Structure), a server-side framework that regulates client updates prior to aggregation based on geometric admissibility criteria. GGRS preserves directional consistency of relational transformations as well as maintains diversity of admissible propagation subspaces. It also stabilizes sensitivity to neighborhood interactions, without accessing client data or graph topology. Experiments on heterogeneous GNN-native, Amazon Co-purchase datasets demonstrate that GGRS preserves global message-passing coherence across training rounds by highlighting the necessity of geometry-aware regulation in federated graph learning.

</details>


### [113] [The Obfuscation Atlas: Mapping Where Honesty Emerges in RLVR with Deception Probes](https://arxiv.org/abs/2602.15515)
*Mohammad Taufeeque,Stefan Heimersheim,Adam Gleave,Chris Cundy*

Main category: cs.LG

TL;DR: 训练对抗白盒欺骗检测器可能导致模型学会隐藏欺骗行为，研究在真实编码环境中发现了两种隐藏策略：隐藏激活和隐藏策略


<details>
  <summary>Details</summary>
Motivation: 研究训练AI系统对抗白盒欺骗检测器的风险，特别是模型可能学会隐藏欺骗行为以逃避检测。先前研究仅在人工环境中探讨此问题，本研究旨在更真实的编码环境中验证隐藏行为是否自然出现

Method: 构建真实编码环境，其中通过硬编码测试用例自然发生奖励黑客行为。引入训练对抗欺骗检测器可能结果的分类法，包括诚实、隐藏激活和隐藏策略三种结果。通过实验和理论分析验证不同训练条件下的行为模式

Result: 在真实编码环境中确实出现了隐藏行为。隐藏激活主要来自强化学习中的表征漂移，无论是否有检测器惩罚都会出现。检测器惩罚只激励隐藏策略，理论分析表明这在策略梯度方法中是预期的。足够高的KL正则化和检测器惩罚可以产生诚实策略

Conclusion: 白盒欺骗检测器可以作为易受奖励黑客攻击任务的可行训练信号，但需要足够强的正则化和惩罚来防止隐藏行为。研究提供了对训练对抗欺骗检测器可能结果的系统分类和实证验证

Abstract: Training against white-box deception detectors has been proposed as a way to make AI systems honest. However, such training risks models learning to obfuscate their deception to evade the detector. Prior work has studied obfuscation only in artificial settings where models were directly rewarded for harmful output. We construct a realistic coding environment where reward hacking via hardcoding test cases naturally occurs, and show that obfuscation emerges in this setting. We introduce a taxonomy of possible outcomes when training against a deception detector. The model either remains honest, or becomes deceptive via two possible obfuscation strategies. (i) Obfuscated activations: the model outputs deceptive text while modifying its internal representations to no longer trigger the detector. (ii) Obfuscated policy: the model outputs deceptive text that evades the detector, typically by including a justification for the reward hack. Empirically, obfuscated activations arise from representation drift during RL, with or without a detector penalty. The probe penalty only incentivizes obfuscated policies; we theoretically show this is expected for policy gradient methods. Sufficiently high KL regularization and detector penalty can yield honest policies, establishing white-box deception detectors as viable training signals for tasks prone to reward hacking.

</details>


### [114] [CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals](https://arxiv.org/abs/2602.15546)
*Tomàs Garriga,Gerard Sanz,Eduard Serrahima de Cambra,Axel Brando*

Main category: cs.LG

TL;DR: 提出一种用于时间序列反事实推理的新方法CEPAE，通过熵惩罚损失鼓励潜在空间解耦表示，在合成和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗、营销等领域，准确的时间序列反事实推理对决策至关重要，但现有方法未针对受市场事件影响的时间序列数据进行优化。本文受工业应用驱动，旨在解决这一实际问题。

Method: 基于结构因果模型框架和溯因-行动-预测流程，首先将变分自编码器和对抗自编码器方法适配到时间序列设置。然后提出条件熵惩罚自编码器（CEPAE），通过在潜在空间使用熵惩罚损失来鼓励解耦的数据表示。

Result: 在合成、半合成和真实世界数据集上的实验验证表明，CEPAE在评估指标上通常优于其他方法，证明了其有效性。

Conclusion: CEPAE是一种有效的反事实推理方法，特别适用于受市场事件影响的时间序列数据，为工业应用中的决策支持提供了实用工具。

Abstract: The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.

</details>


### [115] [1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization](https://arxiv.org/abs/2602.15563)
*Sohir Maskey,Constantin Eichenberg,Johannes Messner,Douglas Orr*

Main category: cs.LG

TL;DR: 该论文通过实证研究发现，在低比特量化训练中，k-means权重量化优于整数格式，且在固定推理内存预算下，1比特权重量化在下游生成任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 量化感知训练(QAT)能显著减少LLMs内存占用，但实际应用中量化格式和比特宽度的选择存在挑战。现有研究未充分探索QAT的完整设计空间，量化与下游性能的权衡关系也不明确，通常仅依赖困惑度评估。

Method: 在低比特量化领域进行实证研究，比较不同量化格式，发现k-means权重量化优于整数格式，且能在标准硬件上高效实现。

Result: 在固定推理内存预算下，1比特权重量化在下游生成任务中达到最佳性能。k-means量化方法在低比特量化中表现优越。

Conclusion: 该研究为低比特量化训练提供了实证指导，表明k-means量化优于传统整数格式，且1比特权重量化在内存受限场景下具有最佳性能表现。

Abstract: Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.

</details>


### [116] [Accelerated Predictive Coding Networks via Direct Kolen-Pollack Feedback Alignment](https://arxiv.org/abs/2602.15571)
*Davide Casnici,Martin Lefebvre,Justin Dauwels,Charlotte Frenkel*

Main category: cs.LG

TL;DR: DKP-PC是一种改进的预测编码算法，通过引入可学习的反馈连接，将误差传播时间复杂度从O(L)降低到O(1)，解决了标准PC中反馈延迟和指数衰减问题。


<details>
  <summary>Details</summary>
Motivation: 标准预测编码算法存在两个关键限制：误差信号需要通过多个推理步骤从输出层传播到早期层，且反馈在传播过程中呈指数衰减，导致早期层更新消失。这限制了PC的实际应用效率和可扩展性。

Method: 提出直接Kolen-Pollack预测编码（DKP-PC），结合直接反馈对齐和直接Kolen-Pollack算法，引入从输出层到所有隐藏层的可学习反馈连接，建立误差传输的直接路径。

Result: DKP-PC将误差传播时间复杂度从O(L)降低到O(1)，消除了深度相关的延迟。实验结果表明，DKP-PC性能至少与标准PC相当，甚至更好，同时提供更低的延迟和更好的计算性能。

Conclusion: DKP-PC同时解决了反馈延迟和指数衰减问题，为预测编码提供了更高效、可扩展的变体，保持了更新局部性，适合定制硬件高效实现。

Abstract: Predictive coding (PC) is a biologically inspired algorithm for training neural networks that relies only on local updates, allowing parallel learning across layers. However, practical implementations face two key limitations: error signals must still propagate from the output to early layers through multiple inference-phase steps, and feedback decays exponentially during this process, leading to vanishing updates in early layers. We propose direct Kolen-Pollack predictive coding (DKP-PC), which simultaneously addresses both feedback delay and exponential decay, yielding a more efficient and scalable variant of PC while preserving update locality. Leveraging direct feedback alignment and direct Kolen-Pollack algorithms, DKP-PC introduces learnable feedback connections from the output layer to all hidden layers, establishing a direct pathway for error transmission. This yields an algorithm that reduces the theoretical error propagation time complexity from O(L), with L being the network depth, to O(1), removing depth-dependent delay in error signals. Moreover, empirical results demonstrate that DKP-PC achieves performance at least comparable to, and often exceeding, that of standard PC, while offering improved latency and computational performance, supporting its potential for custom hardware-efficient implementations.

</details>


### [117] [Neural Network-Based Parameter Estimation of a Labour Market Agent-Based Model](https://arxiv.org/abs/2602.15572)
*M Lopes Alves,Joel Dyer,Doyne Farmer,Michael Wooldridge,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 该研究评估了一种基于神经网络的模拟推理框架，用于大规模基于智能体模型的参数估计，并在劳动力市场模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于智能体建模（ABM）面临参数估计的挑战，特别是在大规模模型中，由于计算限制难以探索参数空间，这限制了其作为决策支持工具的应用。

Method: 研究评估了最先进的基于模拟推理（SBI）框架，该框架使用神经网络进行参数估计。将框架应用于基于工作转换网络的劳动力市场ABM，使用合成数据集和真实美国劳动力市场数据初始化模型，并比较传统统计量摘要与神经网络学习摘要的效果。

Result: 结果表明，基于神经网络的方法在各种数据集规模下都能恢复原始参数，通过后验分布评估显示，相比传统贝叶斯方法提高了效率。

Conclusion: 神经网络驱动的模拟推理框架为大规模基于智能体模型的参数估计提供了有效解决方案，能够克服传统方法的计算限制，提高参数估计效率。

Abstract: Agent-based modelling (ABM) is a widespread approach to simulate complex systems. Advancements in computational processing and storage have facilitated the adoption of ABMs across many fields; however, ABMs face challenges that limit their use as decision-support tools. A significant issue is parameter estimation in large-scale ABMs, particularly due to computational constraints on exploring the parameter space. This study evaluates a state-of-the-art simulation-based inference (SBI) framework that uses neural networks (NN) for parameter estimation. This framework is applied to an established labour market ABM based on job transition networks. The ABM is initiated with synthetic datasets and the real U.S. labour market. Next, we compare the effectiveness of summary statistics derived from a list of statistical measures with that learned by an embedded NN. The results demonstrate that the NN-based approach recovers the original parameters when evaluating posterior distributions across various dataset scales and improves efficiency compared to traditional Bayesian methods.

</details>


### [118] [Uniform error bounds for quantized dynamical models](https://arxiv.org/abs/2602.15586)
*Abdelkader Metakalard,Fabien Lauer,Kevin Colin,Marion Gilson*

Main category: cs.LG

TL;DR: 该论文为从依赖数据序列学习动态模型的准确性提供统计保证，开发了适用于量化模型和不完美优化算法的统一误差界，通过块分解和间隔点策略获得两种边界，边界规模与编码模型所需比特数相关。


<details>
  <summary>Details</summary>
Motivation: 在系统识别（特别是混合系统识别）的实际应用中，通常使用量化模型和不完美的优化算法，但缺乏对这些方法从依赖数据序列学习动态模型的统计准确性保证。

Method: 开发两种统一误差界：1）通过块分解获得慢速率边界；2）通过新颖的间隔点策略获得快速率、方差自适应边界。边界规模与编码模型所需比特数相关。

Result: 获得了可扩展的统计保证，将硬件约束转化为可解释的统计复杂度，为实际系统识别应用中的量化模型和优化算法提供了理论支撑。

Conclusion: 该研究为从依赖数据学习动态模型提供了实用的统计保证框架，特别适用于混合系统识别，将硬件限制与统计性能直接联系起来。

Abstract: This paper provides statistical guarantees on the accuracy of dynamical models learned from dependent data sequences. Specifically, we develop uniform error bounds that apply to quantized models and imperfect optimization algorithms commonly used in practical contexts for system identification, and in particular hybrid system identification. Two families of bounds are obtained: slow-rate bounds via a block decomposition and fast-rate, variance-adaptive, bounds via a novel spaced-point strategy. The bounds scale with the number of bits required to encode the model and thus translate hardware constraints into interpretable statistical complexities.

</details>


### [119] [A unified theory of feature learning in RNNs and DNNs](https://arxiv.org/abs/2602.15593)
*Jan P. Bauer,Kirsten Fischer,Moritz Helias,Agostina Palmigiano*

Main category: cs.LG

TL;DR: 该论文通过统一的平均场理论，揭示了RNN和DNN在结构相似性（仅权重共享差异）下的功能差异，发现RNN在时序任务中通过权重共享产生跨时间步的相关表示和归纳偏置。


<details>
  <summary>Details</summary>
Motivation: RNN和DNN在结构上仅相差权重共享（通过时间展开可证明），但表现出不同的功能特性。研究者希望理解这种结构相似性如何与功能差异相协调，探究权重共享对网络功能的具体影响。

Method: 开发了RNN和DNN的统一平均场理论，使用表示核描述在特征学习（μP）机制下完全训练的网络。该理论将训练视为对序列和模式的贝叶斯推断，直接揭示RNN权重共享的功能含义。

Result: 在DNN典型任务中，当学习信号克服权重随机性产生的噪声时，存在相变：低于阈值时RNN和DNN行为相同；高于阈值时，只有RNN能发展跨时间步的相关表示。在时序任务中，RNN的权重共享产生归纳偏置，通过插值无监督时间步来帮助泛化。

Conclusion: 该理论提供了一种将架构结构连接到功能偏置的方法，揭示了RNN权重共享如何导致跨时间步的相关表示和时序任务的归纳偏置，为理解神经网络架构与功能关系提供了理论框架。

Abstract: Recurrent and deep neural networks (RNNs/DNNs) are cornerstone architectures in machine learning. Remarkably, RNNs differ from DNNs only by weight sharing, as can be shown through unrolling in time. How does this structural similarity fit with the distinct functional properties these networks exhibit? To address this question, we here develop a unified mean-field theory for RNNs and DNNs in terms of representational kernels, describing fully trained networks in the feature learning ($μ$P) regime. This theory casts training as Bayesian inference over sequences and patterns, directly revealing the functional implications induced by the RNNs' weight sharing. In DNN-typical tasks, we identify a phase transition when the learning signal overcomes the noise due to randomness in the weights: below this threshold, RNNs and DNNs behave identically; above it, only RNNs develop correlated representations across timesteps. For sequential tasks, the RNNs' weight sharing furthermore induces an inductive bias that aids generalization by interpolating unsupervised time steps. Overall, our theory offers a way to connect architectural structure to functional biases.

</details>


### [120] [Multi-Objective Coverage via Constraint Active Search](https://arxiv.org/abs/2602.15595)
*Zakaria Shams Siam,Xuefeng Liu,Chong Liu*

Main category: cs.LG

TL;DR: 提出多目标覆盖（MOC）问题，旨在识别一小部分代表性样本，使其预测结果广泛覆盖可行多目标空间，以加速药物发现等科学过程。


<details>
  <summary>Details</summary>
Motivation: 在药物发现和材料设计等关键应用中，需要快速评估样本。现有方法要么关注样本空间覆盖，要么专注于帕累托前沿的多目标优化，无法直接解决化学多样性样本可能产生相同目标剖面且安全约束通常定义在目标上的问题。

Method: 提出MOC-CAS算法，使用基于上置信界的采集函数，在高斯过程后验预测指导下选择乐观样本。开发平滑松弛的硬可行性测试和近似优化器以实现高效优化。

Result: 在SARS-CoV-2和癌症相关的大规模蛋白质靶点数据集上，与竞争基线相比，MOC-CAS在基于SMILES特征的五项目标评估中取得了优越性能。

Conclusion: MOC-CAS算法有效解决了多目标覆盖问题，能够识别代表性样本集，显著加速科学发现过程，在药物发现等实际应用中具有重要价值。

Abstract: In this paper, we formulate the new multi-objective coverage (MOC) problem where our goal is to identify a small set of representative samples whose predicted outcomes broadly cover the feasible multi-objective space. This problem is of great importance in many critical real-world applications, e.g., drug discovery and materials design, as this representative set can be evaluated much faster than the whole feasible set, thus significantly accelerating the scientific discovery process. Existing works cannot be directly applied as they either focus on sample space coverage or multi-objective optimization that targets the Pareto front. However, chemically diverse samples often yield identical objective profiles, and safety constraints are usually defined on the objectives. To solve this MOC problem, we propose a novel search algorithm, MOC-CAS, which employs an upper confidence bound-based acquisition function to select optimistic samples guided by Gaussian process posterior predictions. For enabling efficient optimization, we develop a smoothed relaxation of the hard feasibility test and derive an approximate optimizer. Compared to the competitive baselines, we show that our MOC-CAS empirically achieves superior performances across large-scale protein-target datasets for SARS-CoV-2 and cancer, each assessed on five objectives derived from SMILES-based features.

</details>


### [121] [Guided Diffusion by Optimized Loss Functions on Relaxed Parameters for Inverse Material Design](https://arxiv.org/abs/2602.15648)
*Jens U. Kreber,Christian Weißenfels,Joerg Stueckler*

Main category: cs.LG

TL;DR: 提出基于扩散模型的逆设计方法，通过连续网格表示和可微分仿真解决离散参数空间问题，在复合材料设计中实现高精度匹配目标性能


<details>
  <summary>Details</summary>
Motivation: 逆设计问题在工程和材料科学中常见，但面临多个挑战：1）多个设计参数可能产生相同输出（多模态问题）；2）离散参数或约束使梯度优化无法直接应用；3）需要数值仿真作为中间步骤

Method: 1）将原始设计空间松弛为连续网格表示，实现可微分仿真；2）在松弛参数空间上训练扩散模型作为先验；3）通过引导扩散采样，利用从目标函数通过可微分仿真传播的梯度；4）通过反向投影获得原始参数空间的设计样本

Result: 在复合材料设计问题中，方法能够在中到高目标体积模量范围内，在2D和3D设置下找到相对误差在1%以内的多样化设计。同时通过多目标损失函数最小化材料密度

Conclusion: 提出的基于扩散模型的逆设计方法有效解决了离散参数空间问题，能够生成多样化且精确匹配目标性能的设计，为工程逆设计问题提供了新解决方案

Abstract: Inverse design problems are common in engineering and materials science. The forward direction, i.e., computing output quantities from design parameters, typically requires running a numerical simulation, such as a FEM, as an intermediate step, which is an optimization problem by itself. In many scenarios, several design parameters can lead to the same or similar output values. For such cases, multi-modal probabilistic approaches are advantageous to obtain diverse solutions. A major difficulty in inverse design stems from the structure of the design space, since discrete parameters or further constraints disallow the direct use of gradient-based optimization. To tackle this problem, we propose a novel inverse design method based on diffusion models. Our approach relaxes the original design space into a continuous grid representation, where gradients can be computed by implicit differentiation in the forward simulation. A diffusion model is trained on this relaxed parameter space in order to serve as a prior for plausible relaxed designs. Parameters are sampled by guided diffusion using gradients that are propagated from an objective function specified at inference time through the differentiable simulation. A design sample is obtained by backprojection into the original parameter space. We develop our approach for a composite material design problem where the forward process is modeled as a linear FEM problem. We evaluate the performance of our approach in finding designs that match a specified bulk modulus. We demonstrate that our method can propose diverse designs within 1% relative error margin from medium to high target bulk moduli in 2D and 3D settings. We also demonstrate that the material density of generated samples can be minimized simultaneously by using a multi-objective loss function.

</details>


### [122] [Certified Per-Instance Unlearning Using Individual Sensitivity Bounds](https://arxiv.org/abs/2602.15602)
*Hanna Benarroch,Jamal Atif,Olivier Cappé*

Main category: cs.LG

TL;DR: 提出基于自适应逐实例噪声校准的认证机器学习遗忘方法，相比传统基于最坏情况敏感性的差分隐私方法，能显著减少噪声注入并保持性能


<details>
  <summary>Details</summary>
Motivation: 传统基于差分隐私的认证机器学习遗忘方法需要根据最坏情况敏感性校准噪声，导致性能显著下降，限制了实际应用。需要探索更高效的遗忘机制。

Method: 采用自适应逐实例噪声校准方法，针对每个数据点对学习解的个体贡献进行定制化噪声注入。使用逐实例差分隐私定义数据点敏感性，针对岭回归的Langevin动力学训练推导高概率的逐实例敏感性边界。

Result: 理论分析表明该方法能实现认证遗忘，同时显著减少噪声注入。在线性设置实验中验证了理论发现，并在深度学习设置中提供了进一步实证证据。

Conclusion: 基于逐实例敏感性校准的自适应噪声注入方法为认证机器学习遗忘提供了更实用的解决方案，相比传统最坏情况敏感性方法能更好地平衡隐私保护与模型性能。

Abstract: Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.

</details>


### [123] [Symbolic recovery of PDEs from measurement data](https://arxiv.org/abs/2602.15603)
*Erion Morina,Philipp Scholl,Martin Holler*

Main category: cs.LG

TL;DR: 该论文提出使用基于有理函数的神经网络架构进行偏微分方程模型的符号表示，证明了在无噪声完整测量条件下，此类符号网络能唯一重建最简单的物理定律，并通过ParFam架构进行了实证验证。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程模型在自然科学中描述复杂关系很强大，但准确识别代表底层物理定律的PDE模型通常依赖间接噪声测量，且传统方法难以获得符号表达式，阻碍了可解释性。

Method: 采用基于有理函数的神经网络架构进行物理定律的符号表示，利用有理函数的逼近能力和算术运算灵活性。主要贡献是证明在无噪声完整测量条件下，符号网络能唯一重建最简单的物理定律。

Result: 理论证明符号网络能唯一重建物理定律，重建的定律在符号网络架构内可表达，正则化最小化参数化促进可解释性和稀疏性（L1正则化）。提供了符号网络的规律性结果，并通过ParFam架构进行了实证验证。

Conclusion: 基于有理函数的符号神经网络架构能够有效重建偏微分方程模型中的物理定律，在理论可识别性和实际可重建性方面都表现出色，为物理定律的可解释性表示提供了新途径。

Abstract: Models based on partial differential equations (PDEs) are powerful for describing a wide range of complex relationships in the natural sciences. Accurately identifying the PDE model, which represents the underlying physical law, is essential for a proper understanding of the problem. This reconstruction typically relies on indirect and noisy measurements of the system's state and, without specifically tailored methods, rarely yields symbolic expressions, thereby hindering interpretability. In this work, we address this issue by considering existing neural network architectures based on rational functions for the symbolic representation of physical laws. These networks leverage the approximation power of rational functions while also benefiting from their flexibility in representing arithmetic operations. Our main contribution is an identifiability result, showing that, in the limit of noiseless, complete measurements, such symbolic networks can uniquely reconstruct the simplest physical law within the PDE model. Specifically, reconstructed laws remain expressible within the symbolic network architecture, with regularization-minimizing parameterizations promoting interpretability and sparsity in case of $L^1$-regularization. In addition, we provide regularity results for symbolic networks. Empirical validation using the ParFam architecture supports these theoretical findings, providing evidence for the practical reconstructibility of physical laws.

</details>


### [124] [DNN-Enabled Multi-User Beamforming for Throughput Maximization under Adjustable Fairness](https://arxiv.org/abs/2602.15617)
*Kaifeng Lu,Markus Rupp,Stefan Schwarz*

Main category: cs.LG

TL;DR: 提出基于无线Transformer架构的优化无监督学习方法，通过拉格朗日乘子自动更新机制，在保证预设公平性的同时最大化总速率，实现公平性与总速率之间的帕累托前沿追踪。


<details>
  <summary>Details</summary>
Motivation: 无线通信中确保用户公平性是一个基本挑战，平衡公平性与总速率之间的权衡会导致非凸、多目标优化问题，其复杂度随网络规模增长而增加。

Method: 提出基于无线Transformer架构的优化无监督学习方法，通过拉格朗日乘子将总速率和公平性目标结合，采用双重上升算法自动更新拉格朗日乘子，实现可控的公平性约束。

Result: 该方法能够在规定公平性要求下灵活管理权衡优化，有效实现两个冲突目标之间的帕累托前沿追踪。

Conclusion: 所提出的基于无线Transformer的优化无监督学习方法为解决无线通信中的公平性-总速率权衡问题提供了灵活有效的解决方案。

Abstract: Ensuring user fairness in wireless communications is a fundamental challenge, as balancing the trade-off between fairness and sum rate leads to a non-convex, multi-objective optimization whose complexity grows with network scale. To alleviate this conflict, we propose an optimization-based unsupervised learning approach based on the wireless transformer (WiT) architecture that learns from channel state information (CSI) features. We reformulate the trade-off by combining the sum rate and fairness objectives through a Lagrangian multiplier, which is updated automatically via a dual-ascent algorithm. This mechanism allows for a controllable fairness constraint while simultaneously maximizing the sum rate, effectively realizing a trace on the Pareto front between two conflicting objectives. Our findings show that the proposed approach offers a flexible solution for managing the trade-off optimization under prescribed fairness.

</details>


### [125] [Beyond ReLU: Bifurcation, Oversmoothing, and Topological Priors](https://arxiv.org/abs/2602.15634)
*Erkan Turan,Gaspard Abel,Maysam Behmanesh,Emery Pierson,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 该论文从分岔理论角度重新审视GNN中的过平滑问题，提出通过替换激活函数来破坏同质稳定点，从而防止节点特征收敛到非信息状态。


<details>
  <summary>Details</summary>
Motivation: 图神经网络通过基于网络的消息传递学习节点表示，但深层GNN存在过平滑问题，节点特征会收敛到同质的非信息状态。作者希望从分岔理论的角度重新理解这一表示崩溃问题。

Method: 将过平滑问题重新定义为向稳定"同质固定点"的收敛问题。通过理论分析发现，用特定函数类替换标准单调激活函数（如ReLU）可以破坏这种不期望的稳定性。使用Lyapunov-Schmidt约简方法，分析证明这种替换会诱导分岔，使同质状态失稳并产生一对稳定的非同质模式。

Result: 理论预测了这些涌现模式的振幅具有精确的非平凡标度律，并在实验中得到了定量验证。推导了封闭形式的分岔感知初始化方法，并在真实基准实验中展示了其实用性。

Conclusion: 从分岔理论视角重新审视GNN过平滑问题，提出了通过激活函数替换来破坏同质稳定性的理论框架，为缓解深层GNN的过平滑问题提供了新的理论指导和实用方法。

Abstract: Graph Neural Networks (GNNs) learn node representations through iterative network-based message-passing. While powerful, deep GNNs suffer from oversmoothing, where node features converge to a homogeneous, non-informative state. We re-frame this problem of representational collapse from a \emph{bifurcation theory} perspective, characterizing oversmoothing as convergence to a stable ``homogeneous fixed point.'' Our central contribution is the theoretical discovery that this undesired stability can be broken by replacing standard monotone activations (e.g., ReLU) with a class of functions. Using Lyapunov-Schmidt reduction, we analytically prove that this substitution induces a bifurcation that destabilizes the homogeneous state and creates a new pair of stable, non-homogeneous \emph{patterns} that provably resist oversmoothing. Our theory predicts a precise, nontrivial scaling law for the amplitude of these emergent patterns, which we quantitatively validate in experiments. Finally, we demonstrate the practical utility of our theory by deriving a closed-form, bifurcation-aware initialization and showing its utility in real benchmark experiments.

</details>


### [126] [The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems](https://arxiv.org/abs/2602.15637)
*Amirreza Dolatpour Fathkouhi,Alireza Namazi,Heman Shakeri*

Main category: cs.LG

TL;DR: 传统时间序列插补基准存在"平稳性偏差"，简单方法在主导平稳区间表现好，但在关键瞬态区间形态保真度差。论文提出分层压力测试框架，区分平稳和瞬态区间评估，发现深度学习在瞬态区间保持点准确性和形态完整性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列插补基准使用均匀随机掩码和形状无关指标（MSE、RMSE），在具有主导吸引子的系统中会产生系统性"平稳性偏差"——简单方法因主要采样低熵平稳区间而显得优越，掩盖了在关键瞬态区间的性能缺陷。

Method: 提出"分层压力测试"框架：1）将评估划分为平稳和瞬态两个区间；2）使用连续血糖监测（CGM）作为测试平台，利用其精确的地面真实强迫函数（饮食、胰岛素）实现精确区间识别；3）从临床试验中推导经验缺失分布并应用于完整训练数据，防止模型利用不现实的干净观测。

Result: 1）平稳效率：线性插值在稳定区间达到最先进重建性能；2）瞬态保真度：线性方法在关键瞬态期间形态保真度（DTW）严重下降，出现"RMSE幻象"——低点误差掩盖信号形状破坏；3）区间条件模型选择：深度学习模型在瞬态期间保持点准确性和形态完整性。

Conclusion: 该框架适用于任何常规平稳性主导关键瞬态的受调控系统，强调需要区分评估区间，深度学习对于安全关键下游任务至关重要，而简单方法在低熵平稳区间足够且计算高效。

Abstract: Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings with broad implications:(i)~Stationary Efficiency: Linear interpolation achieves state-of-the-art reconstruction during stable intervals, confirming that complex architectures are computationally wasteful in low-entropy regimes.(ii)~Transient Fidelity: During critical transients (post-prandial peaks, hypoglycemic events), linear methods exhibit drastically degraded morphological fidelity (DTW), disproportionate to their RMSE -- a phenomenon we term the \emph{RMSE Mirage}, where low pointwise error masks the destruction of signal shape.(iii)~Regime-Conditional Model Selection: Deep learning models preserve both pointwise accuracy and morphological integrity during transients, making them essential for safety-critical downstream tasks. We further derive empirical missingness distributions from clinical trials and impose them on complete training data, preventing models from exploiting unrealistically clean observations and encouraging robustness under real-world missingness. This framework generalizes to any regulated system where routine stationarity dominates critical transients.

</details>


### [127] [Continuous-Time Piecewise-Linear Recurrent Neural Networks](https://arxiv.org/abs/2602.15649)
*Alena Brändle,Lukas Eisenmann,Florian Götz,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 提出连续时间分段线性循环神经网络(cPLRNN)，解决传统离散时间PLRNN在连续时间系统和不规则采样数据上的限制，同时保持数学可分析性和高性能


<details>
  <summary>Details</summary>
Motivation: 现有分段线性RNN(PLRNN)在动力系统重建中表现优异且具有数学可分析性，但都是离散时间模型，与大多数物理和生物过程的连续时间本质不符，且难以处理不规则时间间隔数据。神经ODE虽然解决连续时间问题，但性能不如PLRNN且缺乏可分析性

Method: 开发连续时间PLRNN(cPLRNN)理论，提出新的训练和模拟算法，利用分段线性结构绕过数值积分，并展示如何半解析地确定平衡点、极限环等重要拓扑结构

Result: 在动力系统重建基准测试中，cPLRNN既优于离散时间PLRNN，也优于神经ODE，包括具有硬阈值的间断系统

Conclusion: cPLRNN成功结合了连续时间建模的优势和PLRNN的高性能与可分析性，为科学和医学领域的动力系统重建提供了更合适的工具

Abstract: In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals. Neural ODEs are one solution, but they do not reach the DSR performance of PLRNNs and often lack their tractability. Here we develop theory for continuous-time PLRNNs (cPLRNNs): We present a novel algorithm for training and simulating such models, bypassing numerical integration by efficiently exploiting their PL structure. We further demonstrate how important topological objects like equilibria or limit cycles can be determined semi-analytically in trained models. We compare cPLRNNs to both their discrete-time cousins as well as Neural ODEs on DSR benchmarks, including systems with discontinuities which come with hard thresholds.

</details>


### [128] [Relative Geometry of Neural Forecasters: Linking Accuracy and Alignment in Learned Latent Geometry](https://arxiv.org/abs/2602.15676)
*Deniz Kucukahmetler,Maximilian Jean Hemmann,Julian Mosig von Aehrenfeld,Maximilian Amthor,Christian Deubel,Nico Scherf,Diaaeldin Taha*

Main category: cs.LG

TL;DR: 研究通过相对几何嵌入分析神经网络对动力系统的内部表示，发现不同模型家族在表示对齐上呈现可重复的结构模式，对齐度与预测精度相关但不完全一致。


<details>
  <summary>Details</summary>
Motivation: 神经网络能够准确预测复杂动力系统，但其内部如何表示潜在几何结构仍不清楚。研究旨在通过表示对齐的视角理解神经网络如何内部表示动力系统的潜在几何。

Method: 引入基于锚点的、几何无关的相对嵌入方法，消除潜在空间中的旋转和缩放模糊性。将此框架应用于七个典型动力系统（从周期性到混沌系统），分析多层感知机、循环神经网络、变换器和回声状态网络等不同模型家族的表示对齐。

Result: 发现可重复的家族级结构：多层感知机与MLPs对齐，循环网络与RNNs对齐，而变换器和回声状态网络虽然预测能力强但表示对齐较弱。对齐度通常与预测精度相关，但高精度可以与低对齐共存。

Conclusion: 相对几何为比较不同模型家族如何内部化和表示动力系统结构提供了一个简单、可重复的基础框架，揭示了神经网络表示动力系统的一致模式。

Abstract: Neural networks can accurately forecast complex dynamical systems, yet how they internally represent underlying latent geometry remains poorly understood. We study neural forecasters through the lens of representational alignment, introducing anchor-based, geometry-agnostic relative embeddings that remove rotational and scaling ambiguities in latent spaces. Applying this framework across seven canonical dynamical systems - ranging from periodic to chaotic - we reveal reproducible family-level structure: multilayer perceptrons align with other MLPs, recurrent networks with RNNs, while transformers and echo-state networks achieve strong forecasts despite weaker alignment. Alignment generally correlates with forecasting accuracy, yet high accuracy can coexist with low alignment. Relative geometry thus provides a simple, reproducible foundation for comparing how model families internalize and represent dynamical structure.

</details>


### [129] [Random Wavelet Features for Graph Kernel Machines](https://arxiv.org/abs/2602.15711)
*Valentin de Bassompierre,Jean-Charles Delvenne,Laurent Jacques*

Main category: cs.LG

TL;DR: 该论文提出了一种基于随机特征方法的随机谱节点嵌入技术，能够高效近似任何图核，在保持结构信息的同时实现可扩展的图表示学习。


<details>
  <summary>Details</summary>
Motivation: 图核提供了一种定义节点相似性的原则性方法，但直接计算在大规模网络上计算代价过高。需要设计能够高效近似图核的可扩展节点嵌入方法。

Method: 受欧几里得空间中核近似的随机特征方法启发，提出了随机谱节点嵌入技术，通过随机特征构造低维嵌入，其点积能够估计特定图核的低秩近似。

Result: 理论和实证结果表明，该方法比现有方法实现了更准确的核近似，特别是在谱局部化核方面表现优异。

Conclusion: 随机谱构造方法为可扩展且原则性的图表示学习提供了有效途径，能够高效近似图核并保持结构信息。

Abstract: Node embeddings map graph vertices into low-dimensional Euclidean spaces while preserving structural information. They are central to tasks such as node classification, link prediction, and signal reconstruction. A key goal is to design node embeddings whose dot products capture meaningful notions of node similarity induced by the graph. Graph kernels offer a principled way to define such similarities, but their direct computation is often prohibitive for large networks. Inspired by random feature methods for kernel approximation in Euclidean spaces, we introduce randomized spectral node embeddings whose dot products estimate a low-rank approximation of any specific graph kernel. We provide theoretical and empirical results showing that our embeddings achieve more accurate kernel approximations than existing methods, particularly for spectrally localized kernels. These results demonstrate the effectiveness of randomized spectral constructions for scalable and principled graph representation learning.

</details>


### [130] [CAMEL: An ECG Language Model for Forecasting Cardiac Events](https://arxiv.org/abs/2602.15677)
*Neelay Velingker,Alaia Solko-Breslin,Mayank Keoliya,Seewon Choi,Jiayi Xin,Anika Marathe,Alireza Oraii,Rajat Deo,Sameed Khatana,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAMEL是首个能够进行心电图长期信号推理和未来心脏事件预测的心电图语言模型，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前的心电图语言模型虽然能进行分类和报告生成，但无法预测未来心脏事件，而这对临床早期干预具有重要价值。

Method: 提出专门的心电图编码器实现心电信号与文本的跨模态理解，采用LoRA适配和课程学习管道训练，包括分类、指标计算和多轮对话推理。

Result: 在6个任务9个数据集上展示强大的零样本性能，在ECGBench上获得+7.0%绝对平均增益，在ECGForecastBench上超过全监督模型+12.4%，超过零样本ELMs +21.1%。

Conclusion: CAMEL是首个具备长期信号推理和预测能力的心电图语言模型，在多个基准测试中达到最先进水平，为临床早期干预提供了新工具。

Abstract: Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).

</details>


### [131] [Controlled oscillation modeling using port-Hamiltonian neural networks](https://arxiv.org/abs/2602.15704)
*Maximino Linares,Guillaume Doras,Thomas Hélie*

Main category: cs.LG

TL;DR: 本文提出将二阶离散梯度方法嵌入到端口哈密顿神经网络中学习动力系统，相比传统龙格-库塔方法表现更优，在三个不同动态行为的振荡器系统中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动方法学习动力系统时难以捕捉守恒定律，现有端口哈密顿神经网络方法虽然基于功率平衡原理，但通常不考虑功率保持离散化，且依赖龙格-库塔数值方法，这限制了其泛化能力。

Method: 提出将二阶离散梯度方法嵌入到端口哈密顿神经网络的学习框架中，用于建模动力系统。该方法在三个不同动态行为的振荡器系统上进行测试：基线谐波振荡器（二次能量存储）、杜芬振荡器（非二次哈密顿量）和自持振荡器（非线性耗散）。

Result: 实验表明，使用离散梯度方法在性能上优于同阶的龙格-库塔方法。同时比较了两种理论上等价的端口哈密顿系统表述，并分析了训练过程中正则化端口哈密顿神经网络雅可比矩阵的影响。

Conclusion: 二阶离散梯度方法能够更好地保持系统的功率平衡特性，提高端口哈密顿神经网络学习动力系统的准确性和泛化能力，特别是在处理不同动态行为的系统时表现优异。

Abstract: Learning dynamical systems through purely data-driven methods is challenging as they do not learn the underlying conservation laws that enable them to correctly generalize. Existing port-Hamiltonian neural network methods have recently been successfully applied for modeling mechanical systems. However, even though these methods are designed on power-balance principles, they usually do not consider power-preserving discretizations and often rely on Runge-Kutta numerical methods. In this work, we propose to use a second-order discrete gradient method embedded in the learning of dynamical systems with port-Hamiltonian neural networks. Numerical results are provided for three systems deliberately selected to span different ranges of dynamical behavior under control: a baseline harmonic oscillator with quadratic energy storage; a Duffing oscillator, with a non-quadratic Hamiltonian offering amplitude-dependent effects; and a self-sustained oscillator, which can stabilize in a controlled limit cycle through the incorporation of a nonlinear dissipation. We show how the use of this discrete gradient method outperforms the performance of a Runge-Kutta method of the same order. Experiments are also carried out to compare two theoretically equivalent port-Hamiltonian systems formulations and to analyze the impact of regularizing the Jacobian of port-Hamiltonian neural networks during training.

</details>


### [132] [MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2602.15740)
*Fatemeh Khalvandi,Saadat Izadi,Abdolah Chalechale*

Main category: cs.LG

TL;DR: 提出MRC-GAT模型用于阿尔茨海默病分类，通过copula相似性对齐、关系注意力和节点融合实现多模态特征整合，在TADPOLE和NACC数据集上达到96.87%和92.31%的准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病需要早期精确诊断，现有基于图的方法大多依赖固定结构设计，缺乏灵活性且难以泛化到异质患者数据。

Method: 提出Meta-Relational Copula-Based Graph Attention Network (MRC-GAT)，整合copula相似性对齐、关系注意力和节点融合作为元学习核心组件，将风险因素、认知测试分数和MRI特征在多模态统计空间中对齐并组合。

Result: 在TADPOLE和NACC数据集上分别达到96.87%和92.31%的准确率，优于现有诊断模型，展示了最先进的性能。

Conclusion: MRC-GAT模型通过提供疾病诊断各阶段的可解释性，证实了方法的鲁棒性和适用性，为阿尔茨海默病早期诊断提供了有效解决方案。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.

</details>


### [133] [UrbanVerse: Learning Urban Region Representation Across Cities and Tasks](https://arxiv.org/abs/2602.15750)
*Fengze Sun,Egemen Tanin,Shanika Karunasekera,Zuqing Li,Flora D. Salim,Jianzhong Qi*

Main category: cs.LG

TL;DR: UrbanVerse：一个用于跨城市表示学习和跨任务城市分析的基础模型，通过图神经网络和条件扩散模型实现城市区域表征的通用学习。


<details>
  <summary>Details</summary>
Motivation: 现有城市区域表示学习方法在跨城市和跨任务泛化能力上存在局限，需要构建一个基础模型来支持通用的城市分析应用。

Method: 1. 将区域建模为图节点，使用随机游走生成反映局部和邻域结构特征的"区域序列"；2. 提出HCondDiffCT模块，将区域条件先验知识和任务条件语义集成到扩散过程中，联合建模多个下游预测任务。

Result: 在真实数据集上的实验表明，UrbanVerse在跨城市设置的六个任务中始终优于最先进方法，预测准确率提升最高达35.89%。

Conclusion: UrbanVerse成功实现了跨城市和跨任务的通用城市表示学习，为城市分析提供了一个有效的基础模型框架。

Abstract: Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form "sequences of regions" that reflect both local and neighborhood structural features for urban region representation learning. For cross-task generalization, we propose a cross-task learning module named HCondDiffCT. This module integrates region-conditioned prior knowledge and task-conditioned semantics into the diffusion process to jointly model multiple downstream urban prediction tasks. HCondDiffCT is generic. It can also be integrated with existing urban representation learning models to enhance their downstream task effectiveness. Experiments on real-world datasets show that UrbanVerse consistently outperforms state-of-the-art methods across six tasks under cross-city settings, achieving up to 35.89% improvements in prediction accuracy.

</details>


### [134] [Beyond Match Maximization and Fairness: Retention-Optimized Two-Sided Matching](https://arxiv.org/abs/2602.15752)
*Ren Kishimoto,Rikiya Takehi,Koichi Tanaka,Masahiro Nomura,Riku Togashi,Yoji Tomita,Yuta Saito*

Main category: cs.LG

TL;DR: 提出MRet算法，通过动态学习个性化留存曲线来优化双边匹配平台的用户留存，而非传统方法只关注匹配数量或公平性。


<details>
  <summary>Details</summary>
Motivation: 传统双边匹配平台（如在线约会和招聘）通常最大化匹配数量，但这会导致用户匹配分布不均：部分用户获得过多匹配，而许多用户获得极少匹配并最终流失。用户留存对依赖订阅的平台至关重要。虽然公平性目标可以解决匹配最大化问题，但公平性本身并非平台的终极目标，用户不会仅仅因为曝光均等就奖励平台。在实践中，用户留存通常是最终目标，随意依赖公平性会让留存优化变得随机。

Method: 提出Matching for Retention (MRet)算法，这是一种动态学习排序(LTR)方法。不同于传统双边匹配算法，MRet通过学习每个用户的个人资料和交互历史来建模个性化留存曲线。基于这些曲线，MRet动态调整推荐，联合考虑接收推荐用户和被推荐用户的留存收益，从而将有限的匹配机会分配到最能提升整体留存的地方。

Result: 在合成数据集和来自主要在线约会平台的真实数据集上的实证评估表明，MRet实现了更高的用户留存率。这是因为传统方法主要优化匹配数量或公平性，而非留存。

Conclusion: MRet算法通过直接优化用户留存而非间接的匹配数量或公平性指标，为双边匹配平台提供了更有效的用户保留策略。该方法将有限的匹配机会分配到最能提升整体留存的地方，解决了传统方法导致的用户流失问题。

Abstract: On two-sided matching platforms such as online dating and recruiting, recommendation algorithms often aim to maximize the total number of matches. However, this objective creates an imbalance, where some users receive far too many matches while many others receive very few and eventually abandon the platform. Retaining users is crucial for many platforms, such as those that depend heavily on subscriptions. Some may use fairness objectives to solve the problem of match maximization. However, fairness in itself is not the ultimate objective for many platforms, as users do not suddenly reward the platform simply because exposure is equalized. In practice, where user retention is often the ultimate goal, casually relying on fairness will leave the optimization of retention up to luck.
  In this work, instead of maximizing matches or axiomatically defining fairness, we formally define the new problem setting of maximizing user retention in two-sided matching platforms. To this end, we introduce a dynamic learning-to-rank (LTR) algorithm called Matching for Retention (MRet). Unlike conventional algorithms for two-sided matching, our approach models user retention by learning personalized retention curves from each user's profile and interaction history. Based on these curves, MRet dynamically adapts recommendations by jointly considering the retention gains of both the user receiving recommendations and those who are being recommended, so that limited matching opportunities can be allocated where they most improve overall retention. Naturally but importantly, empirical evaluations on synthetic and real-world datasets from a major online dating platform show that MRet achieves higher user retention, since conventional methods optimize matches or fairness rather than retention.

</details>


### [135] [GLM-5: from Vibe Coding to Agentic Engineering](https://arxiv.org/abs/2602.15763)
*GLM-5 Team,:,Aohan Zeng,Xin Lv,Zhenyu Hou,Zhengxiao Du,Qinkai Zheng,Bin Chen,Da Yin,Chendi Ge,Chengxing Xie,Cunxiang Wang,Gengzheng Pan,Hao Zeng,Haoke Zhang,Haoran Wang,Huilong Chen,Jiajie Zhang,Jian Jiao,Jiaqi Guo,Jingsen Wang,Jingzhao Du,Jinzhu Wu,Kedong Wang,Lei Li,Lin Fan,Lucen Zhong,Mingdao Liu,Mingming Zhao,Pengfan Du,Qian Dong,Rui Lu,Shuang-Li,Shulin Cao,Song Liu,Ting Jiang,Xiaodong Chen,Xiaohan Zhang,Xuancheng Huang,Xuezhen Dong,Yabo Xu,Yao Wei,Yifan An,Yilin Niu,Yitong Zhu,Yuanhao Wen,Yukuo Cen,Yushi Bai,Zhongpei Qiao,Zihan Wang,Zikang Wang,Zilin Zhu,Ziqiang Liu,Zixuan Li,Bojie Wang,Bosi Wen,Can Huang,Changpeng Cai,Chao Yu,Chen Li,Chen Li,Chenghua Huang,Chengwei Hu,Chenhui Zhang,Chenzheng Zhu,Congfeng Yin,Daoyan Lin,Dayong Yang,Di Wang,Ding Ai,Erle Zhu,Fangzhou Yi,Feiyu Chen,Guohong Wen,Hailong Sun,Haisha Zhao,Haiyi Hu,Hanchen Zhang,Hanrui Liu,Hanyu Zhang,Hao Peng,Hao Tai,Haobo Zhang,He Liu,Hongwei Wang,Hongxi Yan,Hongyu Ge,Huan Liu,Huan Liu,Huanpeng Chu,Jia'ni Zhao,Jiachen Wang,Jiajing Zhao,Jiamin Ren,Jiapeng Wang,Jiaxin Zhang,Jiayi Gui,Jiayue Zhao,Jijie Li,Jing An,Jing Li,Jingwei Yuan,Jinhua Du,Jinxin Liu,Junkai Zhi,Junwen Duan,Kaiyue Zhou,Kangjian Wei,Ke Wang,Keyun Luo,Laiqiang Zhang,Leigang Sha,Liang Xu,Lindong Wu,Lintao Ding,Lu Chen,Minghao Li,Nianyi Lin,Pan Ta,Qiang Zou,Rongjun Song,Ruiqi Yang,Shangqing Tu,Shangtong Yang,Shaoxiang Wu,Shengyan Zhang,Shijie Li,Shuang Li,Shuyi Fan,Wei Qin,Wei Tian,Weining Zhang,Wenbo Yu,Wenjie Liang,Xiang Kuang,Xiangmeng Cheng,Xiangyang Li,Xiaoquan Yan,Xiaowei Hu,Xiaoying Ling,Xing Fan,Xingye Xia,Xinyuan Zhang,Xinze Zhang,Xirui Pan,Xunkai Zhang,Yandong Wu,Yanfu Li,Yidong Wang,Yifan Zhu,Yijun Tan,Yilin Zhou,Yiming Pan,Ying Zhang,Yinpei Su,Yipeng Geng,Yipeng Geng,Yong Yan,Yonglin Tan,Yuean Bi,Yuhan Shen,Yuhao Yang,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yurong Wu,Yutao Zhang,Yuxi Duan,Yuxuan Zhang,Zezhen Liu,Zhengtao Jiang,Zhenhe Yan,Zheyu Zhang,Zhixiang Wei,Zhuo Chen,Zhuoer Feng,Zijun Yao,Ziwei Chai,Ziyuan Wang,Zuzhou Zhang,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: GLM-5是一个新一代基础模型，旨在将氛围编码范式转向代理工程，通过DSA降低训练推理成本，采用异步强化学习提升对齐和自主性，在开放基准测试和现实编码任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 将氛围编码范式转向代理工程，解决传统模型在复杂、长视野交互中的局限性，提升模型在端到端软件工程挑战中的实际应用能力。

Method: 1. 采用DSA技术显著降低训练和推理成本同时保持长上下文保真度；2. 实现新的异步强化学习基础设施，通过解耦生成与训练提高后训练效率；3. 提出新颖的异步代理RL算法，提升强化学习质量。

Result: 在主要开放基准测试中达到最先进性能，在现实编码任务中展现前所未有的能力，超越先前基线在处理端到端软件工程挑战方面的表现。

Conclusion: GLM-5通过创新的DSA技术和异步强化学习框架，成功实现了从氛围编码到代理工程的范式转变，为复杂软件工程任务提供了强大的基础模型解决方案。

Abstract: We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.

</details>


### [136] [The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety](https://arxiv.org/abs/2602.15799)
*Max Springer,Chung Peng Lee,Blossom Metevier,Jane Castleman,Bohdan Turbal,Hayoung Jung,Zeyu Shen,Aleksandra Korolova*

Main category: cs.LG

TL;DR: 微调对齐的语言模型即使在良性任务上也会不可预测地降低安全性防护，这是由于对齐几何的低维子空间具有尖锐曲率，导致梯度下降的二阶加速效应将优化轨迹推入对齐敏感区域。


<details>
  <summary>Details</summary>
Motivation: 当前安全微调方法存在结构性盲点：即使训练数据无害且开发者无恶意意图，微调对齐的语言模型仍会不可预测地破坏安全防护。主流解释认为微调更新在高维参数空间中应与安全关键方向正交，但这种正交性在梯度下降动态下结构不稳定。

Method: 通过新颖的几何分析，证明对齐集中在具有尖锐曲率的低维子空间中，创建了脆弱的几何结构。提出对齐不稳定性条件（Alignment Instability Condition），包含三个几何特性，当同时满足时会导致安全性退化。建立四次方缩放定律：对齐损失随训练时间的四次方增长。

Result: 发现对齐脆弱性是梯度下降在弯曲流形上的固有几何特性，而非可修补的缺陷。微调损失曲率产生二阶加速效应，系统性地将优化轨迹引导至对齐敏感区域，导致安全防护退化。

Conclusion: 当前安全范式存在结构性盲点，主流安全微调方法只解决了基本动态问题的初始快照。需要开发曲率感知方法，推动对齐安全分析从反应性红队测试转向开放权重模型部署的预测性诊断。

Abstract: Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters. These results expose a structural blind spot in the current safety paradigm. The dominant approaches to safe fine-tuning address only the initial snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug to be patched; it is an intrinsic geometric property of gradient descent on curved manifolds. Our results motivate the development of curvature-aware methods, and we hope will further enable a shift in alignment safety analysis from reactive red-teaming to predictive diagnostics for open-weight model deployment.

</details>


### [137] [Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning](https://arxiv.org/abs/2602.15817)
*Oswin So,Eric Yang Yu,Songyuan Zhang,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: 提出FGE方法，通过可行性引导探索同时识别可行初始条件子集并学习可达性问题的安全策略，在MuJoCo和Kinetix仿真器中比现有方法覆盖率高50%以上。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在控制任务上表现优异，但应用于可达性问题存在根本性不匹配：可达性寻求最大化系统保持安全的状态集合，而RL优化用户指定分布上的期望回报。这种不匹配可能导致策略在低概率但仍处于安全集合的状态上表现不佳。

Method: 提出可行性引导探索(FGE)方法，同时识别存在安全策略的可行初始条件子集，并学习解决该初始条件集合上可达性问题的策略。

Result: 在MuJoCo仿真器和具有像素观测的Kinetix仿真器的挑战性初始条件任务中，FGE学习的策略比现有最佳方法的覆盖率高50%以上。

Conclusion: FGE通过同时识别可行初始条件和学习可达性策略，有效解决了RL与可达性问题之间的不匹配，在复杂控制任务中显著提高了安全覆盖范围。

Abstract: Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.

</details>


### [138] [CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing](https://arxiv.org/abs/2602.15823)
*Zarif Ikram,Arad Firouzkouhi,Stephen Tu,Mahdi Soltanolkotabi,Paria Rashidinejad*

Main category: cs.LG

TL;DR: CrispEdit是一个可扩展的二阶编辑算法，通过将能力保持作为显式约束，使用低曲率子空间投影来防止大语言模型编辑中的能力退化问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型编辑中存在能力保持的核心挑战：成功改变目标行为的方法可能会悄悄操纵编辑代理并破坏通用能力，产生类似于代理/奖励攻击的退化行为。

Method: 将编辑建模为约束优化问题，通过将编辑更新投影到能力损失景观的低曲率子空间来强制执行约束。使用Bregman散度表达能力约束，通过Kronecker分解近似曲率(K-FAC)和新型矩阵自由投影器实现高效二阶计算。

Result: 在标准模型编辑基准测试中，CrispEdit实现了高编辑成功率，同时在数据集上的平均能力退化保持在1%以下，显著优于先前编辑方法。

Conclusion: CrispEdit提供了一个可扩展且原则性的二阶编辑算法，通过显式约束能力保持，有效解决了大语言模型编辑中的能力退化问题，统一并推广了现有编辑方法。

Abstract: A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.

</details>


### [139] [Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics](https://arxiv.org/abs/2602.15820)
*Anna Zimmel,Paul Setinek,Gianluca Galletti,Johannes Brandstetter,Werner Zellinger*

Main category: cs.LG

TL;DR: 提出基于D-最优统计量的测试时自适应框架，解决机器学习代理模型在工程仿真中因分布偏移导致的性能下降问题，首次系统性地实现了高维仿真回归的有效TTA。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在工程仿真中广泛应用，但训练与部署间的分布偏移（如未见几何或配置）常导致性能严重下降。现有测试时自适应方法主要针对低维分类问题，不适用于高维、非结构化的仿真回归任务。

Method: 提出基于存储最大化信息（D-最优）统计量的TTA框架，能够实现稳定自适应和测试时的原则性参数选择。该方法应用于预训练的仿真代理模型，计算成本可忽略不计。

Result: 在SIMSHIFT和EngiBench基准测试上验证，实现了高达7%的分布外性能提升，计算成本可忽略。这是首次系统性地证明高维仿真回归和生成设计优化中TTA的有效性。

Conclusion: 提出的D-最优统计量TTA框架成功解决了高维仿真回归中的分布偏移问题，为工程仿真中的机器学习代理模型提供了稳定、高效的测试时自适应解决方案。

Abstract: Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.

</details>


### [140] [Operationalising the Superficial Alignment Hypothesis via Task Complexity](https://arxiv.org/abs/2602.15829)
*Tomás Vergara-Browne,Darshan Patil,Ivan Titov,Siva Reddy,Tiago Pimentel,Marius Mosbach*

Main category: cs.LG

TL;DR: 论文提出任务复杂度概念来量化对齐假设，发现预训练大幅降低任务复杂度，后训练进一步将复杂度降低数个数量级，任务适应通常只需极少信息（几KB）


<details>
  <summary>Details</summary>
Motivation: 表面对齐假设（SAH）缺乏精确定义，导致支持论据相互矛盾且受到重要批评。需要建立量化框架来统一理解预训练和后训练在模型知识获取中的作用。

Method: 提出任务复杂度概念：达到目标任务性能所需的最短程序长度。在此框架下，SAH被重新定义为预训练模型大幅降低许多任务的复杂度。通过实验估计数学推理、机器翻译和指令跟随的任务复杂度。

Result: 预训练模型能访问任务的高性能，但可能需要GB级长度的程序；后训练将达到相同性能的复杂度降低数个数量级。任务适应通常只需极少信息（几KB）。

Conclusion: 任务复杂度框架为表面对齐假设提供了精确的量化定义，统一了先前看似矛盾的支持论据。实验表明预训练大幅降低任务复杂度，后训练进一步显著压缩复杂度，任务适应所需信息量通常很小。

Abstract: The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model. Further, we find that pre-training enables access to strong performances on our tasks, but it can require programs of gigabytes of length to access them. Post-training, on the other hand, collapses the complexity of reaching this same performance by several orders of magnitude. Overall, our results highlight that task adaptation often requires surprisingly little information -- often just a few kilobytes.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [141] ["The Intangible Victory", Interactive Audiovisual Installation](https://arxiv.org/abs/2602.15071)
*Konstantinos Tsioutas,Panagiotis Pangalos,Konstantinos Tiligadis,Andreas Sitorengo*

Main category: cs.MM

TL;DR: 《无形胜利》是一个基于萨莫色雷斯胜利女神像的视听装置，通过数字媒体重新定义古代雕塑的视觉象征意义，关注时间作为磨损因素（熵）和虚空作为雕塑形式缺失的特殊重要性。


<details>
  <summary>Details</summary>
Motivation: 重新诠释古代雕塑的视觉象征意义，特别关注时间作为磨损因素（熵）的作用，以及虚空作为雕塑形式缺失的重要性。探索在当代语境下如何通过数字媒体重新解读传统艺术作品。

Method: 使用交互式数字媒体技术，将雕塑形式重构为空间中的纤维圆柱排列。采用彩色导电传感器线，允许参观者通过运动与作品互动，创造声音环境。声音完全取代了体积，虚空与观众共同呈现萨莫色雷斯胜利女神像的视听象征。

Result: 创造了一个新型的体验对话空间，观众与作品的互动产生了空间与时间之间的新关系。通过数字媒体揭示了雕塑形式的缺失，将传统雕塑转化为交互式视听体验，重新定义了艺术作品的当代解读方式。

Conclusion: 虚空与观众的互动共同构成了萨莫色雷斯胜利女神像的视听象征，数字媒体技术成功地将古代雕塑转化为当代交互艺术体验，展现了时间、空间和观众参与在艺术解读中的重要性。

Abstract: "Intangible Victory" is an audiovisual installation in the form of the intangible being of the Victory of Samothrace that uses interactive digital media. Specifically, through this installation, we redefine the visual symbolism of the ancient sculpture, paying attention to time as a wear factor (entropy) and the special importance of the void as an absence of the sculptural form. Emptiness completes the intangible essence of the sculpture in the field of symbolism as well as in that of artistic significance for the interpretation of the work today. The function of the void and the interaction of the viewer with the work, causes the emergence of a new experience-dialogue between space and time. The use of digital media and technology reveals the absence of the sculptural form as it is visualized in the Victory of Samothrace. The sculptural form is reconstructed from fibers in space in a cylindrical arrangement. The form is rendered with colored strings - conductive sensors, that allow the visitor to interact with the work, creating a sound environment through movement. The sound completely replaces the volume, as the void of the sculptural form together with the viewer in unison present an audiovisual symbolism of the Victory of Samothrace.

</details>


### [142] [Proactive Conversational Assistant for a Procedural Manual Task based on Audio and IMU](https://arxiv.org/abs/2602.15707)
*Rehana Mahfuz,Yinyi Guo,Erik Visser,Phanidhar Chinchili*

Main category: cs.MM

TL;DR: 提出首个仅使用音频和IMU等轻量隐私保护模态的实时对话助手，用于家具组装任务指导，通过UWA LoRA微调方法显著提升性能并实现边缘部署。


<details>
  <summary>Details</summary>
Motivation: 传统基于视频的对话助手计算成本高且侵犯用户隐私，需要开发仅使用轻量隐私保护模态（音频和IMU）的实时对话助手来指导程序性任务。

Method: 1) 构建包含对话指导家具组装任务的数据集；2) 设计User Whim Agnostic (UWA) LoRA微调方法，抑制非信息性对话同时保留重要指令；3) 在边缘设备上实现不依赖云端的部署。

Result: UWA LoRA微调使F-score提升超过30%，通过消除提示中的上下文示例实现16倍加速，成功在边缘设备上实现实时对话助手。

Conclusion: 首次证明仅使用音频和IMU等轻量隐私保护模态即可构建实时对话助手，UWA LoRA微调方法有效提升模型性能，实现了边缘部署的实用系统。

Abstract: Real-time conversational assistants for procedural tasks often depend on video input, which can be computationally expensive and compromise user privacy. For the first time, we propose a real-time conversational assistant that provides comprehensive guidance for a procedural task using only lightweight privacy-preserving modalities such as audio and IMU inputs from a user's wearable device to understand the context. This assistant proactively communicates step-by-step instructions to a user performing a furniture assembly task, and answers user questions. We construct a dataset containing conversations where the assistant guides the user in performing the task. On observing that an off-the-shelf language model is a very talkative assistant, we design a novel User Whim Agnostic (UWA) LoRA finetuning method which improves the model's ability to suppress less informative dialogues, while maintaining its tendency to communicate important instructions. This leads to >30% improvement in the F-score. Finetuning the model also results in a 16x speedup by eliminating the need to provide in-context examples in the prompt. We further describe how such an assistant is implemented on edge devices with no dependence on the cloud.

</details>
